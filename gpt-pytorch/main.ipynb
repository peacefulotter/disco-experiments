{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/minGPT.git\n",
    "!pip install -e ./minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.55M\n"
     ]
    }
   ],
   "source": [
    "model_type = 'gpt-nano'\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = 50257 # openai's model vocabulary\n",
    "model_config.block_size = 1024  # openai's model block_size (i.e. input context length)\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "batch_size = 64\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(\n",
    "#     lambda batch: tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt'), \n",
    "#     remove_columns=['text'], \n",
    "#     batch_size=batch_size,\n",
    "#     batched=True, \n",
    "# )\n",
    "# return dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 582510\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 2461\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wikitext(split, tokenizer, max_length=512):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=split)\n",
    "    dataset = dataset.filter(lambda x: len(x['text']) > 0)\n",
    "    def encode(batch):\n",
    "        return tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    dataset.set_transform(encode)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = get_wikitext('train[:50%]', tokenizer, max_length=max_length)\n",
    "eval_dataset = get_wikitext('validation', tokenizer, max_length=max_length)\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikitextDataset(Dataset):\n",
    "    def __init__(self, dataset, max_length=512):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataset[idx]['input_ids']\n",
    "        y = None\n",
    "\n",
    "        if isinstance(idx, torch.Tensor) or isinstance(idx, slice):\n",
    "            size, _ = x.shape\n",
    "            y = torch.empty((size, max_length))\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = torch.ones(size) * 50256\n",
    "        else:\n",
    "            y = torch.empty((max_length), dtype=torch.long)\n",
    "            y[:-1] = x[1:]\n",
    "            y[-1] = 50256\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "tr_dataset = WikitextDataset(train_dataset, max_length)\n",
    "ev_dataset = WikitextDataset(train_dataset, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WikitextDataset(Dataset):\n",
    "#     def __init__(self, dataset, max_length=512):\n",
    "#         x = dataset['input_ids']\n",
    "#         size, _ = x.shape\n",
    "#         y = torch.empty((size, max_length))\n",
    "#         y[:,:-1] = x[:,1:]\n",
    "#         y[:,-1] = torch.ones(size) * 50256\n",
    "#         self.data = torch.cat((x.unsqueeze(2), y.unsqueeze(2)), dim=2)\n",
    "#         self.data = self.data.reshape((size, 2, max_length))\n",
    "#         self.data = self.data.to(torch.int64)\n",
    "#         print(self.data.shape)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x, y = self.data[idx]\n",
    "#         return x, y\n",
    "\n",
    "# tr_dataset = WikitextDataset(train_dataset, max_length)\n",
    "# ev_dataset = WikitextDataset(eval_dataset, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651210752"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import psutil\n",
    "process = psutil.Process()\n",
    "\n",
    "def get_mem():\n",
    "    return process.memory_info().rss \n",
    "\n",
    "get_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 6e-4\n",
    "train_config.max_iters = 60_000\n",
    "train_config.batch_size = batch_size\n",
    "train_config.num_workers = 4\n",
    "trainer = Trainer(train_config, model, tr_dataset)\n",
    "\n",
    "ev_loader = DataLoader(\n",
    "    ev_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def custom_evaluate(model, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in ev_loader:\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        x, y = batch\n",
    "        logits, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "benchmark = []\n",
    "\n",
    "def on_batch_end(t):\n",
    "    # if t.iter_num % 2 == 0: \n",
    "    eval_loss = 0 # custom_evaluate(t.model, device=t.device)\n",
    "    mem = get_mem()\n",
    "    print(f'DT: {t.iter_dt:.3f}, iter: {t.iter_num:05d}, train_loss: {t.loss:.4f}, eval_loss: {eval_loss:.4f}, mem: {mem / (1024 * 1024):.2f} MB')\n",
    "    benchmark.append({'iter': t.iter_num, 'train_loss': t.loss, 'eval_loss': eval_loss, 'time': t.iter_time, 'mem': mem})\n",
    "\n",
    "trainer.add_callback('on_batch_end', on_batch_end)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
