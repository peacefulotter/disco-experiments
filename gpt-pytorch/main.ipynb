{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/edge/epfl/ma3/disco/gpt-pytorch/minGPT\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/edge/.local/lib/python3.10/site-packages (from minGPT==0.0.1) (2.0.0+cu118)\n",
      "Requirement already satisfied: networkx in /home/edge/.local/lib/python3.10/site-packages (from torch->minGPT==0.0.1) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/edge/.local/lib/python3.10/site-packages (from torch->minGPT==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/edge/.local/lib/python3.10/site-packages (from torch->minGPT==0.0.1) (3.12.4)\n",
      "Requirement already satisfied: sympy in /home/edge/.local/lib/python3.10/site-packages (from torch->minGPT==0.0.1) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /home/edge/.local/lib/python3.10/site-packages (from torch->minGPT==0.0.1) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /home/edge/.local/lib/python3.10/site-packages (from torch->minGPT==0.0.1) (3.1.2)\n",
      "Requirement already satisfied: lit in /home/edge/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->minGPT==0.0.1) (17.0.3)\n",
      "Requirement already satisfied: cmake in /home/edge/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->minGPT==0.0.1) (3.27.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/edge/.local/lib/python3.10/site-packages (from jinja2->torch->minGPT==0.0.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/edge/.local/lib/python3.10/site-packages (from sympy->torch->minGPT==0.0.1) (1.3.0)\n",
      "Installing collected packages: minGPT\n",
      "  Attempting uninstall: minGPT\n",
      "    Found existing installation: minGPT 0.0.1\n",
      "    Uninstalling minGPT-0.0.1:\n",
      "      Successfully uninstalled minGPT-0.0.1\n",
      "  Running setup.py develop for minGPT\n",
      "Successfully installed minGPT-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/karpathy/minGPT.git\n",
    "!pip install -e ./minGPT\n",
    "# !pip install mingpt --no-index --find-links file:///home/edge/epfl/ma3/disco/gpt-pytorch/minGPT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from wikitext import get_wikitext_data\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./minGPT/mingpt/trainer.py | grep assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-nano',\n",
       " 'n_head': 3,\n",
       " 'n_layer': 3,\n",
       " 'n_embd': 48,\n",
       " 'dataset': 'wikitext',\n",
       " 'batch_size': 16,\n",
       " 'seq_length': 256,\n",
       " 'lr': 0.001,\n",
       " 'max_iters': 1200,\n",
       " 'weight_decay': 0.001,\n",
       " 'optimizer': 'adamw',\n",
       " 'grad_clip': 1,\n",
       " 'scheduler': None,\n",
       " 'dropout': 0,\n",
       " 'num_workers': 4,\n",
       " 'vocab_size': 50257,\n",
       " 'wandb_project': 'disco-gpt-benchmark',\n",
       " 'wandb_name': 'gpt-nano_wikitext_bs=16_seq=256_lr=0.001_iter=3600'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../config.json') as f:    \n",
    "    config = json.load(f)\n",
    "config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.51M\n"
     ]
    }
   ],
   "source": [
    "model_type = config['model']\n",
    "model_config = GPT.get_default_config()\n",
    "# model_config.n_head = config['n_head']\n",
    "# model_config.n_layer = config['n_layer']\n",
    "# model_config.n_embd = config['n_embd']\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = config['vocab_size'] # openai's model vocabulary\n",
    "model_config.block_size = config['seq_length']  # openai's model block_size (i.e. input context length)\n",
    "model_config.attn_pdrop = config['dropout']\n",
    "model_config.resid_pdrop = config['dropout']\n",
    "model_config.embd_pdrop = config['dropout']\n",
    "model_config.weight_decay = config['weight_decay']\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "batch_size = config['batch_size']\n",
    "max_length = config['seq_length'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(\n",
    "#     lambda batch: tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt'), \n",
    "#     remove_columns=['text'], \n",
    "#     batch_size=batch_size,\n",
    "#     batched=True, \n",
    "# )\n",
    "# return dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaa3b795ff340f3aaf2dde5f0a4a434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/582510 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_wikitext(split, tokenizer, max_length=512):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=split)\n",
    "    dataset = dataset.filter(lambda x: len(x['text']) > 0)\n",
    "    # dataset = dataset.map(lambda x: {'text': x['text'], 'length': [len(y) for y in x['text']] }, batched=True, batch_size=64)\n",
    "    # dataset = dataset.sort('length')\n",
    "    # def encode(batch):\n",
    "    #     return tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    # dataset.set_transform(encode)\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt'), \n",
    "        remove_columns=['text'], \n",
    "        batch_size=batch_size,\n",
    "        batched=True, \n",
    "    )\n",
    "    dataset = dataset.remove_columns(['attention_mask'])\n",
    "    dataset = dataset.with_format(\"torch\")\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_wikitext('train[:50%]', tokenizer, max_length=max_length)\n",
    "eval_dataset = get_wikitext('validation', tokenizer, max_length=max_length)\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikitextDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    # x = tokenizer(self.dataset[idx][\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    # x = x['input_ids']\n",
    "    \n",
    "    # y = torch.empty(x.shape)\n",
    "    # y[:,:-1] = x[:,1:]\n",
    "    # y[:,-1] = torch.ones(x.shape[0]) * 50256\n",
    "\n",
    "    # =========================================================\n",
    "\n",
    "    # x = self.dataset[idx]['input_ids']\n",
    "    # y = None\n",
    "    # if isinstance(idx, torch.Tensor) or isinstance(idx, slice):\n",
    "    #     y = torch.empty(x.shape)\n",
    "    #     y[:,:-1] = x[:,1:]\n",
    "    #     y[:,-1] = torch.ones(x.shape[0]) * 50256\n",
    "    # else:\n",
    "    #     y = torch.empty(x.shape, dtype=torch.long)\n",
    "    #     y[:-1] = x[1:]\n",
    "    #     y[-1] = 50256\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataset[idx]['input_ids']\n",
    "        y = torch.empty(x.shape, dtype=x.dtype)\n",
    "        y[:-1] = x[1:]\n",
    "        y[-1] = 50256\n",
    "        return x, y\n",
    "\n",
    "tr_dataset = WikitextDataset(train_dataset)\n",
    "ev_dataset = WikitextDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256],\n",
       "          [ 2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,  1279,\n",
       "            2954,    29, 17740,   357,  4960,  1058, 10545,   230,    99,   161,\n",
       "             254,   112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,\n",
       "             837,  6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,\n",
       "            1267,   837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,\n",
       "            6711,  2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
       "              31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13,\n",
       "           44206,   329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,\n",
       "             287,  2869,   837,   340,   318,   262,  2368,   983,   287,   262,\n",
       "             569, 18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,\n",
       "             286, 16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,\n",
       "             663, 27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,\n",
       "             983,   290,  5679,   262,   366, 17871,  5321,   366,   837,   257,\n",
       "           23634,  2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,\n",
       "             262,  5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,\n",
       "             290,   389, 46852,  1028,   262, 11773,  4326,   366,  1279,  2954,\n",
       "              29, 12552,   366,   764,   220,   198, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       "  tensor([[ 2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,  1279,\n",
       "            2954,    29, 17740,   357,  4960,  1058, 10545,   230,    99,   161,\n",
       "             254,   112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,\n",
       "             837,  6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,\n",
       "            1267,   837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,\n",
       "            6711,  2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
       "              31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13,\n",
       "           44206,   329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,\n",
       "             287,  2869,   837,   340,   318,   262,  2368,   983,   287,   262,\n",
       "             569, 18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,\n",
       "             286, 16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,\n",
       "             663, 27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,\n",
       "             983,   290,  5679,   262,   366, 17871,  5321,   366,   837,   257,\n",
       "           23634,  2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,\n",
       "             262,  5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,\n",
       "             290,   389, 46852,  1028,   262, 11773,  4326,   366,  1279,  2954,\n",
       "              29, 12552,   366,   764,   220,   198, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256],\n",
       "          [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256]])),\n",
       " (tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256]),\n",
       "  tensor([  569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256])))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dataset[:2], tr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WikitextDataset(Dataset):\n",
    "#     def __init__(self, dataset, max_length=512):\n",
    "#         x = dataset['input_ids']\n",
    "#         size, _ = x.shape\n",
    "#         y = torch.empty((size, max_length))\n",
    "#         y[:,:-1] = x[:,1:]\n",
    "#         y[:,-1] = torch.ones(size) * 50256\n",
    "#         self.data = torch.cat((x.unsqueeze(2), y.unsqueeze(2)), dim=2)\n",
    "#         self.data = self.data.reshape((size, 2, max_length))\n",
    "#         self.data = self.data.to(torch.int64)\n",
    "#         print(self.data.shape)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x, y = self.data[idx]\n",
    "#         return x, y\n",
    "\n",
    "# tr_dataset = WikitextDataset(train_dataset, max_length)\n",
    "# ev_dataset = WikitextDataset(eval_dataset, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_wikitext_data()\n",
    "tr_dataset = data['train']\n",
    "ev_dataset = data['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565850112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import psutil\n",
    "\n",
    "gc.collect()\n",
    "process = psutil.Process()\n",
    "\n",
    "def get_mem():\n",
    "    return process.memory_info().rss \n",
    "\n",
    "get_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = config['lr']\n",
    "train_config.max_iters = config['max_iters']\n",
    "train_config.batch_size = config['batch_size']\n",
    "train_config.seq_length = config['seq_length']\n",
    "train_config.num_workers = config['num_workers']\n",
    "trainer = Trainer(train_config, model, tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev_loader = DataLoader(\n",
    "#     ev_dataset,\n",
    "#     shuffle=False,\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def custom_evaluate(model, device):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     for batch in ev_loader:\n",
    "#         batch = [t.to(device) for t in batch]\n",
    "#         x, y = batch\n",
    "#         logits, loss = model(x, y)\n",
    "#         losses.append(loss.item())\n",
    "#     model.train()\n",
    "#     return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeacefulotter\u001b[0m (\u001b[33motters-gang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/edge/epfl/ma3/disco/gpt-pytorch/wandb/run-20231031_104237-99z6yoyk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/otters-gang/disco-gpt-benchmark/runs/99z6yoyk' target=\"_blank\">mingpt_gpt-nano_wikitext_bs=16_seq=256_lr=0.001_iter=3600</a></strong> to <a href='https://wandb.ai/otters-gang/disco-gpt-benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/otters-gang/disco-gpt-benchmark' target=\"_blank\">https://wandb.ai/otters-gang/disco-gpt-benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/otters-gang/disco-gpt-benchmark/runs/99z6yoyk' target=\"_blank\">https://wandb.ai/otters-gang/disco-gpt-benchmark/runs/99z6yoyk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: 0.000, iter: 00000, train_loss: 10.8374, eval_loss: 0.0000, mem: 1555.62 MB\n",
      "DT: 1.204, iter: 00001, train_loss: 10.7881, eval_loss: 0.0000, mem: 1682.58 MB\n",
      "DT: 1.445, iter: 00002, train_loss: 10.7253, eval_loss: 0.0000, mem: 1683.66 MB\n",
      "DT: 1.198, iter: 00003, train_loss: 10.6819, eval_loss: 0.0000, mem: 1684.47 MB\n",
      "DT: 1.222, iter: 00004, train_loss: 10.6323, eval_loss: 0.0000, mem: 1685.57 MB\n",
      "DT: 1.240, iter: 00005, train_loss: 10.5857, eval_loss: 0.0000, mem: 1686.58 MB\n",
      "DT: 1.326, iter: 00006, train_loss: 10.5292, eval_loss: 0.0000, mem: 1687.52 MB\n",
      "DT: 1.129, iter: 00007, train_loss: 10.4868, eval_loss: 0.0000, mem: 1650.18 MB\n",
      "DT: 1.133, iter: 00008, train_loss: 10.4268, eval_loss: 0.0000, mem: 1699.12 MB\n",
      "DT: 1.004, iter: 00009, train_loss: 10.3765, eval_loss: 0.0000, mem: 1736.18 MB\n",
      "DT: 1.104, iter: 00010, train_loss: 10.3270, eval_loss: 0.0000, mem: 1700.40 MB\n",
      "DT: 1.117, iter: 00011, train_loss: 10.2586, eval_loss: 0.0000, mem: 1701.59 MB\n",
      "DT: 1.086, iter: 00012, train_loss: 10.1955, eval_loss: 0.0000, mem: 1702.49 MB\n",
      "DT: 1.211, iter: 00013, train_loss: 10.1443, eval_loss: 0.0000, mem: 1672.68 MB\n",
      "DT: 1.192, iter: 00014, train_loss: 10.0973, eval_loss: 0.0000, mem: 1682.50 MB\n",
      "DT: 1.256, iter: 00015, train_loss: 10.0241, eval_loss: 0.0000, mem: 1767.56 MB\n",
      "DT: 1.166, iter: 00016, train_loss: 9.9842, eval_loss: 0.0000, mem: 1743.92 MB\n",
      "DT: 1.187, iter: 00017, train_loss: 9.9289, eval_loss: 0.0000, mem: 1744.93 MB\n",
      "DT: 1.324, iter: 00018, train_loss: 9.8863, eval_loss: 0.0000, mem: 1745.93 MB\n",
      "DT: 1.176, iter: 00019, train_loss: 9.8136, eval_loss: 0.0000, mem: 1752.25 MB\n",
      "DT: 1.182, iter: 00020, train_loss: 9.7497, eval_loss: 0.0000, mem: 1753.19 MB\n",
      "DT: 1.242, iter: 00021, train_loss: 9.7411, eval_loss: 0.0000, mem: 1753.89 MB\n",
      "DT: 1.274, iter: 00022, train_loss: 9.6578, eval_loss: 0.0000, mem: 1626.82 MB\n",
      "DT: 1.257, iter: 00023, train_loss: 9.5867, eval_loss: 0.0000, mem: 1675.66 MB\n",
      "DT: 1.241, iter: 00024, train_loss: 9.5405, eval_loss: 0.0000, mem: 1712.64 MB\n",
      "DT: 1.196, iter: 00025, train_loss: 9.4773, eval_loss: 0.0000, mem: 1689.07 MB\n",
      "DT: 1.226, iter: 00026, train_loss: 9.5092, eval_loss: 0.0000, mem: 1658.26 MB\n",
      "DT: 1.189, iter: 00027, train_loss: 9.4136, eval_loss: 0.0000, mem: 1719.08 MB\n",
      "DT: 1.200, iter: 00028, train_loss: 9.3723, eval_loss: 0.0000, mem: 1719.90 MB\n",
      "DT: 1.181, iter: 00029, train_loss: 9.3059, eval_loss: 0.0000, mem: 1720.94 MB\n",
      "DT: 1.205, iter: 00030, train_loss: 9.2409, eval_loss: 0.0000, mem: 1721.88 MB\n",
      "DT: 1.199, iter: 00031, train_loss: 9.2205, eval_loss: 0.0000, mem: 1722.55 MB\n",
      "DT: 1.220, iter: 00032, train_loss: 9.1554, eval_loss: 0.0000, mem: 1743.98 MB\n",
      "DT: 1.267, iter: 00033, train_loss: 9.1202, eval_loss: 0.0000, mem: 1720.45 MB\n",
      "DT: 1.200, iter: 00034, train_loss: 9.0718, eval_loss: 0.0000, mem: 1732.94 MB\n",
      "DT: 1.207, iter: 00035, train_loss: 8.9656, eval_loss: 0.0000, mem: 1733.81 MB\n",
      "DT: 1.249, iter: 00036, train_loss: 8.9498, eval_loss: 0.0000, mem: 1734.64 MB\n",
      "DT: 1.201, iter: 00037, train_loss: 8.9714, eval_loss: 0.0000, mem: 1735.53 MB\n",
      "DT: 1.245, iter: 00038, train_loss: 8.8576, eval_loss: 0.0000, mem: 1745.89 MB\n",
      "DT: 1.191, iter: 00039, train_loss: 8.8556, eval_loss: 0.0000, mem: 1721.98 MB\n",
      "DT: 1.189, iter: 00040, train_loss: 8.8019, eval_loss: 0.0000, mem: 1697.98 MB\n",
      "DT: 1.207, iter: 00041, train_loss: 8.7536, eval_loss: 0.0000, mem: 1698.73 MB\n",
      "DT: 1.183, iter: 00042, train_loss: 8.6838, eval_loss: 0.0000, mem: 1751.18 MB\n",
      "DT: 1.166, iter: 00043, train_loss: 8.6689, eval_loss: 0.0000, mem: 1680.11 MB\n",
      "DT: 1.183, iter: 00044, train_loss: 8.6446, eval_loss: 0.0000, mem: 1692.60 MB\n",
      "DT: 1.176, iter: 00045, train_loss: 8.6675, eval_loss: 0.0000, mem: 1717.63 MB\n",
      "DT: 1.184, iter: 00046, train_loss: 8.5600, eval_loss: 0.0000, mem: 1742.44 MB\n",
      "DT: 1.172, iter: 00047, train_loss: 8.5182, eval_loss: 0.0000, mem: 1755.24 MB\n",
      "DT: 1.198, iter: 00048, train_loss: 8.4725, eval_loss: 0.0000, mem: 1732.10 MB\n",
      "DT: 1.175, iter: 00049, train_loss: 8.5053, eval_loss: 0.0000, mem: 1697.02 MB\n",
      "DT: 1.179, iter: 00050, train_loss: 8.4630, eval_loss: 0.0000, mem: 1697.58 MB\n",
      "DT: 1.185, iter: 00051, train_loss: 8.3491, eval_loss: 0.0000, mem: 1686.39 MB\n",
      "DT: 1.192, iter: 00052, train_loss: 8.3324, eval_loss: 0.0000, mem: 1723.20 MB\n",
      "DT: 1.173, iter: 00053, train_loss: 8.3868, eval_loss: 0.0000, mem: 1748.14 MB\n",
      "DT: 1.310, iter: 00054, train_loss: 8.2645, eval_loss: 0.0000, mem: 1724.41 MB\n",
      "DT: 1.271, iter: 00055, train_loss: 8.2448, eval_loss: 0.0000, mem: 1797.19 MB\n",
      "DT: 1.182, iter: 00056, train_loss: 8.2168, eval_loss: 0.0000, mem: 1773.33 MB\n",
      "DT: 1.213, iter: 00057, train_loss: 8.1582, eval_loss: 0.0000, mem: 1774.14 MB\n",
      "DT: 1.202, iter: 00058, train_loss: 8.2101, eval_loss: 0.0000, mem: 1744.68 MB\n",
      "DT: 1.173, iter: 00059, train_loss: 8.0368, eval_loss: 0.0000, mem: 1745.45 MB\n",
      "DT: 1.320, iter: 00060, train_loss: 8.0468, eval_loss: 0.0000, mem: 1760.73 MB\n",
      "DT: 1.209, iter: 00061, train_loss: 8.0635, eval_loss: 0.0000, mem: 1761.36 MB\n",
      "DT: 1.217, iter: 00062, train_loss: 8.0548, eval_loss: 0.0000, mem: 1762.11 MB\n",
      "DT: 1.201, iter: 00063, train_loss: 8.0970, eval_loss: 0.0000, mem: 1762.83 MB\n",
      "DT: 1.251, iter: 00064, train_loss: 8.0019, eval_loss: 0.0000, mem: 1739.27 MB\n",
      "DT: 1.229, iter: 00065, train_loss: 7.9656, eval_loss: 0.0000, mem: 1739.83 MB\n",
      "DT: 1.220, iter: 00066, train_loss: 7.9374, eval_loss: 0.0000, mem: 1740.54 MB\n",
      "DT: 1.308, iter: 00067, train_loss: 7.9364, eval_loss: 0.0000, mem: 1741.23 MB\n",
      "DT: 1.218, iter: 00068, train_loss: 7.9649, eval_loss: 0.0000, mem: 1774.73 MB\n",
      "DT: 1.231, iter: 00069, train_loss: 7.8321, eval_loss: 0.0000, mem: 1738.99 MB\n",
      "DT: 1.290, iter: 00070, train_loss: 7.8635, eval_loss: 0.0000, mem: 1749.52 MB\n",
      "DT: 1.343, iter: 00071, train_loss: 7.7120, eval_loss: 0.0000, mem: 1822.28 MB\n",
      "DT: 1.220, iter: 00072, train_loss: 7.8077, eval_loss: 0.0000, mem: 1774.63 MB\n",
      "DT: 1.239, iter: 00073, train_loss: 7.8695, eval_loss: 0.0000, mem: 1676.96 MB\n",
      "DT: 1.548, iter: 00074, train_loss: 7.7389, eval_loss: 0.0000, mem: 1737.77 MB\n",
      "DT: 1.499, iter: 00075, train_loss: 7.7535, eval_loss: 0.0000, mem: 1689.79 MB\n",
      "DT: 1.422, iter: 00076, train_loss: 7.8958, eval_loss: 0.0000, mem: 1702.44 MB\n",
      "DT: 1.488, iter: 00077, train_loss: 7.7020, eval_loss: 0.0000, mem: 1739.00 MB\n",
      "DT: 1.251, iter: 00078, train_loss: 7.7854, eval_loss: 0.0000, mem: 1739.74 MB\n",
      "DT: 1.330, iter: 00079, train_loss: 7.7591, eval_loss: 0.0000, mem: 1716.22 MB\n",
      "DT: 1.247, iter: 00080, train_loss: 7.7016, eval_loss: 0.0000, mem: 1728.02 MB\n",
      "DT: 1.233, iter: 00081, train_loss: 7.6085, eval_loss: 0.0000, mem: 1737.95 MB\n",
      "DT: 1.267, iter: 00082, train_loss: 7.6568, eval_loss: 0.0000, mem: 1738.90 MB\n",
      "DT: 1.352, iter: 00083, train_loss: 7.5830, eval_loss: 0.0000, mem: 1762.59 MB\n",
      "DT: 1.301, iter: 00084, train_loss: 7.5633, eval_loss: 0.0000, mem: 1727.50 MB\n",
      "DT: 1.307, iter: 00085, train_loss: 7.5078, eval_loss: 0.0000, mem: 1728.06 MB\n",
      "DT: 1.299, iter: 00086, train_loss: 7.6711, eval_loss: 0.0000, mem: 1728.62 MB\n",
      "DT: 1.298, iter: 00087, train_loss: 7.4906, eval_loss: 0.0000, mem: 1729.31 MB\n",
      "DT: 1.444, iter: 00088, train_loss: 7.5248, eval_loss: 0.0000, mem: 1767.98 MB\n",
      "DT: 1.338, iter: 00089, train_loss: 7.4855, eval_loss: 0.0000, mem: 1694.88 MB\n",
      "DT: 1.266, iter: 00090, train_loss: 7.4081, eval_loss: 0.0000, mem: 1707.18 MB\n",
      "DT: 1.271, iter: 00091, train_loss: 7.5078, eval_loss: 0.0000, mem: 1755.75 MB\n",
      "DT: 1.284, iter: 00092, train_loss: 7.5499, eval_loss: 0.0000, mem: 1792.48 MB\n",
      "DT: 1.335, iter: 00093, train_loss: 7.4560, eval_loss: 0.0000, mem: 1741.38 MB\n",
      "DT: 1.294, iter: 00094, train_loss: 7.5237, eval_loss: 0.0000, mem: 1742.01 MB\n",
      "DT: 1.263, iter: 00095, train_loss: 7.4133, eval_loss: 0.0000, mem: 1743.00 MB\n",
      "DT: 1.275, iter: 00096, train_loss: 7.5128, eval_loss: 0.0000, mem: 1743.44 MB\n",
      "DT: 1.232, iter: 00097, train_loss: 7.3338, eval_loss: 0.0000, mem: 1811.01 MB\n",
      "DT: 1.290, iter: 00098, train_loss: 7.4133, eval_loss: 0.0000, mem: 1784.21 MB\n",
      "DT: 1.275, iter: 00099, train_loss: 7.4235, eval_loss: 0.0000, mem: 1785.02 MB\n",
      "DT: 1.290, iter: 00100, train_loss: 7.4195, eval_loss: 0.0000, mem: 1758.40 MB\n",
      "DT: 1.386, iter: 00101, train_loss: 7.3327, eval_loss: 0.0000, mem: 1794.09 MB\n",
      "DT: 1.345, iter: 00102, train_loss: 7.4260, eval_loss: 0.0000, mem: 1747.13 MB\n",
      "DT: 1.303, iter: 00103, train_loss: 7.3199, eval_loss: 0.0000, mem: 1759.55 MB\n",
      "DT: 1.362, iter: 00104, train_loss: 7.3174, eval_loss: 0.0000, mem: 1760.22 MB\n",
      "DT: 1.267, iter: 00105, train_loss: 7.3623, eval_loss: 0.0000, mem: 1760.84 MB\n",
      "DT: 1.312, iter: 00106, train_loss: 7.3814, eval_loss: 0.0000, mem: 1761.50 MB\n",
      "DT: 1.329, iter: 00107, train_loss: 7.3284, eval_loss: 0.0000, mem: 1790.78 MB\n",
      "DT: 1.377, iter: 00108, train_loss: 7.3464, eval_loss: 0.0000, mem: 1743.16 MB\n",
      "DT: 1.310, iter: 00109, train_loss: 7.3335, eval_loss: 0.0000, mem: 1743.57 MB\n",
      "DT: 1.512, iter: 00110, train_loss: 7.3445, eval_loss: 0.0000, mem: 1744.40 MB\n",
      "DT: 1.474, iter: 00111, train_loss: 7.4048, eval_loss: 0.0000, mem: 1744.86 MB\n",
      "DT: 1.326, iter: 00112, train_loss: 7.3420, eval_loss: 0.0000, mem: 1745.90 MB\n",
      "DT: 1.325, iter: 00113, train_loss: 7.3600, eval_loss: 0.0000, mem: 1770.10 MB\n",
      "DT: 1.266, iter: 00114, train_loss: 7.2057, eval_loss: 0.0000, mem: 1794.81 MB\n",
      "DT: 1.352, iter: 00115, train_loss: 7.2799, eval_loss: 0.0000, mem: 1795.25 MB\n",
      "DT: 1.298, iter: 00116, train_loss: 7.2689, eval_loss: 0.0000, mem: 1795.94 MB\n",
      "DT: 1.313, iter: 00117, train_loss: 7.3454, eval_loss: 0.0000, mem: 1796.59 MB\n",
      "DT: 1.336, iter: 00118, train_loss: 7.2400, eval_loss: 0.0000, mem: 1797.07 MB\n",
      "DT: 1.361, iter: 00119, train_loss: 7.3031, eval_loss: 0.0000, mem: 1764.33 MB\n",
      "DT: 1.354, iter: 00120, train_loss: 7.1974, eval_loss: 0.0000, mem: 1765.03 MB\n",
      "DT: 1.353, iter: 00121, train_loss: 7.1229, eval_loss: 0.0000, mem: 1765.59 MB\n",
      "DT: 1.346, iter: 00122, train_loss: 7.5254, eval_loss: 0.0000, mem: 1774.91 MB\n",
      "DT: 1.362, iter: 00123, train_loss: 7.3222, eval_loss: 0.0000, mem: 1809.91 MB\n",
      "DT: 1.354, iter: 00124, train_loss: 7.1528, eval_loss: 0.0000, mem: 1762.59 MB\n",
      "DT: 1.339, iter: 00125, train_loss: 7.1176, eval_loss: 0.0000, mem: 1764.60 MB\n",
      "DT: 1.390, iter: 00126, train_loss: 7.1419, eval_loss: 0.0000, mem: 1765.16 MB\n",
      "DT: 1.343, iter: 00127, train_loss: 7.1976, eval_loss: 0.0000, mem: 1812.63 MB\n",
      "DT: 1.372, iter: 00128, train_loss: 7.2023, eval_loss: 0.0000, mem: 1813.21 MB\n",
      "DT: 1.327, iter: 00129, train_loss: 7.0817, eval_loss: 0.0000, mem: 1787.86 MB\n",
      "DT: 1.356, iter: 00130, train_loss: 7.0847, eval_loss: 0.0000, mem: 1788.18 MB\n",
      "DT: 1.321, iter: 00131, train_loss: 7.0850, eval_loss: 0.0000, mem: 1788.86 MB\n",
      "DT: 1.319, iter: 00132, train_loss: 7.1855, eval_loss: 0.0000, mem: 1789.33 MB\n",
      "DT: 1.541, iter: 00133, train_loss: 7.1944, eval_loss: 0.0000, mem: 1789.64 MB\n",
      "DT: 1.401, iter: 00134, train_loss: 7.1393, eval_loss: 0.0000, mem: 1790.20 MB\n",
      "DT: 1.341, iter: 00135, train_loss: 6.9622, eval_loss: 0.0000, mem: 1790.89 MB\n",
      "DT: 1.396, iter: 00136, train_loss: 7.1821, eval_loss: 0.0000, mem: 1791.20 MB\n",
      "DT: 1.347, iter: 00137, train_loss: 7.1939, eval_loss: 0.0000, mem: 1791.67 MB\n",
      "DT: 1.360, iter: 00138, train_loss: 7.1201, eval_loss: 0.0000, mem: 1792.36 MB\n",
      "DT: 1.344, iter: 00139, train_loss: 7.1067, eval_loss: 0.0000, mem: 1872.80 MB\n",
      "DT: 1.390, iter: 00140, train_loss: 7.2226, eval_loss: 0.0000, mem: 1873.36 MB\n",
      "DT: 1.379, iter: 00141, train_loss: 7.2427, eval_loss: 0.0000, mem: 1873.80 MB\n",
      "DT: 1.405, iter: 00142, train_loss: 7.0009, eval_loss: 0.0000, mem: 1844.40 MB\n",
      "DT: 1.356, iter: 00143, train_loss: 7.0505, eval_loss: 0.0000, mem: 1845.21 MB\n",
      "DT: 1.339, iter: 00144, train_loss: 7.1880, eval_loss: 0.0000, mem: 1755.41 MB\n",
      "DT: 1.417, iter: 00145, train_loss: 7.1457, eval_loss: 0.0000, mem: 1756.15 MB\n",
      "DT: 1.370, iter: 00146, train_loss: 7.1423, eval_loss: 0.0000, mem: 1780.59 MB\n",
      "DT: 1.380, iter: 00147, train_loss: 7.0708, eval_loss: 0.0000, mem: 1805.12 MB\n",
      "DT: 1.389, iter: 00148, train_loss: 6.9914, eval_loss: 0.0000, mem: 1768.53 MB\n",
      "DT: 1.390, iter: 00149, train_loss: 6.9305, eval_loss: 0.0000, mem: 1768.84 MB\n",
      "DT: 1.392, iter: 00150, train_loss: 7.0274, eval_loss: 0.0000, mem: 1814.89 MB\n",
      "DT: 1.411, iter: 00151, train_loss: 7.0508, eval_loss: 0.0000, mem: 1750.97 MB\n",
      "DT: 1.456, iter: 00152, train_loss: 7.0410, eval_loss: 0.0000, mem: 1787.58 MB\n",
      "DT: 1.407, iter: 00153, train_loss: 6.8752, eval_loss: 0.0000, mem: 1787.87 MB\n",
      "DT: 1.451, iter: 00154, train_loss: 6.9635, eval_loss: 0.0000, mem: 1788.11 MB\n",
      "DT: 1.357, iter: 00155, train_loss: 7.0867, eval_loss: 0.0000, mem: 1788.92 MB\n",
      "DT: 1.468, iter: 00156, train_loss: 6.9442, eval_loss: 0.0000, mem: 1789.45 MB\n",
      "DT: 1.329, iter: 00157, train_loss: 6.8767, eval_loss: 0.0000, mem: 1862.91 MB\n",
      "DT: 1.376, iter: 00158, train_loss: 6.9433, eval_loss: 0.0000, mem: 1863.47 MB\n",
      "DT: 1.332, iter: 00159, train_loss: 6.8637, eval_loss: 0.0000, mem: 1863.84 MB\n",
      "DT: 1.405, iter: 00160, train_loss: 6.9484, eval_loss: 0.0000, mem: 1799.55 MB\n",
      "DT: 1.407, iter: 00161, train_loss: 7.0211, eval_loss: 0.0000, mem: 1800.18 MB\n",
      "DT: 1.360, iter: 00162, train_loss: 6.8307, eval_loss: 0.0000, mem: 1809.55 MB\n",
      "DT: 1.317, iter: 00163, train_loss: 6.8360, eval_loss: 0.0000, mem: 1809.99 MB\n",
      "DT: 1.379, iter: 00164, train_loss: 6.9629, eval_loss: 0.0000, mem: 1810.73 MB\n",
      "DT: 1.360, iter: 00165, train_loss: 6.8647, eval_loss: 0.0000, mem: 1811.07 MB\n",
      "DT: 1.349, iter: 00166, train_loss: 7.0062, eval_loss: 0.0000, mem: 1811.63 MB\n",
      "DT: 1.399, iter: 00167, train_loss: 6.9483, eval_loss: 0.0000, mem: 1812.19 MB\n",
      "DT: 1.421, iter: 00168, train_loss: 7.0679, eval_loss: 0.0000, mem: 1812.68 MB\n",
      "DT: 1.385, iter: 00169, train_loss: 6.8412, eval_loss: 0.0000, mem: 1813.02 MB\n",
      "DT: 1.398, iter: 00170, train_loss: 7.0729, eval_loss: 0.0000, mem: 1813.55 MB\n",
      "DT: 1.454, iter: 00171, train_loss: 6.9088, eval_loss: 0.0000, mem: 1814.16 MB\n",
      "DT: 1.426, iter: 00172, train_loss: 6.8939, eval_loss: 0.0000, mem: 1814.20 MB\n",
      "DT: 1.386, iter: 00173, train_loss: 6.8217, eval_loss: 0.0000, mem: 1814.64 MB\n",
      "DT: 1.382, iter: 00174, train_loss: 7.0131, eval_loss: 0.0000, mem: 1837.92 MB\n",
      "DT: 1.355, iter: 00175, train_loss: 6.9275, eval_loss: 0.0000, mem: 1838.36 MB\n",
      "DT: 1.417, iter: 00176, train_loss: 6.9327, eval_loss: 0.0000, mem: 1838.82 MB\n",
      "DT: 1.368, iter: 00177, train_loss: 6.7780, eval_loss: 0.0000, mem: 1839.49 MB\n",
      "DT: 1.368, iter: 00178, train_loss: 6.8480, eval_loss: 0.0000, mem: 1815.38 MB\n",
      "DT: 1.431, iter: 00179, train_loss: 7.0060, eval_loss: 0.0000, mem: 1815.70 MB\n",
      "DT: 1.389, iter: 00180, train_loss: 6.8172, eval_loss: 0.0000, mem: 1828.79 MB\n",
      "DT: 1.408, iter: 00181, train_loss: 6.7999, eval_loss: 0.0000, mem: 1805.35 MB\n",
      "DT: 1.415, iter: 00182, train_loss: 6.7629, eval_loss: 0.0000, mem: 1805.45 MB\n",
      "DT: 1.392, iter: 00183, train_loss: 6.8514, eval_loss: 0.0000, mem: 1818.01 MB\n",
      "DT: 1.373, iter: 00184, train_loss: 6.6269, eval_loss: 0.0000, mem: 1818.20 MB\n",
      "DT: 1.435, iter: 00185, train_loss: 6.8898, eval_loss: 0.0000, mem: 1818.36 MB\n",
      "DT: 1.490, iter: 00186, train_loss: 6.7769, eval_loss: 0.0000, mem: 1747.98 MB\n",
      "DT: 1.397, iter: 00187, train_loss: 6.9272, eval_loss: 0.0000, mem: 1844.29 MB\n",
      "DT: 1.462, iter: 00188, train_loss: 6.7625, eval_loss: 0.0000, mem: 1845.11 MB\n",
      "DT: 1.439, iter: 00189, train_loss: 6.9025, eval_loss: 0.0000, mem: 1819.04 MB\n",
      "DT: 1.393, iter: 00190, train_loss: 6.7328, eval_loss: 0.0000, mem: 1843.59 MB\n",
      "DT: 1.493, iter: 00191, train_loss: 6.7885, eval_loss: 0.0000, mem: 1819.61 MB\n",
      "DT: 1.438, iter: 00192, train_loss: 6.8207, eval_loss: 0.0000, mem: 1819.67 MB\n",
      "DT: 1.444, iter: 00193, train_loss: 6.8473, eval_loss: 0.0000, mem: 1853.67 MB\n",
      "DT: 1.424, iter: 00194, train_loss: 6.6471, eval_loss: 0.0000, mem: 1818.16 MB\n",
      "DT: 1.470, iter: 00195, train_loss: 6.9615, eval_loss: 0.0000, mem: 1830.23 MB\n",
      "DT: 1.502, iter: 00196, train_loss: 6.8092, eval_loss: 0.0000, mem: 1831.15 MB\n",
      "DT: 1.434, iter: 00197, train_loss: 6.8075, eval_loss: 0.0000, mem: 1840.20 MB\n",
      "DT: 1.405, iter: 00198, train_loss: 6.8852, eval_loss: 0.0000, mem: 1840.88 MB\n",
      "DT: 1.470, iter: 00199, train_loss: 6.8319, eval_loss: 0.0000, mem: 1814.23 MB\n",
      "DT: 1.439, iter: 00200, train_loss: 6.8721, eval_loss: 0.0000, mem: 1814.67 MB\n",
      "DT: 1.409, iter: 00201, train_loss: 6.6280, eval_loss: 0.0000, mem: 1814.73 MB\n",
      "DT: 1.425, iter: 00202, train_loss: 6.7199, eval_loss: 0.0000, mem: 1815.05 MB\n",
      "DT: 1.426, iter: 00203, train_loss: 6.7995, eval_loss: 0.0000, mem: 1815.41 MB\n",
      "DT: 1.459, iter: 00204, train_loss: 6.8821, eval_loss: 0.0000, mem: 1891.80 MB\n",
      "DT: 1.513, iter: 00205, train_loss: 6.6961, eval_loss: 0.0000, mem: 1741.74 MB\n",
      "DT: 1.502, iter: 00206, train_loss: 6.7594, eval_loss: 0.0000, mem: 1813.81 MB\n",
      "DT: 1.425, iter: 00207, train_loss: 6.8617, eval_loss: 0.0000, mem: 1838.50 MB\n",
      "DT: 1.472, iter: 00208, train_loss: 6.7399, eval_loss: 0.0000, mem: 1778.65 MB\n",
      "DT: 1.361, iter: 00209, train_loss: 6.8793, eval_loss: 0.0000, mem: 1814.84 MB\n",
      "DT: 1.485, iter: 00210, train_loss: 6.8065, eval_loss: 0.0000, mem: 1779.23 MB\n",
      "DT: 1.407, iter: 00211, train_loss: 6.7869, eval_loss: 0.0000, mem: 1754.74 MB\n",
      "DT: 1.421, iter: 00212, train_loss: 6.5860, eval_loss: 0.0000, mem: 1826.90 MB\n",
      "DT: 1.433, iter: 00213, train_loss: 6.6528, eval_loss: 0.0000, mem: 1827.09 MB\n",
      "DT: 1.432, iter: 00214, train_loss: 6.6763, eval_loss: 0.0000, mem: 1770.93 MB\n",
      "DT: 1.447, iter: 00215, train_loss: 6.6269, eval_loss: 0.0000, mem: 1758.72 MB\n",
      "DT: 1.437, iter: 00216, train_loss: 6.7227, eval_loss: 0.0000, mem: 1795.30 MB\n",
      "DT: 1.441, iter: 00217, train_loss: 6.5088, eval_loss: 0.0000, mem: 1855.48 MB\n",
      "DT: 1.512, iter: 00218, train_loss: 6.5564, eval_loss: 0.0000, mem: 1879.55 MB\n",
      "DT: 1.512, iter: 00219, train_loss: 6.8009, eval_loss: 0.0000, mem: 1807.98 MB\n",
      "DT: 1.454, iter: 00220, train_loss: 6.5669, eval_loss: 0.0000, mem: 1808.31 MB\n",
      "DT: 1.464, iter: 00221, train_loss: 6.6192, eval_loss: 0.0000, mem: 1808.96 MB\n",
      "DT: 1.403, iter: 00222, train_loss: 6.5941, eval_loss: 0.0000, mem: 1809.03 MB\n",
      "DT: 1.617, iter: 00223, train_loss: 6.7222, eval_loss: 0.0000, mem: 1809.43 MB\n",
      "DT: 1.529, iter: 00224, train_loss: 6.5887, eval_loss: 0.0000, mem: 1821.27 MB\n",
      "DT: 1.500, iter: 00225, train_loss: 6.5565, eval_loss: 0.0000, mem: 1797.84 MB\n",
      "DT: 1.466, iter: 00226, train_loss: 6.4775, eval_loss: 0.0000, mem: 1798.28 MB\n",
      "DT: 1.501, iter: 00227, train_loss: 6.8058, eval_loss: 0.0000, mem: 1798.68 MB\n",
      "DT: 1.430, iter: 00228, train_loss: 6.6230, eval_loss: 0.0000, mem: 1774.76 MB\n",
      "DT: 1.567, iter: 00229, train_loss: 6.7138, eval_loss: 0.0000, mem: 1798.70 MB\n",
      "DT: 1.544, iter: 00230, train_loss: 6.4421, eval_loss: 0.0000, mem: 1895.59 MB\n",
      "DT: 1.578, iter: 00231, train_loss: 6.7352, eval_loss: 0.0000, mem: 1896.15 MB\n",
      "DT: 1.450, iter: 00232, train_loss: 6.5952, eval_loss: 0.0000, mem: 1896.14 MB\n",
      "DT: 1.389, iter: 00233, train_loss: 6.7226, eval_loss: 0.0000, mem: 1896.36 MB\n",
      "DT: 1.412, iter: 00234, train_loss: 6.6594, eval_loss: 0.0000, mem: 1846.05 MB\n",
      "DT: 1.416, iter: 00235, train_loss: 6.5439, eval_loss: 0.0000, mem: 1743.70 MB\n",
      "DT: 1.405, iter: 00236, train_loss: 6.6684, eval_loss: 0.0000, mem: 1786.53 MB\n",
      "DT: 1.482, iter: 00237, train_loss: 6.5924, eval_loss: 0.0000, mem: 1793.25 MB\n",
      "DT: 1.458, iter: 00238, train_loss: 6.3834, eval_loss: 0.0000, mem: 1806.41 MB\n",
      "DT: 1.395, iter: 00239, train_loss: 6.4277, eval_loss: 0.0000, mem: 1806.89 MB\n",
      "DT: 1.412, iter: 00240, train_loss: 6.6033, eval_loss: 0.0000, mem: 1795.10 MB\n",
      "DT: 1.447, iter: 00241, train_loss: 6.6305, eval_loss: 0.0000, mem: 1807.54 MB\n",
      "DT: 1.437, iter: 00242, train_loss: 6.8205, eval_loss: 0.0000, mem: 1831.82 MB\n",
      "DT: 1.468, iter: 00243, train_loss: 6.8174, eval_loss: 0.0000, mem: 1780.87 MB\n",
      "DT: 1.474, iter: 00244, train_loss: 6.6048, eval_loss: 0.0000, mem: 1781.13 MB\n",
      "DT: 1.544, iter: 00245, train_loss: 6.5447, eval_loss: 0.0000, mem: 1815.20 MB\n",
      "DT: 1.549, iter: 00246, train_loss: 6.7469, eval_loss: 0.0000, mem: 1875.64 MB\n",
      "DT: 1.528, iter: 00247, train_loss: 6.8393, eval_loss: 0.0000, mem: 1876.09 MB\n",
      "DT: 1.492, iter: 00248, train_loss: 6.5489, eval_loss: 0.0000, mem: 1876.53 MB\n",
      "DT: 1.457, iter: 00249, train_loss: 6.7021, eval_loss: 0.0000, mem: 1876.65 MB\n",
      "DT: 1.435, iter: 00250, train_loss: 6.6148, eval_loss: 0.0000, mem: 1826.99 MB\n",
      "DT: 1.474, iter: 00251, train_loss: 6.6426, eval_loss: 0.0000, mem: 1776.31 MB\n",
      "DT: 1.441, iter: 00252, train_loss: 6.6460, eval_loss: 0.0000, mem: 1847.75 MB\n",
      "DT: 1.418, iter: 00253, train_loss: 6.6622, eval_loss: 0.0000, mem: 1884.13 MB\n",
      "DT: 1.443, iter: 00254, train_loss: 6.4859, eval_loss: 0.0000, mem: 1859.89 MB\n",
      "DT: 1.424, iter: 00255, train_loss: 6.5601, eval_loss: 0.0000, mem: 1860.20 MB\n",
      "DT: 1.473, iter: 00256, train_loss: 6.5102, eval_loss: 0.0000, mem: 1860.36 MB\n",
      "DT: 1.472, iter: 00257, train_loss: 6.7350, eval_loss: 0.0000, mem: 1831.88 MB\n",
      "DT: 1.450, iter: 00258, train_loss: 6.7168, eval_loss: 0.0000, mem: 1832.38 MB\n",
      "DT: 1.491, iter: 00259, train_loss: 6.4802, eval_loss: 0.0000, mem: 1832.80 MB\n",
      "DT: 1.570, iter: 00260, train_loss: 6.7150, eval_loss: 0.0000, mem: 1833.36 MB\n",
      "DT: 1.481, iter: 00261, train_loss: 6.4844, eval_loss: 0.0000, mem: 1833.70 MB\n",
      "DT: 1.465, iter: 00262, train_loss: 6.5795, eval_loss: 0.0000, mem: 1833.95 MB\n",
      "DT: 1.492, iter: 00263, train_loss: 6.6203, eval_loss: 0.0000, mem: 1834.28 MB\n",
      "DT: 1.495, iter: 00264, train_loss: 6.6264, eval_loss: 0.0000, mem: 1834.47 MB\n",
      "DT: 1.455, iter: 00265, train_loss: 6.6632, eval_loss: 0.0000, mem: 1835.76 MB\n",
      "DT: 1.512, iter: 00266, train_loss: 6.6601, eval_loss: 0.0000, mem: 1802.45 MB\n",
      "DT: 1.515, iter: 00267, train_loss: 6.5469, eval_loss: 0.0000, mem: 1862.45 MB\n",
      "DT: 1.467, iter: 00268, train_loss: 6.6415, eval_loss: 0.0000, mem: 1898.78 MB\n",
      "DT: 1.505, iter: 00269, train_loss: 6.6512, eval_loss: 0.0000, mem: 1872.34 MB\n",
      "DT: 1.504, iter: 00270, train_loss: 6.4192, eval_loss: 0.0000, mem: 1838.97 MB\n",
      "DT: 1.450, iter: 00271, train_loss: 6.6640, eval_loss: 0.0000, mem: 1850.91 MB\n",
      "DT: 1.483, iter: 00272, train_loss: 6.6439, eval_loss: 0.0000, mem: 1857.10 MB\n",
      "DT: 1.464, iter: 00273, train_loss: 6.6445, eval_loss: 0.0000, mem: 1869.65 MB\n",
      "DT: 1.439, iter: 00274, train_loss: 6.4017, eval_loss: 0.0000, mem: 1845.34 MB\n",
      "DT: 1.512, iter: 00275, train_loss: 6.4626, eval_loss: 0.0000, mem: 1845.57 MB\n",
      "DT: 1.575, iter: 00276, train_loss: 6.6124, eval_loss: 0.0000, mem: 1845.88 MB\n",
      "DT: 1.504, iter: 00277, train_loss: 6.6233, eval_loss: 0.0000, mem: 1846.20 MB\n",
      "DT: 1.487, iter: 00278, train_loss: 6.5369, eval_loss: 0.0000, mem: 1846.51 MB\n",
      "DT: 1.484, iter: 00279, train_loss: 6.6688, eval_loss: 0.0000, mem: 1846.80 MB\n",
      "DT: 1.511, iter: 00280, train_loss: 6.5514, eval_loss: 0.0000, mem: 1846.73 MB\n",
      "DT: 1.494, iter: 00281, train_loss: 6.6257, eval_loss: 0.0000, mem: 1847.17 MB\n",
      "DT: 1.483, iter: 00282, train_loss: 6.7079, eval_loss: 0.0000, mem: 1858.08 MB\n",
      "DT: 1.598, iter: 00283, train_loss: 6.5267, eval_loss: 0.0000, mem: 1858.68 MB\n",
      "DT: 1.499, iter: 00284, train_loss: 6.3636, eval_loss: 0.0000, mem: 1858.62 MB\n",
      "DT: 1.495, iter: 00285, train_loss: 6.4642, eval_loss: 0.0000, mem: 1858.85 MB\n",
      "DT: 1.632, iter: 00286, train_loss: 6.5736, eval_loss: 0.0000, mem: 1859.13 MB\n",
      "DT: 1.527, iter: 00287, train_loss: 6.6833, eval_loss: 0.0000, mem: 1859.24 MB\n",
      "DT: 1.507, iter: 00288, train_loss: 6.6065, eval_loss: 0.0000, mem: 1859.80 MB\n",
      "DT: 1.589, iter: 00289, train_loss: 6.5927, eval_loss: 0.0000, mem: 1866.90 MB\n",
      "DT: 1.529, iter: 00290, train_loss: 6.4766, eval_loss: 0.0000, mem: 1867.02 MB\n",
      "DT: 1.490, iter: 00291, train_loss: 6.4975, eval_loss: 0.0000, mem: 1947.97 MB\n",
      "DT: 1.485, iter: 00292, train_loss: 6.6713, eval_loss: 0.0000, mem: 1924.12 MB\n",
      "DT: 1.485, iter: 00293, train_loss: 6.5242, eval_loss: 0.0000, mem: 1924.30 MB\n",
      "DT: 1.510, iter: 00294, train_loss: 6.6778, eval_loss: 0.0000, mem: 1924.63 MB\n",
      "DT: 1.467, iter: 00295, train_loss: 6.6041, eval_loss: 0.0000, mem: 1896.21 MB\n",
      "DT: 1.518, iter: 00296, train_loss: 6.4131, eval_loss: 0.0000, mem: 1896.44 MB\n",
      "DT: 1.569, iter: 00297, train_loss: 6.3510, eval_loss: 0.0000, mem: 1872.06 MB\n",
      "DT: 1.496, iter: 00298, train_loss: 6.4362, eval_loss: 0.0000, mem: 1872.37 MB\n",
      "DT: 1.502, iter: 00299, train_loss: 6.4862, eval_loss: 0.0000, mem: 1872.56 MB\n",
      "DT: 1.488, iter: 00300, train_loss: 6.5136, eval_loss: 0.0000, mem: 1848.62 MB\n",
      "DT: 1.509, iter: 00301, train_loss: 6.5087, eval_loss: 0.0000, mem: 1884.81 MB\n",
      "DT: 1.742, iter: 00302, train_loss: 6.4369, eval_loss: 0.0000, mem: 1885.29 MB\n",
      "DT: 1.520, iter: 00303, train_loss: 6.4084, eval_loss: 0.0000, mem: 1885.36 MB\n",
      "DT: 1.504, iter: 00304, train_loss: 6.5826, eval_loss: 0.0000, mem: 1885.89 MB\n",
      "DT: 1.474, iter: 00305, train_loss: 6.6372, eval_loss: 0.0000, mem: 1885.96 MB\n",
      "DT: 1.500, iter: 00306, train_loss: 6.2948, eval_loss: 0.0000, mem: 1860.39 MB\n",
      "DT: 1.494, iter: 00307, train_loss: 6.4281, eval_loss: 0.0000, mem: 1860.54 MB\n",
      "DT: 1.438, iter: 00308, train_loss: 6.3169, eval_loss: 0.0000, mem: 1860.76 MB\n",
      "DT: 1.515, iter: 00309, train_loss: 6.4903, eval_loss: 0.0000, mem: 1881.82 MB\n",
      "DT: 1.512, iter: 00310, train_loss: 6.4689, eval_loss: 0.0000, mem: 1858.23 MB\n",
      "DT: 1.493, iter: 00311, train_loss: 6.5883, eval_loss: 0.0000, mem: 1858.80 MB\n",
      "DT: 1.493, iter: 00312, train_loss: 6.5681, eval_loss: 0.0000, mem: 1869.18 MB\n",
      "DT: 1.487, iter: 00313, train_loss: 6.5352, eval_loss: 0.0000, mem: 1869.50 MB\n",
      "DT: 1.525, iter: 00314, train_loss: 6.4486, eval_loss: 0.0000, mem: 1869.31 MB\n",
      "DT: 1.527, iter: 00315, train_loss: 6.5128, eval_loss: 0.0000, mem: 1869.77 MB\n",
      "DT: 1.564, iter: 00316, train_loss: 6.4890, eval_loss: 0.0000, mem: 1869.98 MB\n",
      "DT: 1.490, iter: 00317, train_loss: 6.4782, eval_loss: 0.0000, mem: 1870.30 MB\n",
      "DT: 1.531, iter: 00318, train_loss: 6.6380, eval_loss: 0.0000, mem: 1880.40 MB\n",
      "DT: 1.506, iter: 00319, train_loss: 6.2659, eval_loss: 0.0000, mem: 1880.52 MB\n",
      "DT: 1.474, iter: 00320, train_loss: 6.5604, eval_loss: 0.0000, mem: 1880.78 MB\n",
      "DT: 1.625, iter: 00321, train_loss: 6.4622, eval_loss: 0.0000, mem: 1881.22 MB\n",
      "DT: 1.464, iter: 00322, train_loss: 6.3894, eval_loss: 0.0000, mem: 1889.16 MB\n",
      "DT: 1.530, iter: 00323, train_loss: 6.3352, eval_loss: 0.0000, mem: 1864.61 MB\n",
      "DT: 1.491, iter: 00324, train_loss: 6.4143, eval_loss: 0.0000, mem: 1864.68 MB\n",
      "DT: 1.492, iter: 00325, train_loss: 6.4645, eval_loss: 0.0000, mem: 1864.72 MB\n",
      "DT: 1.426, iter: 00326, train_loss: 6.3404, eval_loss: 0.0000, mem: 1864.93 MB\n",
      "DT: 1.446, iter: 00327, train_loss: 6.4591, eval_loss: 0.0000, mem: 1802.11 MB\n",
      "DT: 1.494, iter: 00328, train_loss: 6.4238, eval_loss: 0.0000, mem: 1838.23 MB\n",
      "DT: 1.475, iter: 00329, train_loss: 6.3975, eval_loss: 0.0000, mem: 1862.41 MB\n",
      "DT: 1.507, iter: 00330, train_loss: 6.2016, eval_loss: 0.0000, mem: 1862.52 MB\n",
      "DT: 1.501, iter: 00331, train_loss: 6.3971, eval_loss: 0.0000, mem: 1862.58 MB\n",
      "DT: 1.457, iter: 00332, train_loss: 6.4205, eval_loss: 0.0000, mem: 1862.89 MB\n",
      "DT: 1.504, iter: 00333, train_loss: 6.3923, eval_loss: 0.0000, mem: 1890.77 MB\n",
      "DT: 1.527, iter: 00334, train_loss: 6.4997, eval_loss: 0.0000, mem: 1891.17 MB\n",
      "DT: 1.438, iter: 00335, train_loss: 6.5595, eval_loss: 0.0000, mem: 1891.11 MB\n",
      "DT: 1.468, iter: 00336, train_loss: 6.3890, eval_loss: 0.0000, mem: 1891.45 MB\n",
      "DT: 1.516, iter: 00337, train_loss: 6.3634, eval_loss: 0.0000, mem: 1891.72 MB\n",
      "DT: 1.491, iter: 00338, train_loss: 6.3955, eval_loss: 0.0000, mem: 1891.91 MB\n",
      "DT: 1.503, iter: 00339, train_loss: 6.4523, eval_loss: 0.0000, mem: 1892.42 MB\n",
      "DT: 1.538, iter: 00340, train_loss: 6.3043, eval_loss: 0.0000, mem: 1892.82 MB\n",
      "DT: 1.515, iter: 00341, train_loss: 6.4177, eval_loss: 0.0000, mem: 1892.76 MB\n",
      "DT: 1.471, iter: 00342, train_loss: 6.4766, eval_loss: 0.0000, mem: 1821.17 MB\n",
      "DT: 1.580, iter: 00343, train_loss: 6.1953, eval_loss: 0.0000, mem: 1857.42 MB\n",
      "DT: 1.497, iter: 00344, train_loss: 6.4694, eval_loss: 0.0000, mem: 1857.58 MB\n",
      "DT: 1.656, iter: 00345, train_loss: 6.2907, eval_loss: 0.0000, mem: 1857.59 MB\n",
      "DT: 1.500, iter: 00346, train_loss: 6.3173, eval_loss: 0.0000, mem: 1796.67 MB\n",
      "DT: 1.536, iter: 00347, train_loss: 6.4767, eval_loss: 0.0000, mem: 1807.01 MB\n",
      "DT: 1.611, iter: 00348, train_loss: 6.4615, eval_loss: 0.0000, mem: 1927.43 MB\n",
      "DT: 1.528, iter: 00349, train_loss: 6.3886, eval_loss: 0.0000, mem: 1807.67 MB\n",
      "DT: 1.538, iter: 00350, train_loss: 6.4087, eval_loss: 0.0000, mem: 1807.82 MB\n",
      "DT: 1.481, iter: 00351, train_loss: 6.4843, eval_loss: 0.0000, mem: 1808.13 MB\n",
      "DT: 1.490, iter: 00352, train_loss: 6.4915, eval_loss: 0.0000, mem: 1797.59 MB\n",
      "DT: 1.525, iter: 00353, train_loss: 6.3283, eval_loss: 0.0000, mem: 1869.14 MB\n",
      "DT: 1.536, iter: 00354, train_loss: 6.4537, eval_loss: 0.0000, mem: 1869.52 MB\n",
      "DT: 1.558, iter: 00355, train_loss: 6.3455, eval_loss: 0.0000, mem: 1869.83 MB\n",
      "DT: 1.471, iter: 00356, train_loss: 6.2958, eval_loss: 0.0000, mem: 1870.02 MB\n",
      "DT: 1.589, iter: 00357, train_loss: 6.3826, eval_loss: 0.0000, mem: 1870.08 MB\n",
      "DT: 1.506, iter: 00358, train_loss: 6.4766, eval_loss: 0.0000, mem: 1870.45 MB\n",
      "DT: 1.611, iter: 00359, train_loss: 6.2751, eval_loss: 0.0000, mem: 1870.72 MB\n",
      "DT: 1.554, iter: 00360, train_loss: 6.5052, eval_loss: 0.0000, mem: 1870.79 MB\n",
      "DT: 1.475, iter: 00361, train_loss: 6.3953, eval_loss: 0.0000, mem: 1831.05 MB\n",
      "DT: 1.509, iter: 00362, train_loss: 6.3416, eval_loss: 0.0000, mem: 1830.98 MB\n",
      "DT: 1.561, iter: 00363, train_loss: 6.4924, eval_loss: 0.0000, mem: 1831.27 MB\n",
      "DT: 1.548, iter: 00364, train_loss: 6.3528, eval_loss: 0.0000, mem: 1805.01 MB\n",
      "DT: 1.466, iter: 00365, train_loss: 6.2546, eval_loss: 0.0000, mem: 1841.34 MB\n",
      "DT: 1.554, iter: 00366, train_loss: 6.4565, eval_loss: 0.0000, mem: 1841.35 MB\n",
      "DT: 1.509, iter: 00367, train_loss: 6.2622, eval_loss: 0.0000, mem: 1841.88 MB\n",
      "DT: 1.570, iter: 00368, train_loss: 6.4442, eval_loss: 0.0000, mem: 1811.39 MB\n",
      "DT: 1.503, iter: 00369, train_loss: 6.1051, eval_loss: 0.0000, mem: 1811.45 MB\n",
      "DT: 1.577, iter: 00370, train_loss: 6.5539, eval_loss: 0.0000, mem: 1811.77 MB\n",
      "DT: 1.564, iter: 00371, train_loss: 6.2089, eval_loss: 0.0000, mem: 1811.70 MB\n",
      "DT: 1.500, iter: 00372, train_loss: 6.4038, eval_loss: 0.0000, mem: 1842.27 MB\n",
      "DT: 1.516, iter: 00373, train_loss: 6.4217, eval_loss: 0.0000, mem: 1842.58 MB\n",
      "DT: 1.519, iter: 00374, train_loss: 6.2733, eval_loss: 0.0000, mem: 1842.77 MB\n",
      "DT: 1.500, iter: 00375, train_loss: 6.2204, eval_loss: 0.0000, mem: 1850.41 MB\n",
      "DT: 1.524, iter: 00376, train_loss: 6.4727, eval_loss: 0.0000, mem: 1886.94 MB\n",
      "DT: 1.587, iter: 00377, train_loss: 6.5189, eval_loss: 0.0000, mem: 1845.25 MB\n",
      "DT: 1.540, iter: 00378, train_loss: 6.2807, eval_loss: 0.0000, mem: 1869.38 MB\n",
      "DT: 1.582, iter: 00379, train_loss: 6.5302, eval_loss: 0.0000, mem: 1869.69 MB\n",
      "DT: 1.631, iter: 00380, train_loss: 6.3051, eval_loss: 0.0000, mem: 1869.88 MB\n",
      "DT: 1.494, iter: 00381, train_loss: 6.2125, eval_loss: 0.0000, mem: 1939.00 MB\n",
      "DT: 1.507, iter: 00382, train_loss: 6.3600, eval_loss: 0.0000, mem: 1828.33 MB\n",
      "DT: 1.714, iter: 00383, train_loss: 6.4490, eval_loss: 0.0000, mem: 1828.75 MB\n",
      "DT: 1.703, iter: 00384, train_loss: 6.2637, eval_loss: 0.0000, mem: 1876.57 MB\n",
      "DT: 1.599, iter: 00385, train_loss: 6.2769, eval_loss: 0.0000, mem: 1912.96 MB\n",
      "DT: 1.546, iter: 00386, train_loss: 6.3600, eval_loss: 0.0000, mem: 1886.27 MB\n",
      "DT: 1.497, iter: 00387, train_loss: 6.3618, eval_loss: 0.0000, mem: 1829.46 MB\n",
      "DT: 1.659, iter: 00388, train_loss: 6.3916, eval_loss: 0.0000, mem: 1841.70 MB\n",
      "DT: 1.462, iter: 00389, train_loss: 6.4968, eval_loss: 0.0000, mem: 1853.76 MB\n",
      "DT: 1.459, iter: 00390, train_loss: 6.5047, eval_loss: 0.0000, mem: 1853.88 MB\n",
      "DT: 1.483, iter: 00391, train_loss: 6.3403, eval_loss: 0.0000, mem: 1876.39 MB\n",
      "DT: 1.655, iter: 00392, train_loss: 6.3677, eval_loss: 0.0000, mem: 1876.70 MB\n",
      "DT: 1.618, iter: 00393, train_loss: 6.1858, eval_loss: 0.0000, mem: 1876.76 MB\n",
      "DT: 1.545, iter: 00394, train_loss: 6.4133, eval_loss: 0.0000, mem: 1876.61 MB\n",
      "DT: 1.579, iter: 00395, train_loss: 6.4769, eval_loss: 0.0000, mem: 1851.14 MB\n",
      "DT: 1.670, iter: 00396, train_loss: 6.4718, eval_loss: 0.0000, mem: 1851.65 MB\n",
      "DT: 1.515, iter: 00397, train_loss: 6.5251, eval_loss: 0.0000, mem: 1851.84 MB\n",
      "DT: 1.552, iter: 00398, train_loss: 6.4399, eval_loss: 0.0000, mem: 1851.65 MB\n",
      "DT: 1.599, iter: 00399, train_loss: 6.3106, eval_loss: 0.0000, mem: 1851.59 MB\n",
      "DT: 1.633, iter: 00400, train_loss: 6.4298, eval_loss: 0.0000, mem: 1851.78 MB\n",
      "DT: 1.520, iter: 00401, train_loss: 6.3989, eval_loss: 0.0000, mem: 1851.86 MB\n",
      "DT: 1.523, iter: 00402, train_loss: 6.5130, eval_loss: 0.0000, mem: 1851.92 MB\n",
      "DT: 1.549, iter: 00403, train_loss: 6.2833, eval_loss: 0.0000, mem: 1852.25 MB\n",
      "DT: 1.467, iter: 00404, train_loss: 6.5537, eval_loss: 0.0000, mem: 1910.95 MB\n",
      "DT: 1.542, iter: 00405, train_loss: 6.2653, eval_loss: 0.0000, mem: 1863.01 MB\n",
      "DT: 1.503, iter: 00406, train_loss: 6.1831, eval_loss: 0.0000, mem: 1863.09 MB\n",
      "DT: 1.434, iter: 00407, train_loss: 6.3293, eval_loss: 0.0000, mem: 1908.13 MB\n",
      "DT: 1.491, iter: 00408, train_loss: 6.1189, eval_loss: 0.0000, mem: 1860.36 MB\n",
      "DT: 1.569, iter: 00409, train_loss: 6.2224, eval_loss: 0.0000, mem: 1860.78 MB\n",
      "DT: 1.644, iter: 00410, train_loss: 6.4625, eval_loss: 0.0000, mem: 1920.55 MB\n",
      "DT: 1.554, iter: 00411, train_loss: 6.1978, eval_loss: 0.0000, mem: 1848.80 MB\n",
      "DT: 1.512, iter: 00412, train_loss: 6.2763, eval_loss: 0.0000, mem: 1848.77 MB\n",
      "DT: 1.516, iter: 00413, train_loss: 6.3187, eval_loss: 0.0000, mem: 1895.28 MB\n",
      "DT: 1.554, iter: 00414, train_loss: 6.1791, eval_loss: 0.0000, mem: 1871.25 MB\n",
      "DT: 1.617, iter: 00415, train_loss: 6.6064, eval_loss: 0.0000, mem: 1871.19 MB\n",
      "DT: 1.602, iter: 00416, train_loss: 6.5215, eval_loss: 0.0000, mem: 1871.57 MB\n",
      "DT: 1.492, iter: 00417, train_loss: 6.2824, eval_loss: 0.0000, mem: 1916.42 MB\n",
      "DT: 1.533, iter: 00418, train_loss: 6.2308, eval_loss: 0.0000, mem: 1867.71 MB\n",
      "DT: 1.548, iter: 00419, train_loss: 6.1595, eval_loss: 0.0000, mem: 1868.02 MB\n",
      "DT: 1.532, iter: 00420, train_loss: 6.2476, eval_loss: 0.0000, mem: 1867.96 MB\n",
      "DT: 1.584, iter: 00421, train_loss: 6.1489, eval_loss: 0.0000, mem: 1868.12 MB\n",
      "DT: 1.587, iter: 00422, train_loss: 6.2321, eval_loss: 0.0000, mem: 1868.51 MB\n",
      "DT: 1.676, iter: 00423, train_loss: 6.3932, eval_loss: 0.0000, mem: 1868.51 MB\n",
      "DT: 1.565, iter: 00424, train_loss: 6.3118, eval_loss: 0.0000, mem: 1904.62 MB\n",
      "DT: 1.528, iter: 00425, train_loss: 6.4662, eval_loss: 0.0000, mem: 1904.86 MB\n",
      "DT: 1.599, iter: 00426, train_loss: 6.3398, eval_loss: 0.0000, mem: 1904.93 MB\n",
      "DT: 1.513, iter: 00427, train_loss: 6.4098, eval_loss: 0.0000, mem: 1904.98 MB\n",
      "DT: 1.523, iter: 00428, train_loss: 6.3452, eval_loss: 0.0000, mem: 1854.41 MB\n",
      "DT: 1.578, iter: 00429, train_loss: 6.1782, eval_loss: 0.0000, mem: 1866.48 MB\n",
      "DT: 1.498, iter: 00430, train_loss: 6.2548, eval_loss: 0.0000, mem: 1890.73 MB\n",
      "DT: 1.521, iter: 00431, train_loss: 6.3608, eval_loss: 0.0000, mem: 1839.66 MB\n",
      "DT: 1.665, iter: 00432, train_loss: 6.1708, eval_loss: 0.0000, mem: 1884.65 MB\n",
      "DT: 1.515, iter: 00433, train_loss: 6.1081, eval_loss: 0.0000, mem: 1854.92 MB\n",
      "DT: 1.471, iter: 00434, train_loss: 6.1088, eval_loss: 0.0000, mem: 1902.95 MB\n",
      "DT: 1.481, iter: 00435, train_loss: 6.3395, eval_loss: 0.0000, mem: 1926.89 MB\n",
      "DT: 1.669, iter: 00436, train_loss: 6.1664, eval_loss: 0.0000, mem: 1878.31 MB\n",
      "DT: 1.549, iter: 00437, train_loss: 6.3035, eval_loss: 0.0000, mem: 1854.80 MB\n",
      "DT: 1.669, iter: 00438, train_loss: 6.2211, eval_loss: 0.0000, mem: 1866.90 MB\n",
      "DT: 1.577, iter: 00439, train_loss: 6.3970, eval_loss: 0.0000, mem: 1902.86 MB\n",
      "DT: 1.465, iter: 00440, train_loss: 6.3848, eval_loss: 0.0000, mem: 1854.92 MB\n",
      "DT: 1.524, iter: 00441, train_loss: 6.3334, eval_loss: 0.0000, mem: 1855.23 MB\n",
      "DT: 1.516, iter: 00442, train_loss: 6.3109, eval_loss: 0.0000, mem: 1855.17 MB\n",
      "DT: 1.582, iter: 00443, train_loss: 6.5620, eval_loss: 0.0000, mem: 1855.43 MB\n",
      "DT: 1.539, iter: 00444, train_loss: 6.1340, eval_loss: 0.0000, mem: 1855.75 MB\n",
      "DT: 1.535, iter: 00445, train_loss: 6.3077, eval_loss: 0.0000, mem: 1904.06 MB\n",
      "DT: 1.531, iter: 00446, train_loss: 6.3069, eval_loss: 0.0000, mem: 1976.12 MB\n",
      "DT: 1.542, iter: 00447, train_loss: 6.2567, eval_loss: 0.0000, mem: 1976.31 MB\n",
      "DT: 1.545, iter: 00448, train_loss: 6.2348, eval_loss: 0.0000, mem: 1976.55 MB\n",
      "DT: 1.655, iter: 00449, train_loss: 6.3853, eval_loss: 0.0000, mem: 1859.47 MB\n",
      "DT: 1.654, iter: 00450, train_loss: 6.5090, eval_loss: 0.0000, mem: 1889.86 MB\n",
      "DT: 1.576, iter: 00451, train_loss: 6.4528, eval_loss: 0.0000, mem: 1889.93 MB\n",
      "DT: 1.688, iter: 00452, train_loss: 6.3530, eval_loss: 0.0000, mem: 1830.76 MB\n",
      "DT: 1.625, iter: 00453, train_loss: 6.3232, eval_loss: 0.0000, mem: 1878.69 MB\n",
      "DT: 1.581, iter: 00454, train_loss: 6.0468, eval_loss: 0.0000, mem: 1938.93 MB\n",
      "DT: 1.571, iter: 00455, train_loss: 6.0120, eval_loss: 0.0000, mem: 1843.05 MB\n",
      "DT: 1.552, iter: 00456, train_loss: 6.2381, eval_loss: 0.0000, mem: 1843.36 MB\n",
      "DT: 1.530, iter: 00457, train_loss: 6.1815, eval_loss: 0.0000, mem: 1867.28 MB\n",
      "DT: 1.545, iter: 00458, train_loss: 6.4199, eval_loss: 0.0000, mem: 1842.73 MB\n",
      "DT: 1.525, iter: 00459, train_loss: 6.2824, eval_loss: 0.0000, mem: 1938.80 MB\n",
      "DT: 1.503, iter: 00460, train_loss: 6.1376, eval_loss: 0.0000, mem: 1938.96 MB\n",
      "DT: 1.491, iter: 00461, train_loss: 6.0977, eval_loss: 0.0000, mem: 1877.85 MB\n",
      "DT: 1.587, iter: 00462, train_loss: 6.3319, eval_loss: 0.0000, mem: 1901.94 MB\n",
      "DT: 1.519, iter: 00463, train_loss: 6.2577, eval_loss: 0.0000, mem: 1902.21 MB\n",
      "DT: 1.561, iter: 00464, train_loss: 6.1511, eval_loss: 0.0000, mem: 1902.52 MB\n",
      "DT: 1.631, iter: 00465, train_loss: 6.1307, eval_loss: 0.0000, mem: 1876.46 MB\n",
      "DT: 1.593, iter: 00466, train_loss: 6.0516, eval_loss: 0.0000, mem: 1876.39 MB\n",
      "DT: 1.570, iter: 00467, train_loss: 6.2118, eval_loss: 0.0000, mem: 1876.54 MB\n",
      "DT: 1.671, iter: 00468, train_loss: 6.1872, eval_loss: 0.0000, mem: 1883.36 MB\n",
      "DT: 1.528, iter: 00469, train_loss: 6.2854, eval_loss: 0.0000, mem: 1919.66 MB\n",
      "DT: 1.575, iter: 00470, train_loss: 6.1519, eval_loss: 0.0000, mem: 1895.45 MB\n",
      "DT: 1.594, iter: 00471, train_loss: 6.1530, eval_loss: 0.0000, mem: 1919.39 MB\n",
      "DT: 1.589, iter: 00472, train_loss: 6.3761, eval_loss: 0.0000, mem: 1893.84 MB\n",
      "DT: 1.545, iter: 00473, train_loss: 6.2417, eval_loss: 0.0000, mem: 1819.54 MB\n",
      "DT: 1.486, iter: 00474, train_loss: 6.4128, eval_loss: 0.0000, mem: 1840.53 MB\n",
      "DT: 1.632, iter: 00475, train_loss: 6.3006, eval_loss: 0.0000, mem: 1841.20 MB\n",
      "DT: 1.480, iter: 00476, train_loss: 6.1995, eval_loss: 0.0000, mem: 1865.22 MB\n",
      "DT: 1.549, iter: 00477, train_loss: 6.1798, eval_loss: 0.0000, mem: 1799.57 MB\n",
      "DT: 1.567, iter: 00478, train_loss: 6.2571, eval_loss: 0.0000, mem: 1835.41 MB\n",
      "DT: 1.544, iter: 00479, train_loss: 6.2218, eval_loss: 0.0000, mem: 1872.13 MB\n",
      "DT: 1.609, iter: 00480, train_loss: 6.2526, eval_loss: 0.0000, mem: 1846.15 MB\n",
      "DT: 1.553, iter: 00481, train_loss: 6.2225, eval_loss: 0.0000, mem: 1846.03 MB\n",
      "DT: 1.594, iter: 00482, train_loss: 6.1907, eval_loss: 0.0000, mem: 1899.47 MB\n",
      "DT: 1.567, iter: 00483, train_loss: 6.1945, eval_loss: 0.0000, mem: 1851.14 MB\n",
      "DT: 1.555, iter: 00484, train_loss: 6.2451, eval_loss: 0.0000, mem: 1825.25 MB\n",
      "DT: 1.613, iter: 00485, train_loss: 6.2613, eval_loss: 0.0000, mem: 1825.45 MB\n",
      "DT: 1.545, iter: 00486, train_loss: 6.2591, eval_loss: 0.0000, mem: 1883.96 MB\n",
      "DT: 1.520, iter: 00487, train_loss: 6.4822, eval_loss: 0.0000, mem: 1908.40 MB\n",
      "DT: 1.608, iter: 00488, train_loss: 6.4073, eval_loss: 0.0000, mem: 1908.32 MB\n",
      "DT: 1.634, iter: 00489, train_loss: 6.2595, eval_loss: 0.0000, mem: 1908.45 MB\n",
      "DT: 1.606, iter: 00490, train_loss: 6.2383, eval_loss: 0.0000, mem: 1883.04 MB\n",
      "DT: 1.659, iter: 00491, train_loss: 6.2126, eval_loss: 0.0000, mem: 1883.17 MB\n",
      "DT: 1.560, iter: 00492, train_loss: 6.0385, eval_loss: 0.0000, mem: 1846.61 MB\n",
      "DT: 1.526, iter: 00493, train_loss: 6.1389, eval_loss: 0.0000, mem: 1858.29 MB\n",
      "DT: 1.556, iter: 00494, train_loss: 6.1510, eval_loss: 0.0000, mem: 1858.74 MB\n",
      "DT: 1.540, iter: 00495, train_loss: 6.2297, eval_loss: 0.0000, mem: 1870.44 MB\n",
      "DT: 1.664, iter: 00496, train_loss: 6.2919, eval_loss: 0.0000, mem: 1942.65 MB\n",
      "DT: 1.533, iter: 00497, train_loss: 6.2435, eval_loss: 0.0000, mem: 1918.29 MB\n",
      "DT: 1.577, iter: 00498, train_loss: 6.2915, eval_loss: 0.0000, mem: 1856.72 MB\n",
      "DT: 1.582, iter: 00499, train_loss: 6.5061, eval_loss: 0.0000, mem: 1891.94 MB\n",
      "DT: 1.567, iter: 00500, train_loss: 6.2891, eval_loss: 0.0000, mem: 1892.06 MB\n",
      "DT: 1.628, iter: 00501, train_loss: 6.1470, eval_loss: 0.0000, mem: 1892.15 MB\n",
      "DT: 1.555, iter: 00502, train_loss: 6.3807, eval_loss: 0.0000, mem: 1892.34 MB\n",
      "DT: 1.534, iter: 00503, train_loss: 6.2492, eval_loss: 0.0000, mem: 1892.44 MB\n",
      "DT: 1.452, iter: 00504, train_loss: 6.1857, eval_loss: 0.0000, mem: 1923.49 MB\n",
      "DT: 1.505, iter: 00505, train_loss: 6.2874, eval_loss: 0.0000, mem: 1872.63 MB\n",
      "DT: 1.542, iter: 00506, train_loss: 6.1283, eval_loss: 0.0000, mem: 1872.70 MB\n",
      "DT: 1.538, iter: 00507, train_loss: 6.1241, eval_loss: 0.0000, mem: 1878.76 MB\n",
      "DT: 1.544, iter: 00508, train_loss: 6.1119, eval_loss: 0.0000, mem: 1850.95 MB\n",
      "DT: 1.526, iter: 00509, train_loss: 6.3290, eval_loss: 0.0000, mem: 1863.21 MB\n",
      "DT: 1.503, iter: 00510, train_loss: 6.3732, eval_loss: 0.0000, mem: 1887.32 MB\n",
      "DT: 1.532, iter: 00511, train_loss: 6.2137, eval_loss: 0.0000, mem: 1887.16 MB\n",
      "DT: 1.548, iter: 00512, train_loss: 5.8971, eval_loss: 0.0000, mem: 1887.45 MB\n",
      "DT: 1.538, iter: 00513, train_loss: 6.1748, eval_loss: 0.0000, mem: 1887.76 MB\n",
      "DT: 1.543, iter: 00514, train_loss: 6.1794, eval_loss: 0.0000, mem: 1917.07 MB\n",
      "DT: 1.519, iter: 00515, train_loss: 6.0331, eval_loss: 0.0000, mem: 1864.19 MB\n",
      "DT: 1.609, iter: 00516, train_loss: 6.0603, eval_loss: 0.0000, mem: 1864.16 MB\n",
      "DT: 1.549, iter: 00517, train_loss: 6.2905, eval_loss: 0.0000, mem: 1887.31 MB\n",
      "DT: 1.540, iter: 00518, train_loss: 6.2530, eval_loss: 0.0000, mem: 1863.38 MB\n",
      "DT: 1.657, iter: 00519, train_loss: 6.3535, eval_loss: 0.0000, mem: 1875.44 MB\n",
      "DT: 1.488, iter: 00520, train_loss: 6.2727, eval_loss: 0.0000, mem: 1923.54 MB\n",
      "DT: 1.644, iter: 00521, train_loss: 6.1780, eval_loss: 0.0000, mem: 1899.55 MB\n",
      "DT: 1.506, iter: 00522, train_loss: 6.1730, eval_loss: 0.0000, mem: 1911.37 MB\n",
      "DT: 1.514, iter: 00523, train_loss: 6.1440, eval_loss: 0.0000, mem: 1911.68 MB\n",
      "DT: 1.539, iter: 00524, train_loss: 6.2099, eval_loss: 0.0000, mem: 1911.70 MB\n",
      "DT: 1.525, iter: 00525, train_loss: 6.1698, eval_loss: 0.0000, mem: 1911.88 MB\n",
      "DT: 1.619, iter: 00526, train_loss: 6.1177, eval_loss: 0.0000, mem: 1912.12 MB\n",
      "DT: 1.575, iter: 00527, train_loss: 6.2549, eval_loss: 0.0000, mem: 1912.14 MB\n",
      "DT: 1.582, iter: 00528, train_loss: 6.2969, eval_loss: 0.0000, mem: 1912.21 MB\n",
      "DT: 1.495, iter: 00529, train_loss: 6.1948, eval_loss: 0.0000, mem: 1912.39 MB\n",
      "DT: 1.547, iter: 00530, train_loss: 6.2419, eval_loss: 0.0000, mem: 1912.68 MB\n",
      "DT: 1.591, iter: 00531, train_loss: 5.9983, eval_loss: 0.0000, mem: 1912.65 MB\n",
      "DT: 1.563, iter: 00532, train_loss: 6.0764, eval_loss: 0.0000, mem: 1912.72 MB\n",
      "DT: 1.517, iter: 00533, train_loss: 6.1217, eval_loss: 0.0000, mem: 1913.03 MB\n",
      "DT: 1.584, iter: 00534, train_loss: 6.2127, eval_loss: 0.0000, mem: 1913.33 MB\n",
      "DT: 1.566, iter: 00535, train_loss: 6.3998, eval_loss: 0.0000, mem: 1913.52 MB\n",
      "DT: 1.545, iter: 00536, train_loss: 6.3056, eval_loss: 0.0000, mem: 1913.77 MB\n",
      "DT: 1.597, iter: 00537, train_loss: 6.0788, eval_loss: 0.0000, mem: 1889.90 MB\n",
      "DT: 1.760, iter: 00538, train_loss: 5.9490, eval_loss: 0.0000, mem: 1890.00 MB\n",
      "DT: 1.587, iter: 00539, train_loss: 6.2257, eval_loss: 0.0000, mem: 1890.06 MB\n",
      "DT: 1.585, iter: 00540, train_loss: 6.2977, eval_loss: 0.0000, mem: 1889.96 MB\n",
      "DT: 1.626, iter: 00541, train_loss: 6.2815, eval_loss: 0.0000, mem: 1900.56 MB\n",
      "DT: 1.580, iter: 00542, train_loss: 6.2326, eval_loss: 0.0000, mem: 1900.62 MB\n",
      "DT: 1.611, iter: 00543, train_loss: 6.3859, eval_loss: 0.0000, mem: 1900.90 MB\n",
      "DT: 1.690, iter: 00544, train_loss: 6.2279, eval_loss: 0.0000, mem: 1900.96 MB\n",
      "DT: 1.612, iter: 00545, train_loss: 6.0971, eval_loss: 0.0000, mem: 1900.96 MB\n",
      "DT: 1.634, iter: 00546, train_loss: 6.1581, eval_loss: 0.0000, mem: 1923.03 MB\n",
      "DT: 1.666, iter: 00547, train_loss: 6.0242, eval_loss: 0.0000, mem: 1887.32 MB\n",
      "DT: 1.582, iter: 00548, train_loss: 6.2650, eval_loss: 0.0000, mem: 1887.35 MB\n",
      "DT: 1.590, iter: 00549, train_loss: 6.1275, eval_loss: 0.0000, mem: 1887.96 MB\n",
      "DT: 1.622, iter: 00550, train_loss: 6.1865, eval_loss: 0.0000, mem: 1888.23 MB\n",
      "DT: 1.649, iter: 00551, train_loss: 5.9758, eval_loss: 0.0000, mem: 1888.54 MB\n",
      "DT: 1.570, iter: 00552, train_loss: 6.2312, eval_loss: 0.0000, mem: 1906.61 MB\n",
      "DT: 1.551, iter: 00553, train_loss: 6.1797, eval_loss: 0.0000, mem: 1906.48 MB\n",
      "DT: 1.576, iter: 00554, train_loss: 6.3351, eval_loss: 0.0000, mem: 1906.87 MB\n",
      "DT: 1.551, iter: 00555, train_loss: 6.1139, eval_loss: 0.0000, mem: 1907.08 MB\n",
      "DT: 1.516, iter: 00556, train_loss: 6.1793, eval_loss: 0.0000, mem: 1916.61 MB\n",
      "DT: 1.547, iter: 00557, train_loss: 6.3472, eval_loss: 0.0000, mem: 1952.53 MB\n",
      "DT: 1.516, iter: 00558, train_loss: 6.0698, eval_loss: 0.0000, mem: 1928.84 MB\n",
      "DT: 1.511, iter: 00559, train_loss: 6.1259, eval_loss: 0.0000, mem: 1904.90 MB\n",
      "DT: 1.579, iter: 00560, train_loss: 6.1033, eval_loss: 0.0000, mem: 1893.11 MB\n",
      "DT: 1.549, iter: 00561, train_loss: 6.1828, eval_loss: 0.0000, mem: 1893.20 MB\n",
      "DT: 1.558, iter: 00562, train_loss: 6.2055, eval_loss: 0.0000, mem: 1893.09 MB\n",
      "DT: 1.675, iter: 00563, train_loss: 6.4007, eval_loss: 0.0000, mem: 1893.15 MB\n",
      "DT: 1.703, iter: 00564, train_loss: 6.2273, eval_loss: 0.0000, mem: 1893.10 MB\n",
      "DT: 1.675, iter: 00565, train_loss: 6.2191, eval_loss: 0.0000, mem: 1893.41 MB\n",
      "DT: 1.642, iter: 00566, train_loss: 6.0357, eval_loss: 0.0000, mem: 1893.48 MB\n",
      "DT: 1.629, iter: 00567, train_loss: 6.2564, eval_loss: 0.0000, mem: 1893.84 MB\n",
      "DT: 1.486, iter: 00568, train_loss: 6.1425, eval_loss: 0.0000, mem: 1894.16 MB\n",
      "DT: 1.606, iter: 00569, train_loss: 6.1003, eval_loss: 0.0000, mem: 1894.22 MB\n",
      "DT: 1.516, iter: 00570, train_loss: 6.1400, eval_loss: 0.0000, mem: 1894.21 MB\n",
      "DT: 1.488, iter: 00571, train_loss: 6.0634, eval_loss: 0.0000, mem: 1906.12 MB\n",
      "DT: 1.523, iter: 00572, train_loss: 6.1698, eval_loss: 0.0000, mem: 1906.08 MB\n",
      "DT: 1.749, iter: 00573, train_loss: 6.1063, eval_loss: 0.0000, mem: 1906.28 MB\n",
      "DT: 1.555, iter: 00574, train_loss: 6.1774, eval_loss: 0.0000, mem: 1906.34 MB\n",
      "DT: 1.523, iter: 00575, train_loss: 6.2512, eval_loss: 0.0000, mem: 1827.30 MB\n",
      "DT: 1.551, iter: 00576, train_loss: 6.1324, eval_loss: 0.0000, mem: 1827.35 MB\n",
      "DT: 1.559, iter: 00577, train_loss: 6.0570, eval_loss: 0.0000, mem: 1935.26 MB\n",
      "DT: 1.562, iter: 00578, train_loss: 6.1623, eval_loss: 0.0000, mem: 1935.47 MB\n",
      "DT: 1.575, iter: 00579, train_loss: 6.1682, eval_loss: 0.0000, mem: 1935.50 MB\n",
      "DT: 1.599, iter: 00580, train_loss: 6.0962, eval_loss: 0.0000, mem: 1935.61 MB\n",
      "DT: 1.583, iter: 00581, train_loss: 6.4653, eval_loss: 0.0000, mem: 1837.74 MB\n",
      "DT: 1.669, iter: 00582, train_loss: 6.3021, eval_loss: 0.0000, mem: 1837.80 MB\n",
      "DT: 1.589, iter: 00583, train_loss: 6.3492, eval_loss: 0.0000, mem: 1885.36 MB\n",
      "DT: 1.538, iter: 00584, train_loss: 6.1761, eval_loss: 0.0000, mem: 1969.38 MB\n",
      "DT: 1.663, iter: 00585, train_loss: 6.2961, eval_loss: 0.0000, mem: 1942.56 MB\n",
      "DT: 1.591, iter: 00586, train_loss: 6.0592, eval_loss: 0.0000, mem: 1837.74 MB\n",
      "DT: 1.620, iter: 00587, train_loss: 6.1735, eval_loss: 0.0000, mem: 1921.70 MB\n",
      "DT: 1.657, iter: 00588, train_loss: 6.2802, eval_loss: 0.0000, mem: 1945.82 MB\n",
      "DT: 1.502, iter: 00589, train_loss: 6.3436, eval_loss: 0.0000, mem: 1909.96 MB\n",
      "DT: 1.518, iter: 00590, train_loss: 6.0409, eval_loss: 0.0000, mem: 1883.04 MB\n",
      "DT: 1.543, iter: 00591, train_loss: 6.5077, eval_loss: 0.0000, mem: 1919.23 MB\n",
      "DT: 1.587, iter: 00592, train_loss: 6.1398, eval_loss: 0.0000, mem: 1861.55 MB\n",
      "DT: 1.533, iter: 00593, train_loss: 6.0521, eval_loss: 0.0000, mem: 1861.61 MB\n",
      "DT: 1.531, iter: 00594, train_loss: 6.1894, eval_loss: 0.0000, mem: 1861.62 MB\n",
      "DT: 1.619, iter: 00595, train_loss: 6.1282, eval_loss: 0.0000, mem: 1948.62 MB\n",
      "DT: 1.565, iter: 00596, train_loss: 6.2421, eval_loss: 0.0000, mem: 1960.55 MB\n",
      "DT: 1.757, iter: 00597, train_loss: 6.1808, eval_loss: 0.0000, mem: 1960.61 MB\n",
      "DT: 1.619, iter: 00598, train_loss: 6.2297, eval_loss: 0.0000, mem: 1861.21 MB\n",
      "DT: 1.582, iter: 00599, train_loss: 6.2977, eval_loss: 0.0000, mem: 1861.34 MB\n",
      "DT: 1.553, iter: 00600, train_loss: 6.3765, eval_loss: 0.0000, mem: 1897.08 MB\n",
      "DT: 1.516, iter: 00601, train_loss: 6.0355, eval_loss: 0.0000, mem: 1860.77 MB\n",
      "DT: 1.678, iter: 00602, train_loss: 6.1285, eval_loss: 0.0000, mem: 1833.95 MB\n",
      "DT: 1.541, iter: 00603, train_loss: 6.0968, eval_loss: 0.0000, mem: 1834.02 MB\n",
      "DT: 1.604, iter: 00604, train_loss: 6.2972, eval_loss: 0.0000, mem: 1929.75 MB\n",
      "DT: 1.682, iter: 00605, train_loss: 6.2741, eval_loss: 0.0000, mem: 1899.90 MB\n",
      "DT: 1.553, iter: 00606, train_loss: 6.2331, eval_loss: 0.0000, mem: 1846.25 MB\n",
      "DT: 1.520, iter: 00607, train_loss: 6.3785, eval_loss: 0.0000, mem: 1905.88 MB\n",
      "DT: 1.669, iter: 00608, train_loss: 6.0163, eval_loss: 0.0000, mem: 1905.82 MB\n",
      "DT: 1.562, iter: 00609, train_loss: 6.3896, eval_loss: 0.0000, mem: 1906.01 MB\n",
      "DT: 1.683, iter: 00610, train_loss: 6.2164, eval_loss: 0.0000, mem: 1882.13 MB\n",
      "DT: 1.556, iter: 00611, train_loss: 6.3433, eval_loss: 0.0000, mem: 1906.19 MB\n",
      "DT: 1.542, iter: 00612, train_loss: 6.1516, eval_loss: 0.0000, mem: 1860.68 MB\n",
      "DT: 1.603, iter: 00613, train_loss: 6.1952, eval_loss: 0.0000, mem: 1944.83 MB\n",
      "DT: 1.570, iter: 00614, train_loss: 6.3191, eval_loss: 0.0000, mem: 1896.61 MB\n",
      "DT: 1.603, iter: 00615, train_loss: 6.1514, eval_loss: 0.0000, mem: 1931.68 MB\n",
      "DT: 1.534, iter: 00616, train_loss: 6.1046, eval_loss: 0.0000, mem: 1884.96 MB\n",
      "DT: 1.646, iter: 00617, train_loss: 6.1074, eval_loss: 0.0000, mem: 1884.90 MB\n",
      "DT: 1.556, iter: 00618, train_loss: 6.2341, eval_loss: 0.0000, mem: 1919.54 MB\n",
      "DT: 1.604, iter: 00619, train_loss: 6.0739, eval_loss: 0.0000, mem: 1895.62 MB\n",
      "DT: 1.610, iter: 00620, train_loss: 6.1542, eval_loss: 0.0000, mem: 1895.79 MB\n",
      "DT: 1.615, iter: 00621, train_loss: 6.1181, eval_loss: 0.0000, mem: 1855.30 MB\n",
      "DT: 1.630, iter: 00622, train_loss: 5.9774, eval_loss: 0.0000, mem: 1866.83 MB\n",
      "DT: 1.537, iter: 00623, train_loss: 6.1583, eval_loss: 0.0000, mem: 1902.68 MB\n",
      "DT: 1.558, iter: 00624, train_loss: 6.2757, eval_loss: 0.0000, mem: 1902.99 MB\n",
      "DT: 1.630, iter: 00625, train_loss: 6.1383, eval_loss: 0.0000, mem: 1926.24 MB\n",
      "DT: 1.571, iter: 00626, train_loss: 6.0874, eval_loss: 0.0000, mem: 1926.22 MB\n",
      "DT: 1.656, iter: 00627, train_loss: 6.0752, eval_loss: 0.0000, mem: 1901.13 MB\n",
      "DT: 1.563, iter: 00628, train_loss: 6.0391, eval_loss: 0.0000, mem: 1865.94 MB\n",
      "DT: 1.563, iter: 00629, train_loss: 5.9634, eval_loss: 0.0000, mem: 1866.20 MB\n",
      "DT: 1.584, iter: 00630, train_loss: 5.9458, eval_loss: 0.0000, mem: 1902.01 MB\n",
      "DT: 1.580, iter: 00631, train_loss: 6.1919, eval_loss: 0.0000, mem: 1901.96 MB\n",
      "DT: 1.539, iter: 00632, train_loss: 6.2407, eval_loss: 0.0000, mem: 1902.02 MB\n",
      "DT: 1.558, iter: 00633, train_loss: 6.1098, eval_loss: 0.0000, mem: 1902.09 MB\n",
      "DT: 1.586, iter: 00634, train_loss: 6.1888, eval_loss: 0.0000, mem: 1866.29 MB\n",
      "DT: 1.703, iter: 00635, train_loss: 6.1547, eval_loss: 0.0000, mem: 1866.23 MB\n",
      "DT: 1.586, iter: 00636, train_loss: 6.1280, eval_loss: 0.0000, mem: 1854.12 MB\n",
      "DT: 1.621, iter: 00637, train_loss: 6.1058, eval_loss: 0.0000, mem: 1878.18 MB\n",
      "DT: 1.604, iter: 00638, train_loss: 6.0459, eval_loss: 0.0000, mem: 1854.24 MB\n",
      "DT: 1.565, iter: 00639, train_loss: 6.0185, eval_loss: 0.0000, mem: 1914.36 MB\n",
      "DT: 1.573, iter: 00640, train_loss: 6.0993, eval_loss: 0.0000, mem: 1962.42 MB\n",
      "DT: 1.566, iter: 00641, train_loss: 6.0416, eval_loss: 0.0000, mem: 1908.41 MB\n",
      "DT: 1.563, iter: 00642, train_loss: 6.1193, eval_loss: 0.0000, mem: 1920.50 MB\n",
      "DT: 1.545, iter: 00643, train_loss: 5.9588, eval_loss: 0.0000, mem: 1920.33 MB\n",
      "DT: 1.618, iter: 00644, train_loss: 6.0381, eval_loss: 0.0000, mem: 1854.55 MB\n",
      "DT: 1.527, iter: 00645, train_loss: 5.9971, eval_loss: 0.0000, mem: 1890.74 MB\n",
      "DT: 1.582, iter: 00646, train_loss: 6.0725, eval_loss: 0.0000, mem: 1866.64 MB\n",
      "DT: 1.638, iter: 00647, train_loss: 6.0822, eval_loss: 0.0000, mem: 1878.58 MB\n",
      "DT: 1.694, iter: 00648, train_loss: 6.1375, eval_loss: 0.0000, mem: 1914.11 MB\n",
      "DT: 1.516, iter: 00649, train_loss: 6.1652, eval_loss: 0.0000, mem: 1865.34 MB\n",
      "DT: 1.655, iter: 00650, train_loss: 6.1617, eval_loss: 0.0000, mem: 1865.46 MB\n",
      "DT: 1.550, iter: 00651, train_loss: 6.0318, eval_loss: 0.0000, mem: 1866.90 MB\n",
      "DT: 1.501, iter: 00652, train_loss: 6.1571, eval_loss: 0.0000, mem: 1867.30 MB\n",
      "DT: 1.640, iter: 00653, train_loss: 6.2573, eval_loss: 0.0000, mem: 1912.30 MB\n",
      "DT: 1.562, iter: 00654, train_loss: 6.1544, eval_loss: 0.0000, mem: 1912.24 MB\n",
      "DT: 1.587, iter: 00655, train_loss: 6.3029, eval_loss: 0.0000, mem: 1912.43 MB\n",
      "DT: 1.565, iter: 00656, train_loss: 6.0075, eval_loss: 0.0000, mem: 1912.50 MB\n",
      "DT: 1.554, iter: 00657, train_loss: 6.2014, eval_loss: 0.0000, mem: 1912.58 MB\n",
      "DT: 1.607, iter: 00658, train_loss: 6.0524, eval_loss: 0.0000, mem: 1912.67 MB\n",
      "DT: 1.611, iter: 00659, train_loss: 6.0716, eval_loss: 0.0000, mem: 1912.92 MB\n",
      "DT: 1.592, iter: 00660, train_loss: 6.3062, eval_loss: 0.0000, mem: 1912.96 MB\n",
      "DT: 1.572, iter: 00661, train_loss: 6.1637, eval_loss: 0.0000, mem: 1912.90 MB\n",
      "DT: 1.550, iter: 00662, train_loss: 6.1315, eval_loss: 0.0000, mem: 1912.84 MB\n",
      "DT: 1.620, iter: 00663, train_loss: 6.1733, eval_loss: 0.0000, mem: 1898.08 MB\n",
      "DT: 1.534, iter: 00664, train_loss: 6.1161, eval_loss: 0.0000, mem: 1981.91 MB\n",
      "DT: 1.555, iter: 00665, train_loss: 5.9933, eval_loss: 0.0000, mem: 1898.27 MB\n",
      "DT: 1.592, iter: 00666, train_loss: 6.0722, eval_loss: 0.0000, mem: 1898.21 MB\n",
      "DT: 1.497, iter: 00667, train_loss: 6.1074, eval_loss: 0.0000, mem: 1898.25 MB\n",
      "DT: 1.594, iter: 00668, train_loss: 6.1278, eval_loss: 0.0000, mem: 1898.31 MB\n",
      "DT: 1.562, iter: 00669, train_loss: 6.1437, eval_loss: 0.0000, mem: 1907.25 MB\n",
      "DT: 1.666, iter: 00670, train_loss: 6.1767, eval_loss: 0.0000, mem: 1907.21 MB\n",
      "DT: 1.629, iter: 00671, train_loss: 6.2007, eval_loss: 0.0000, mem: 1883.29 MB\n",
      "DT: 1.717, iter: 00672, train_loss: 6.1533, eval_loss: 0.0000, mem: 1895.32 MB\n",
      "DT: 1.559, iter: 00673, train_loss: 6.1451, eval_loss: 0.0000, mem: 1895.26 MB\n",
      "DT: 1.559, iter: 00674, train_loss: 5.9876, eval_loss: 0.0000, mem: 1895.22 MB\n",
      "DT: 1.569, iter: 00675, train_loss: 6.1357, eval_loss: 0.0000, mem: 1931.23 MB\n",
      "DT: 1.685, iter: 00676, train_loss: 6.2746, eval_loss: 0.0000, mem: 1931.42 MB\n",
      "DT: 1.571, iter: 00677, train_loss: 6.0397, eval_loss: 0.0000, mem: 1931.48 MB\n",
      "DT: 1.597, iter: 00678, train_loss: 6.2357, eval_loss: 0.0000, mem: 1895.00 MB\n",
      "DT: 1.593, iter: 00679, train_loss: 6.1448, eval_loss: 0.0000, mem: 1895.05 MB\n",
      "DT: 1.646, iter: 00680, train_loss: 6.0435, eval_loss: 0.0000, mem: 1895.20 MB\n",
      "DT: 1.669, iter: 00681, train_loss: 6.0141, eval_loss: 0.0000, mem: 1895.16 MB\n",
      "DT: 1.677, iter: 00682, train_loss: 6.2425, eval_loss: 0.0000, mem: 1895.20 MB\n",
      "DT: 1.626, iter: 00683, train_loss: 6.2484, eval_loss: 0.0000, mem: 1895.53 MB\n",
      "DT: 1.596, iter: 00684, train_loss: 6.1093, eval_loss: 0.0000, mem: 1895.27 MB\n",
      "DT: 1.692, iter: 00685, train_loss: 6.2191, eval_loss: 0.0000, mem: 1922.12 MB\n",
      "DT: 1.637, iter: 00686, train_loss: 5.9920, eval_loss: 0.0000, mem: 1898.18 MB\n",
      "DT: 1.546, iter: 00687, train_loss: 6.2756, eval_loss: 0.0000, mem: 1898.50 MB\n",
      "DT: 1.636, iter: 00688, train_loss: 6.1253, eval_loss: 0.0000, mem: 1898.43 MB\n",
      "DT: 1.561, iter: 00689, train_loss: 6.2795, eval_loss: 0.0000, mem: 1898.12 MB\n",
      "DT: 1.604, iter: 00690, train_loss: 6.2099, eval_loss: 0.0000, mem: 1898.18 MB\n",
      "DT: 1.576, iter: 00691, train_loss: 6.0440, eval_loss: 0.0000, mem: 1898.35 MB\n",
      "DT: 1.565, iter: 00692, train_loss: 6.0430, eval_loss: 0.0000, mem: 1898.46 MB\n",
      "DT: 1.640, iter: 00693, train_loss: 6.3414, eval_loss: 0.0000, mem: 1898.51 MB\n",
      "DT: 1.564, iter: 00694, train_loss: 6.0391, eval_loss: 0.0000, mem: 1898.32 MB\n",
      "DT: 1.622, iter: 00695, train_loss: 6.3130, eval_loss: 0.0000, mem: 1898.65 MB\n",
      "DT: 1.570, iter: 00696, train_loss: 5.9655, eval_loss: 0.0000, mem: 1898.71 MB\n",
      "DT: 1.514, iter: 00697, train_loss: 6.0044, eval_loss: 0.0000, mem: 1920.31 MB\n",
      "DT: 1.701, iter: 00698, train_loss: 6.1941, eval_loss: 0.0000, mem: 1920.31 MB\n",
      "DT: 1.567, iter: 00699, train_loss: 6.0725, eval_loss: 0.0000, mem: 1920.18 MB\n",
      "DT: 1.605, iter: 00700, train_loss: 6.1830, eval_loss: 0.0000, mem: 1920.38 MB\n",
      "DT: 1.542, iter: 00701, train_loss: 6.1274, eval_loss: 0.0000, mem: 1920.32 MB\n",
      "DT: 1.542, iter: 00702, train_loss: 6.0170, eval_loss: 0.0000, mem: 1941.48 MB\n",
      "DT: 1.596, iter: 00703, train_loss: 6.0501, eval_loss: 0.0000, mem: 1917.11 MB\n",
      "DT: 1.609, iter: 00704, train_loss: 6.0319, eval_loss: 0.0000, mem: 1917.10 MB\n",
      "DT: 1.630, iter: 00705, train_loss: 6.2351, eval_loss: 0.0000, mem: 1917.18 MB\n",
      "DT: 1.658, iter: 00706, train_loss: 5.8865, eval_loss: 0.0000, mem: 1917.24 MB\n",
      "DT: 1.640, iter: 00707, train_loss: 6.1808, eval_loss: 0.0000, mem: 1917.30 MB\n",
      "DT: 1.658, iter: 00708, train_loss: 6.0664, eval_loss: 0.0000, mem: 1917.36 MB\n",
      "DT: 1.637, iter: 00709, train_loss: 5.9637, eval_loss: 0.0000, mem: 1917.41 MB\n",
      "DT: 1.719, iter: 00710, train_loss: 6.0947, eval_loss: 0.0000, mem: 1821.08 MB\n",
      "DT: 1.635, iter: 00711, train_loss: 6.1093, eval_loss: 0.0000, mem: 1857.25 MB\n",
      "DT: 1.591, iter: 00712, train_loss: 6.2720, eval_loss: 0.0000, mem: 1829.00 MB\n",
      "DT: 1.612, iter: 00713, train_loss: 6.0783, eval_loss: 0.0000, mem: 1817.71 MB\n",
      "DT: 1.603, iter: 00714, train_loss: 6.1818, eval_loss: 0.0000, mem: 1866.53 MB\n",
      "DT: 1.689, iter: 00715, train_loss: 6.0181, eval_loss: 0.0000, mem: 1830.72 MB\n",
      "DT: 1.574, iter: 00716, train_loss: 6.1838, eval_loss: 0.0000, mem: 1867.56 MB\n",
      "DT: 1.570, iter: 00717, train_loss: 6.1348, eval_loss: 0.0000, mem: 1891.41 MB\n",
      "DT: 1.615, iter: 00718, train_loss: 6.0135, eval_loss: 0.0000, mem: 1867.18 MB\n",
      "DT: 1.560, iter: 00719, train_loss: 6.0954, eval_loss: 0.0000, mem: 1891.25 MB\n",
      "DT: 1.592, iter: 00720, train_loss: 5.9682, eval_loss: 0.0000, mem: 1927.39 MB\n",
      "DT: 1.657, iter: 00721, train_loss: 6.0872, eval_loss: 0.0000, mem: 1951.40 MB\n",
      "DT: 1.687, iter: 00722, train_loss: 6.2230, eval_loss: 0.0000, mem: 1926.96 MB\n",
      "DT: 1.594, iter: 00723, train_loss: 6.1544, eval_loss: 0.0000, mem: 1927.02 MB\n",
      "DT: 1.601, iter: 00724, train_loss: 6.0873, eval_loss: 0.0000, mem: 1877.51 MB\n",
      "DT: 1.550, iter: 00725, train_loss: 6.2820, eval_loss: 0.0000, mem: 1877.68 MB\n",
      "DT: 1.662, iter: 00726, train_loss: 6.3952, eval_loss: 0.0000, mem: 1877.60 MB\n",
      "DT: 1.560, iter: 00727, train_loss: 6.0511, eval_loss: 0.0000, mem: 1877.66 MB\n",
      "DT: 1.593, iter: 00728, train_loss: 6.1994, eval_loss: 0.0000, mem: 1905.48 MB\n",
      "DT: 1.574, iter: 00729, train_loss: 5.9448, eval_loss: 0.0000, mem: 1880.99 MB\n",
      "DT: 1.597, iter: 00730, train_loss: 6.1976, eval_loss: 0.0000, mem: 1881.03 MB\n",
      "DT: 1.480, iter: 00731, train_loss: 6.1389, eval_loss: 0.0000, mem: 1881.19 MB\n",
      "DT: 1.523, iter: 00732, train_loss: 6.4124, eval_loss: 0.0000, mem: 1881.38 MB\n",
      "DT: 1.542, iter: 00733, train_loss: 6.1410, eval_loss: 0.0000, mem: 1881.44 MB\n",
      "DT: 1.538, iter: 00734, train_loss: 5.9867, eval_loss: 0.0000, mem: 1881.43 MB\n",
      "DT: 1.510, iter: 00735, train_loss: 6.1554, eval_loss: 0.0000, mem: 1892.69 MB\n",
      "DT: 1.554, iter: 00736, train_loss: 5.9114, eval_loss: 0.0000, mem: 1892.88 MB\n",
      "DT: 1.556, iter: 00737, train_loss: 6.1621, eval_loss: 0.0000, mem: 1868.93 MB\n",
      "DT: 1.628, iter: 00738, train_loss: 6.2200, eval_loss: 0.0000, mem: 1888.25 MB\n",
      "DT: 1.541, iter: 00739, train_loss: 6.1863, eval_loss: 0.0000, mem: 1888.19 MB\n",
      "DT: 1.589, iter: 00740, train_loss: 6.0316, eval_loss: 0.0000, mem: 1899.73 MB\n",
      "DT: 1.563, iter: 00741, train_loss: 6.0646, eval_loss: 0.0000, mem: 1868.19 MB\n",
      "DT: 1.534, iter: 00742, train_loss: 6.1262, eval_loss: 0.0000, mem: 1868.24 MB\n",
      "DT: 1.580, iter: 00743, train_loss: 6.0921, eval_loss: 0.0000, mem: 1901.16 MB\n",
      "DT: 1.585, iter: 00744, train_loss: 6.0187, eval_loss: 0.0000, mem: 1864.60 MB\n",
      "DT: 1.574, iter: 00745, train_loss: 6.1407, eval_loss: 0.0000, mem: 1912.66 MB\n",
      "DT: 1.632, iter: 00746, train_loss: 6.0657, eval_loss: 0.0000, mem: 1924.60 MB\n",
      "DT: 1.529, iter: 00747, train_loss: 6.0107, eval_loss: 0.0000, mem: 1875.61 MB\n",
      "DT: 1.629, iter: 00748, train_loss: 6.0925, eval_loss: 0.0000, mem: 1899.38 MB\n",
      "DT: 1.601, iter: 00749, train_loss: 6.1452, eval_loss: 0.0000, mem: 1911.40 MB\n",
      "DT: 1.487, iter: 00750, train_loss: 6.0885, eval_loss: 0.0000, mem: 1911.44 MB\n",
      "DT: 1.571, iter: 00751, train_loss: 5.8966, eval_loss: 0.0000, mem: 1911.46 MB\n",
      "DT: 1.533, iter: 00752, train_loss: 6.0473, eval_loss: 0.0000, mem: 1888.20 MB\n",
      "DT: 1.553, iter: 00753, train_loss: 6.1017, eval_loss: 0.0000, mem: 1960.07 MB\n",
      "DT: 1.536, iter: 00754, train_loss: 6.2308, eval_loss: 0.0000, mem: 1896.38 MB\n",
      "DT: 1.634, iter: 00755, train_loss: 6.1452, eval_loss: 0.0000, mem: 1992.45 MB\n",
      "DT: 1.578, iter: 00756, train_loss: 6.0021, eval_loss: 0.0000, mem: 1931.34 MB\n",
      "DT: 1.558, iter: 00757, train_loss: 6.2077, eval_loss: 0.0000, mem: 1931.30 MB\n",
      "DT: 1.605, iter: 00758, train_loss: 5.9615, eval_loss: 0.0000, mem: 1979.39 MB\n",
      "DT: 1.617, iter: 00759, train_loss: 5.9424, eval_loss: 0.0000, mem: 1893.00 MB\n",
      "DT: 1.583, iter: 00760, train_loss: 5.9989, eval_loss: 0.0000, mem: 1857.93 MB\n",
      "DT: 1.628, iter: 00761, train_loss: 6.3656, eval_loss: 0.0000, mem: 1894.12 MB\n",
      "DT: 1.615, iter: 00762, train_loss: 6.3238, eval_loss: 0.0000, mem: 1894.34 MB\n",
      "DT: 1.559, iter: 00763, train_loss: 5.9055, eval_loss: 0.0000, mem: 1893.98 MB\n",
      "DT: 1.584, iter: 00764, train_loss: 6.2501, eval_loss: 0.0000, mem: 1952.54 MB\n",
      "DT: 1.592, iter: 00765, train_loss: 6.1469, eval_loss: 0.0000, mem: 2000.54 MB\n",
      "DT: 1.616, iter: 00766, train_loss: 6.0455, eval_loss: 0.0000, mem: 1919.73 MB\n",
      "DT: 1.555, iter: 00767, train_loss: 5.9884, eval_loss: 0.0000, mem: 1943.58 MB\n",
      "DT: 1.591, iter: 00768, train_loss: 6.0490, eval_loss: 0.0000, mem: 1919.27 MB\n",
      "DT: 1.580, iter: 00769, train_loss: 6.0295, eval_loss: 0.0000, mem: 1919.33 MB\n",
      "DT: 1.588, iter: 00770, train_loss: 6.1154, eval_loss: 0.0000, mem: 1919.49 MB\n",
      "DT: 1.654, iter: 00771, train_loss: 5.9960, eval_loss: 0.0000, mem: 1919.51 MB\n",
      "DT: 1.586, iter: 00772, train_loss: 6.0636, eval_loss: 0.0000, mem: 1919.29 MB\n",
      "DT: 1.532, iter: 00773, train_loss: 5.9799, eval_loss: 0.0000, mem: 1919.43 MB\n",
      "DT: 1.555, iter: 00774, train_loss: 6.2374, eval_loss: 0.0000, mem: 1919.61 MB\n",
      "DT: 1.562, iter: 00775, train_loss: 6.1924, eval_loss: 0.0000, mem: 1919.95 MB\n",
      "DT: 1.694, iter: 00776, train_loss: 6.1065, eval_loss: 0.0000, mem: 1920.26 MB\n",
      "DT: 1.563, iter: 00777, train_loss: 6.0933, eval_loss: 0.0000, mem: 1895.63 MB\n",
      "DT: 1.661, iter: 00778, train_loss: 6.0326, eval_loss: 0.0000, mem: 1943.65 MB\n",
      "DT: 1.641, iter: 00779, train_loss: 6.1303, eval_loss: 0.0000, mem: 1943.86 MB\n",
      "DT: 1.771, iter: 00780, train_loss: 6.1178, eval_loss: 0.0000, mem: 1943.79 MB\n",
      "DT: 1.597, iter: 00781, train_loss: 5.9978, eval_loss: 0.0000, mem: 1944.05 MB\n",
      "DT: 1.611, iter: 00782, train_loss: 5.9501, eval_loss: 0.0000, mem: 1943.99 MB\n",
      "DT: 1.527, iter: 00783, train_loss: 6.0485, eval_loss: 0.0000, mem: 1913.95 MB\n",
      "DT: 1.488, iter: 00784, train_loss: 6.0129, eval_loss: 0.0000, mem: 1833.00 MB\n",
      "DT: 1.499, iter: 00785, train_loss: 6.0432, eval_loss: 0.0000, mem: 1856.86 MB\n",
      "DT: 1.527, iter: 00786, train_loss: 6.1541, eval_loss: 0.0000, mem: 1843.42 MB\n",
      "DT: 1.510, iter: 00787, train_loss: 6.1879, eval_loss: 0.0000, mem: 1927.41 MB\n",
      "DT: 1.710, iter: 00788, train_loss: 6.0626, eval_loss: 0.0000, mem: 1897.61 MB\n",
      "DT: 1.590, iter: 00789, train_loss: 5.8285, eval_loss: 0.0000, mem: 1897.55 MB\n",
      "DT: 1.546, iter: 00790, train_loss: 6.1028, eval_loss: 0.0000, mem: 1897.49 MB\n",
      "DT: 1.636, iter: 00791, train_loss: 6.2854, eval_loss: 0.0000, mem: 1909.22 MB\n",
      "DT: 1.672, iter: 00792, train_loss: 5.8937, eval_loss: 0.0000, mem: 1867.28 MB\n",
      "DT: 1.570, iter: 00793, train_loss: 6.2821, eval_loss: 0.0000, mem: 1879.47 MB\n",
      "DT: 1.554, iter: 00794, train_loss: 6.0166, eval_loss: 0.0000, mem: 1879.53 MB\n",
      "DT: 1.546, iter: 00795, train_loss: 5.8636, eval_loss: 0.0000, mem: 1835.97 MB\n",
      "DT: 1.552, iter: 00796, train_loss: 6.0561, eval_loss: 0.0000, mem: 1848.16 MB\n",
      "DT: 1.575, iter: 00797, train_loss: 6.1299, eval_loss: 0.0000, mem: 1824.30 MB\n",
      "DT: 1.558, iter: 00798, train_loss: 5.9762, eval_loss: 0.0000, mem: 1836.36 MB\n",
      "DT: 1.610, iter: 00799, train_loss: 6.1139, eval_loss: 0.0000, mem: 1896.41 MB\n",
      "DT: 1.482, iter: 00800, train_loss: 6.0727, eval_loss: 0.0000, mem: 1871.84 MB\n",
      "DT: 1.692, iter: 00801, train_loss: 6.1122, eval_loss: 0.0000, mem: 1919.60 MB\n",
      "DT: 1.607, iter: 00802, train_loss: 5.9244, eval_loss: 0.0000, mem: 1943.85 MB\n",
      "DT: 1.563, iter: 00803, train_loss: 5.8412, eval_loss: 0.0000, mem: 1943.89 MB\n",
      "DT: 1.569, iter: 00804, train_loss: 6.0841, eval_loss: 0.0000, mem: 1943.86 MB\n",
      "DT: 1.517, iter: 00805, train_loss: 6.1753, eval_loss: 0.0000, mem: 1943.89 MB\n",
      "DT: 1.530, iter: 00806, train_loss: 5.9832, eval_loss: 0.0000, mem: 1943.93 MB\n",
      "DT: 1.586, iter: 00807, train_loss: 6.0549, eval_loss: 0.0000, mem: 1943.75 MB\n",
      "DT: 1.556, iter: 00808, train_loss: 6.1869, eval_loss: 0.0000, mem: 1833.68 MB\n",
      "DT: 1.576, iter: 00809, train_loss: 6.0209, eval_loss: 0.0000, mem: 1833.76 MB\n",
      "DT: 1.581, iter: 00810, train_loss: 6.1628, eval_loss: 0.0000, mem: 1868.84 MB\n",
      "DT: 1.718, iter: 00811, train_loss: 6.1909, eval_loss: 0.0000, mem: 1904.78 MB\n",
      "DT: 1.600, iter: 00812, train_loss: 6.2496, eval_loss: 0.0000, mem: 1868.21 MB\n",
      "DT: 1.573, iter: 00813, train_loss: 5.8303, eval_loss: 0.0000, mem: 1904.11 MB\n",
      "DT: 1.692, iter: 00814, train_loss: 5.9705, eval_loss: 0.0000, mem: 1916.28 MB\n",
      "DT: 1.602, iter: 00815, train_loss: 6.2212, eval_loss: 0.0000, mem: 1916.34 MB\n",
      "DT: 1.612, iter: 00816, train_loss: 6.1779, eval_loss: 0.0000, mem: 1892.53 MB\n",
      "DT: 1.671, iter: 00817, train_loss: 6.0948, eval_loss: 0.0000, mem: 1867.84 MB\n",
      "DT: 1.671, iter: 00818, train_loss: 6.0501, eval_loss: 0.0000, mem: 1903.56 MB\n",
      "DT: 1.548, iter: 00819, train_loss: 5.9204, eval_loss: 0.0000, mem: 1867.62 MB\n",
      "DT: 1.613, iter: 00820, train_loss: 6.2066, eval_loss: 0.0000, mem: 1915.61 MB\n",
      "DT: 1.548, iter: 00821, train_loss: 6.0291, eval_loss: 0.0000, mem: 1867.79 MB\n",
      "DT: 1.547, iter: 00822, train_loss: 6.0535, eval_loss: 0.0000, mem: 1879.60 MB\n",
      "DT: 1.471, iter: 00823, train_loss: 6.0357, eval_loss: 0.0000, mem: 1939.73 MB\n",
      "DT: 1.585, iter: 00824, train_loss: 5.9310, eval_loss: 0.0000, mem: 1988.01 MB\n",
      "DT: 1.605, iter: 00825, train_loss: 6.1419, eval_loss: 0.0000, mem: 1902.77 MB\n",
      "DT: 1.566, iter: 00826, train_loss: 6.2842, eval_loss: 0.0000, mem: 1914.84 MB\n",
      "DT: 1.549, iter: 00827, train_loss: 6.2103, eval_loss: 0.0000, mem: 1950.00 MB\n",
      "DT: 1.563, iter: 00828, train_loss: 6.1279, eval_loss: 0.0000, mem: 1913.86 MB\n",
      "DT: 1.667, iter: 00829, train_loss: 6.0365, eval_loss: 0.0000, mem: 1913.74 MB\n",
      "DT: 1.648, iter: 00830, train_loss: 6.0560, eval_loss: 0.0000, mem: 1924.30 MB\n",
      "DT: 1.534, iter: 00831, train_loss: 5.9510, eval_loss: 0.0000, mem: 1924.43 MB\n",
      "DT: 1.703, iter: 00832, train_loss: 6.1324, eval_loss: 0.0000, mem: 1924.57 MB\n",
      "DT: 1.568, iter: 00833, train_loss: 6.1070, eval_loss: 0.0000, mem: 1924.42 MB\n",
      "DT: 1.627, iter: 00834, train_loss: 6.2140, eval_loss: 0.0000, mem: 1924.73 MB\n",
      "DT: 1.594, iter: 00835, train_loss: 5.9329, eval_loss: 0.0000, mem: 1935.29 MB\n",
      "DT: 1.575, iter: 00836, train_loss: 6.0450, eval_loss: 0.0000, mem: 1898.11 MB\n",
      "DT: 1.549, iter: 00837, train_loss: 6.0652, eval_loss: 0.0000, mem: 1933.83 MB\n",
      "DT: 1.543, iter: 00838, train_loss: 6.1603, eval_loss: 0.0000, mem: 1933.77 MB\n",
      "DT: 1.629, iter: 00839, train_loss: 5.8622, eval_loss: 0.0000, mem: 1933.95 MB\n",
      "DT: 1.582, iter: 00840, train_loss: 5.8510, eval_loss: 0.0000, mem: 1942.57 MB\n",
      "DT: 1.546, iter: 00841, train_loss: 5.9855, eval_loss: 0.0000, mem: 1918.68 MB\n",
      "DT: 1.692, iter: 00842, train_loss: 6.1045, eval_loss: 0.0000, mem: 1918.61 MB\n",
      "DT: 1.551, iter: 00843, train_loss: 6.0914, eval_loss: 0.0000, mem: 1929.05 MB\n",
      "DT: 1.553, iter: 00844, train_loss: 6.0403, eval_loss: 0.0000, mem: 1977.32 MB\n",
      "DT: 1.432, iter: 00845, train_loss: 6.2023, eval_loss: 0.0000, mem: 1989.39 MB\n",
      "DT: 1.584, iter: 00846, train_loss: 6.0499, eval_loss: 0.0000, mem: 1964.70 MB\n",
      "DT: 1.536, iter: 00847, train_loss: 6.0448, eval_loss: 0.0000, mem: 1939.74 MB\n",
      "DT: 1.523, iter: 00848, train_loss: 5.7993, eval_loss: 0.0000, mem: 1963.61 MB\n",
      "DT: 1.583, iter: 00849, train_loss: 5.9657, eval_loss: 0.0000, mem: 1939.18 MB\n",
      "DT: 1.573, iter: 00850, train_loss: 6.0090, eval_loss: 0.0000, mem: 1939.11 MB\n",
      "DT: 1.564, iter: 00851, train_loss: 6.0530, eval_loss: 0.0000, mem: 1939.16 MB\n",
      "DT: 1.545, iter: 00852, train_loss: 6.0848, eval_loss: 0.0000, mem: 1939.24 MB\n",
      "DT: 1.606, iter: 00853, train_loss: 6.0435, eval_loss: 0.0000, mem: 1939.17 MB\n",
      "DT: 1.582, iter: 00854, train_loss: 6.0704, eval_loss: 0.0000, mem: 1939.33 MB\n",
      "DT: 1.559, iter: 00855, train_loss: 6.1137, eval_loss: 0.0000, mem: 1939.14 MB\n",
      "DT: 1.583, iter: 00856, train_loss: 6.1297, eval_loss: 0.0000, mem: 1939.08 MB\n",
      "DT: 1.589, iter: 00857, train_loss: 6.0550, eval_loss: 0.0000, mem: 1939.12 MB\n",
      "DT: 1.554, iter: 00858, train_loss: 6.1353, eval_loss: 0.0000, mem: 1939.04 MB\n",
      "DT: 1.626, iter: 00859, train_loss: 6.2161, eval_loss: 0.0000, mem: 1939.11 MB\n",
      "DT: 1.535, iter: 00860, train_loss: 5.8490, eval_loss: 0.0000, mem: 1939.17 MB\n",
      "DT: 1.541, iter: 00861, train_loss: 6.1795, eval_loss: 0.0000, mem: 1939.23 MB\n",
      "DT: 1.592, iter: 00862, train_loss: 5.9160, eval_loss: 0.0000, mem: 1939.20 MB\n",
      "DT: 1.698, iter: 00863, train_loss: 6.2449, eval_loss: 0.0000, mem: 1939.12 MB\n",
      "DT: 1.634, iter: 00864, train_loss: 6.0384, eval_loss: 0.0000, mem: 1939.18 MB\n",
      "DT: 1.571, iter: 00865, train_loss: 6.2163, eval_loss: 0.0000, mem: 1939.31 MB\n",
      "DT: 1.575, iter: 00866, train_loss: 5.9529, eval_loss: 0.0000, mem: 1938.95 MB\n",
      "DT: 1.590, iter: 00867, train_loss: 5.9257, eval_loss: 0.0000, mem: 1939.22 MB\n",
      "DT: 1.555, iter: 00868, train_loss: 5.9847, eval_loss: 0.0000, mem: 1939.41 MB\n",
      "DT: 1.666, iter: 00869, train_loss: 6.0358, eval_loss: 0.0000, mem: 1939.52 MB\n",
      "DT: 1.619, iter: 00870, train_loss: 5.9700, eval_loss: 0.0000, mem: 1907.61 MB\n",
      "DT: 1.580, iter: 00871, train_loss: 6.0613, eval_loss: 0.0000, mem: 1943.42 MB\n",
      "DT: 1.688, iter: 00872, train_loss: 6.0005, eval_loss: 0.0000, mem: 1918.94 MB\n",
      "DT: 1.599, iter: 00873, train_loss: 6.1187, eval_loss: 0.0000, mem: 1919.12 MB\n",
      "DT: 1.744, iter: 00874, train_loss: 5.8546, eval_loss: 0.0000, mem: 1919.18 MB\n",
      "DT: 1.601, iter: 00875, train_loss: 6.1946, eval_loss: 0.0000, mem: 1919.24 MB\n",
      "DT: 1.592, iter: 00876, train_loss: 5.9618, eval_loss: 0.0000, mem: 1919.33 MB\n",
      "DT: 1.583, iter: 00877, train_loss: 5.7800, eval_loss: 0.0000, mem: 1919.39 MB\n",
      "DT: 1.579, iter: 00878, train_loss: 6.0372, eval_loss: 0.0000, mem: 1921.34 MB\n",
      "DT: 1.609, iter: 00879, train_loss: 5.9821, eval_loss: 0.0000, mem: 1896.93 MB\n",
      "DT: 1.492, iter: 00880, train_loss: 6.0941, eval_loss: 0.0000, mem: 1896.95 MB\n",
      "DT: 1.565, iter: 00881, train_loss: 5.8740, eval_loss: 0.0000, mem: 1887.26 MB\n",
      "DT: 1.649, iter: 00882, train_loss: 5.9304, eval_loss: 0.0000, mem: 1911.07 MB\n",
      "DT: 1.571, iter: 00883, train_loss: 6.1398, eval_loss: 0.0000, mem: 1911.11 MB\n",
      "DT: 1.750, iter: 00884, train_loss: 6.0693, eval_loss: 0.0000, mem: 1911.06 MB\n",
      "DT: 1.569, iter: 00885, train_loss: 5.8655, eval_loss: 0.0000, mem: 1911.15 MB\n",
      "DT: 1.554, iter: 00886, train_loss: 5.9803, eval_loss: 0.0000, mem: 1911.15 MB\n",
      "DT: 1.583, iter: 00887, train_loss: 6.0195, eval_loss: 0.0000, mem: 1911.15 MB\n",
      "DT: 1.568, iter: 00888, train_loss: 6.2195, eval_loss: 0.0000, mem: 1911.39 MB\n",
      "DT: 1.563, iter: 00889, train_loss: 5.9876, eval_loss: 0.0000, mem: 1911.32 MB\n",
      "DT: 1.556, iter: 00890, train_loss: 6.1336, eval_loss: 0.0000, mem: 1911.22 MB\n",
      "DT: 1.564, iter: 00891, train_loss: 6.1588, eval_loss: 0.0000, mem: 1929.20 MB\n",
      "DT: 1.580, iter: 00892, train_loss: 5.9174, eval_loss: 0.0000, mem: 1893.42 MB\n",
      "DT: 1.559, iter: 00893, train_loss: 6.0921, eval_loss: 0.0000, mem: 1893.36 MB\n",
      "DT: 1.623, iter: 00894, train_loss: 6.0430, eval_loss: 0.0000, mem: 1905.30 MB\n",
      "DT: 1.542, iter: 00895, train_loss: 5.9529, eval_loss: 0.0000, mem: 1953.48 MB\n",
      "DT: 1.585, iter: 00896, train_loss: 6.0700, eval_loss: 0.0000, mem: 1929.59 MB\n",
      "DT: 1.585, iter: 00897, train_loss: 6.0151, eval_loss: 0.0000, mem: 1977.40 MB\n",
      "DT: 1.632, iter: 00898, train_loss: 5.8977, eval_loss: 0.0000, mem: 1952.91 MB\n",
      "DT: 1.519, iter: 00899, train_loss: 6.0059, eval_loss: 0.0000, mem: 1952.95 MB\n",
      "DT: 1.598, iter: 00900, train_loss: 6.1179, eval_loss: 0.0000, mem: 1952.91 MB\n",
      "DT: 1.611, iter: 00901, train_loss: 6.1003, eval_loss: 0.0000, mem: 1953.07 MB\n",
      "DT: 1.613, iter: 00902, train_loss: 5.9824, eval_loss: 0.0000, mem: 1953.12 MB\n",
      "DT: 1.614, iter: 00903, train_loss: 6.0304, eval_loss: 0.0000, mem: 1953.34 MB\n",
      "DT: 1.767, iter: 00904, train_loss: 5.8758, eval_loss: 0.0000, mem: 1953.34 MB\n",
      "DT: 1.692, iter: 00905, train_loss: 6.1011, eval_loss: 0.0000, mem: 1917.62 MB\n",
      "DT: 1.545, iter: 00906, train_loss: 6.1326, eval_loss: 0.0000, mem: 1941.43 MB\n",
      "DT: 1.574, iter: 00907, train_loss: 5.9817, eval_loss: 0.0000, mem: 1878.50 MB\n",
      "DT: 1.541, iter: 00908, train_loss: 6.2158, eval_loss: 0.0000, mem: 1914.43 MB\n",
      "DT: 1.582, iter: 00909, train_loss: 5.9933, eval_loss: 0.0000, mem: 1875.50 MB\n",
      "DT: 1.633, iter: 00910, train_loss: 6.0948, eval_loss: 0.0000, mem: 1844.79 MB\n",
      "DT: 1.585, iter: 00911, train_loss: 5.7747, eval_loss: 0.0000, mem: 1892.03 MB\n",
      "DT: 1.561, iter: 00912, train_loss: 6.1092, eval_loss: 0.0000, mem: 1940.06 MB\n",
      "DT: 1.518, iter: 00913, train_loss: 6.0682, eval_loss: 0.0000, mem: 1914.73 MB\n",
      "DT: 1.658, iter: 00914, train_loss: 5.9992, eval_loss: 0.0000, mem: 1950.69 MB\n",
      "DT: 1.680, iter: 00915, train_loss: 6.1698, eval_loss: 0.0000, mem: 1926.00 MB\n",
      "DT: 1.566, iter: 00916, train_loss: 6.0057, eval_loss: 0.0000, mem: 1926.06 MB\n",
      "DT: 1.581, iter: 00917, train_loss: 5.8029, eval_loss: 0.0000, mem: 1926.00 MB\n",
      "DT: 1.632, iter: 00918, train_loss: 5.8408, eval_loss: 0.0000, mem: 1926.19 MB\n",
      "DT: 1.580, iter: 00919, train_loss: 6.2278, eval_loss: 0.0000, mem: 1926.00 MB\n",
      "DT: 1.675, iter: 00920, train_loss: 6.0413, eval_loss: 0.0000, mem: 1857.48 MB\n",
      "DT: 1.665, iter: 00921, train_loss: 5.9171, eval_loss: 0.0000, mem: 1893.41 MB\n",
      "DT: 1.620, iter: 00922, train_loss: 6.0767, eval_loss: 0.0000, mem: 1929.35 MB\n",
      "DT: 1.554, iter: 00923, train_loss: 6.1076, eval_loss: 0.0000, mem: 1905.10 MB\n",
      "DT: 1.700, iter: 00924, train_loss: 6.0103, eval_loss: 0.0000, mem: 1905.22 MB\n",
      "DT: 1.561, iter: 00925, train_loss: 5.8019, eval_loss: 0.0000, mem: 1905.16 MB\n",
      "DT: 1.591, iter: 00926, train_loss: 5.8414, eval_loss: 0.0000, mem: 1850.90 MB\n",
      "DT: 1.611, iter: 00927, train_loss: 6.0693, eval_loss: 0.0000, mem: 1898.71 MB\n",
      "DT: 1.536, iter: 00928, train_loss: 6.1683, eval_loss: 0.0000, mem: 1934.76 MB\n",
      "DT: 1.538, iter: 00929, train_loss: 6.0159, eval_loss: 0.0000, mem: 1873.79 MB\n",
      "DT: 1.600, iter: 00930, train_loss: 6.0879, eval_loss: 0.0000, mem: 1873.73 MB\n",
      "DT: 1.573, iter: 00931, train_loss: 6.0552, eval_loss: 0.0000, mem: 1920.50 MB\n",
      "DT: 1.622, iter: 00932, train_loss: 5.9565, eval_loss: 0.0000, mem: 1896.26 MB\n",
      "DT: 1.538, iter: 00933, train_loss: 6.1293, eval_loss: 0.0000, mem: 1860.12 MB\n",
      "DT: 1.594, iter: 00934, train_loss: 6.1232, eval_loss: 0.0000, mem: 1932.03 MB\n",
      "DT: 1.719, iter: 00935, train_loss: 5.8990, eval_loss: 0.0000, mem: 1955.97 MB\n",
      "DT: 1.592, iter: 00936, train_loss: 6.0597, eval_loss: 0.0000, mem: 1906.33 MB\n",
      "DT: 1.615, iter: 00937, train_loss: 6.0982, eval_loss: 0.0000, mem: 1906.52 MB\n",
      "DT: 1.562, iter: 00938, train_loss: 5.8929, eval_loss: 0.0000, mem: 1873.20 MB\n",
      "DT: 1.636, iter: 00939, train_loss: 5.9253, eval_loss: 0.0000, mem: 1873.39 MB\n",
      "DT: 1.692, iter: 00940, train_loss: 5.9634, eval_loss: 0.0000, mem: 1873.57 MB\n",
      "DT: 1.579, iter: 00941, train_loss: 5.9545, eval_loss: 0.0000, mem: 1900.59 MB\n",
      "DT: 1.629, iter: 00942, train_loss: 6.0960, eval_loss: 0.0000, mem: 1864.50 MB\n",
      "DT: 1.642, iter: 00943, train_loss: 5.9594, eval_loss: 0.0000, mem: 1875.56 MB\n",
      "DT: 1.598, iter: 00944, train_loss: 5.8612, eval_loss: 0.0000, mem: 1957.39 MB\n",
      "DT: 1.721, iter: 00945, train_loss: 6.0774, eval_loss: 0.0000, mem: 1873.47 MB\n",
      "DT: 1.602, iter: 00946, train_loss: 5.9077, eval_loss: 0.0000, mem: 1848.88 MB\n",
      "DT: 1.562, iter: 00947, train_loss: 5.8797, eval_loss: 0.0000, mem: 1872.70 MB\n",
      "DT: 1.670, iter: 00948, train_loss: 6.0726, eval_loss: 0.0000, mem: 1908.49 MB\n",
      "DT: 1.632, iter: 00949, train_loss: 6.0758, eval_loss: 0.0000, mem: 1932.55 MB\n",
      "DT: 1.638, iter: 00950, train_loss: 6.1603, eval_loss: 0.0000, mem: 1792.44 MB\n",
      "DT: 1.587, iter: 00951, train_loss: 6.0904, eval_loss: 0.0000, mem: 1865.59 MB\n",
      "DT: 1.622, iter: 00952, train_loss: 6.0532, eval_loss: 0.0000, mem: 1925.73 MB\n",
      "DT: 1.536, iter: 00953, train_loss: 5.9298, eval_loss: 0.0000, mem: 1901.50 MB\n",
      "DT: 1.589, iter: 00954, train_loss: 5.9606, eval_loss: 0.0000, mem: 1913.47 MB\n",
      "DT: 1.627, iter: 00955, train_loss: 6.1055, eval_loss: 0.0000, mem: 1913.79 MB\n",
      "DT: 1.562, iter: 00956, train_loss: 6.2303, eval_loss: 0.0000, mem: 1913.68 MB\n",
      "DT: 1.641, iter: 00957, train_loss: 6.0155, eval_loss: 0.0000, mem: 1888.08 MB\n",
      "DT: 1.618, iter: 00958, train_loss: 6.1412, eval_loss: 0.0000, mem: 1912.14 MB\n",
      "DT: 1.579, iter: 00959, train_loss: 5.8926, eval_loss: 0.0000, mem: 1887.86 MB\n",
      "DT: 1.573, iter: 00960, train_loss: 6.0286, eval_loss: 0.0000, mem: 1887.85 MB\n",
      "DT: 1.560, iter: 00961, train_loss: 5.9788, eval_loss: 0.0000, mem: 1824.84 MB\n",
      "DT: 1.551, iter: 00962, train_loss: 5.9807, eval_loss: 0.0000, mem: 1836.80 MB\n",
      "DT: 1.568, iter: 00963, train_loss: 5.8888, eval_loss: 0.0000, mem: 1909.42 MB\n",
      "DT: 1.586, iter: 00964, train_loss: 6.0536, eval_loss: 0.0000, mem: 1945.67 MB\n",
      "DT: 1.606, iter: 00965, train_loss: 5.9202, eval_loss: 0.0000, mem: 1922.14 MB\n",
      "DT: 1.627, iter: 00966, train_loss: 6.0163, eval_loss: 0.0000, mem: 1844.76 MB\n",
      "DT: 1.618, iter: 00967, train_loss: 6.0261, eval_loss: 0.0000, mem: 1892.46 MB\n",
      "DT: 1.530, iter: 00968, train_loss: 6.0381, eval_loss: 0.0000, mem: 1844.13 MB\n",
      "DT: 1.647, iter: 00969, train_loss: 6.1626, eval_loss: 0.0000, mem: 1880.16 MB\n",
      "DT: 1.556, iter: 00970, train_loss: 5.9591, eval_loss: 0.0000, mem: 1880.13 MB\n",
      "DT: 1.630, iter: 00971, train_loss: 5.8517, eval_loss: 0.0000, mem: 1891.54 MB\n",
      "DT: 1.907, iter: 00972, train_loss: 5.9306, eval_loss: 0.0000, mem: 1891.36 MB\n",
      "DT: 1.578, iter: 00973, train_loss: 6.0543, eval_loss: 0.0000, mem: 1891.36 MB\n",
      "DT: 1.702, iter: 00974, train_loss: 5.9642, eval_loss: 0.0000, mem: 1891.70 MB\n",
      "DT: 1.592, iter: 00975, train_loss: 6.1335, eval_loss: 0.0000, mem: 1911.52 MB\n",
      "DT: 1.702, iter: 00976, train_loss: 5.9613, eval_loss: 0.0000, mem: 1875.65 MB\n",
      "DT: 1.513, iter: 00977, train_loss: 5.8191, eval_loss: 0.0000, mem: 1875.47 MB\n",
      "DT: 1.513, iter: 00978, train_loss: 5.8637, eval_loss: 0.0000, mem: 1849.99 MB\n",
      "DT: 1.571, iter: 00979, train_loss: 6.0028, eval_loss: 0.0000, mem: 1885.05 MB\n",
      "DT: 1.628, iter: 00980, train_loss: 6.1980, eval_loss: 0.0000, mem: 1884.99 MB\n",
      "DT: 1.632, iter: 00981, train_loss: 6.0275, eval_loss: 0.0000, mem: 1885.12 MB\n",
      "DT: 1.601, iter: 00982, train_loss: 5.8004, eval_loss: 0.0000, mem: 1885.18 MB\n",
      "DT: 1.539, iter: 00983, train_loss: 5.9395, eval_loss: 0.0000, mem: 1885.13 MB\n",
      "DT: 1.673, iter: 00984, train_loss: 5.9267, eval_loss: 0.0000, mem: 1885.04 MB\n",
      "DT: 1.607, iter: 00985, train_loss: 5.8668, eval_loss: 0.0000, mem: 1890.66 MB\n",
      "DT: 1.581, iter: 00986, train_loss: 6.1046, eval_loss: 0.0000, mem: 1890.60 MB\n",
      "DT: 1.602, iter: 00987, train_loss: 5.8043, eval_loss: 0.0000, mem: 1901.16 MB\n",
      "DT: 1.561, iter: 00988, train_loss: 5.9682, eval_loss: 0.0000, mem: 1876.44 MB\n",
      "DT: 1.573, iter: 00989, train_loss: 6.1844, eval_loss: 0.0000, mem: 1900.25 MB\n",
      "DT: 1.597, iter: 00990, train_loss: 5.8744, eval_loss: 0.0000, mem: 1900.47 MB\n",
      "DT: 1.613, iter: 00991, train_loss: 6.0042, eval_loss: 0.0000, mem: 1917.73 MB\n",
      "DT: 1.556, iter: 00992, train_loss: 6.0456, eval_loss: 0.0000, mem: 1893.25 MB\n",
      "DT: 1.654, iter: 00993, train_loss: 6.0778, eval_loss: 0.0000, mem: 1865.90 MB\n",
      "DT: 1.611, iter: 00994, train_loss: 6.2052, eval_loss: 0.0000, mem: 1876.37 MB\n",
      "DT: 1.663, iter: 00995, train_loss: 5.8931, eval_loss: 0.0000, mem: 1912.43 MB\n",
      "DT: 1.633, iter: 00996, train_loss: 5.9295, eval_loss: 0.0000, mem: 1924.67 MB\n",
      "DT: 1.619, iter: 00997, train_loss: 5.8848, eval_loss: 0.0000, mem: 1888.87 MB\n",
      "DT: 1.539, iter: 00998, train_loss: 6.0056, eval_loss: 0.0000, mem: 1862.70 MB\n",
      "DT: 1.603, iter: 00999, train_loss: 6.0155, eval_loss: 0.0000, mem: 1862.64 MB\n",
      "DT: 1.626, iter: 01000, train_loss: 5.9643, eval_loss: 0.0000, mem: 1867.06 MB\n",
      "DT: 1.565, iter: 01001, train_loss: 6.0076, eval_loss: 0.0000, mem: 1885.21 MB\n",
      "DT: 1.521, iter: 01002, train_loss: 6.3210, eval_loss: 0.0000, mem: 1861.12 MB\n",
      "DT: 1.582, iter: 01003, train_loss: 5.9626, eval_loss: 0.0000, mem: 1896.93 MB\n",
      "DT: 1.601, iter: 01004, train_loss: 6.0081, eval_loss: 0.0000, mem: 1921.37 MB\n",
      "DT: 1.556, iter: 01005, train_loss: 6.0308, eval_loss: 0.0000, mem: 1921.30 MB\n",
      "DT: 1.608, iter: 01006, train_loss: 6.0326, eval_loss: 0.0000, mem: 1921.62 MB\n",
      "DT: 1.567, iter: 01007, train_loss: 6.1705, eval_loss: 0.0000, mem: 1921.84 MB\n",
      "DT: 1.549, iter: 01008, train_loss: 5.9675, eval_loss: 0.0000, mem: 1897.50 MB\n",
      "DT: 1.620, iter: 01009, train_loss: 5.9202, eval_loss: 0.0000, mem: 1897.26 MB\n",
      "DT: 1.488, iter: 01010, train_loss: 6.0525, eval_loss: 0.0000, mem: 1897.32 MB\n",
      "DT: 1.542, iter: 01011, train_loss: 5.9266, eval_loss: 0.0000, mem: 1897.23 MB\n",
      "DT: 1.634, iter: 01012, train_loss: 6.0385, eval_loss: 0.0000, mem: 1897.67 MB\n",
      "DT: 1.652, iter: 01013, train_loss: 5.9519, eval_loss: 0.0000, mem: 1897.80 MB\n",
      "DT: 1.672, iter: 01014, train_loss: 5.8959, eval_loss: 0.0000, mem: 1926.73 MB\n",
      "DT: 1.574, iter: 01015, train_loss: 5.9567, eval_loss: 0.0000, mem: 1878.05 MB\n",
      "DT: 1.588, iter: 01016, train_loss: 6.0602, eval_loss: 0.0000, mem: 1878.26 MB\n",
      "DT: 1.552, iter: 01017, train_loss: 5.9975, eval_loss: 0.0000, mem: 1918.73 MB\n",
      "DT: 1.481, iter: 01018, train_loss: 5.8292, eval_loss: 0.0000, mem: 1882.38 MB\n",
      "DT: 1.597, iter: 01019, train_loss: 6.0683, eval_loss: 0.0000, mem: 1882.29 MB\n",
      "DT: 1.568, iter: 01020, train_loss: 5.9597, eval_loss: 0.0000, mem: 1882.22 MB\n",
      "DT: 1.541, iter: 01021, train_loss: 5.8908, eval_loss: 0.0000, mem: 1892.67 MB\n",
      "DT: 1.518, iter: 01022, train_loss: 5.9712, eval_loss: 0.0000, mem: 1916.53 MB\n",
      "DT: 1.488, iter: 01023, train_loss: 5.9275, eval_loss: 0.0000, mem: 1916.67 MB\n",
      "DT: 1.558, iter: 01024, train_loss: 6.0108, eval_loss: 0.0000, mem: 1916.54 MB\n",
      "DT: 1.555, iter: 01025, train_loss: 5.8183, eval_loss: 0.0000, mem: 1916.46 MB\n",
      "DT: 1.582, iter: 01026, train_loss: 6.0895, eval_loss: 0.0000, mem: 1916.78 MB\n",
      "DT: 1.589, iter: 01027, train_loss: 5.9648, eval_loss: 0.0000, mem: 1949.05 MB\n",
      "DT: 1.577, iter: 01028, train_loss: 5.9777, eval_loss: 0.0000, mem: 1899.61 MB\n",
      "DT: 1.618, iter: 01029, train_loss: 5.9855, eval_loss: 0.0000, mem: 1899.67 MB\n",
      "DT: 1.604, iter: 01030, train_loss: 5.9290, eval_loss: 0.0000, mem: 1910.31 MB\n",
      "DT: 1.660, iter: 01031, train_loss: 5.9956, eval_loss: 0.0000, mem: 1910.21 MB\n",
      "DT: 1.617, iter: 01032, train_loss: 5.9291, eval_loss: 0.0000, mem: 1910.14 MB\n",
      "DT: 1.563, iter: 01033, train_loss: 5.9239, eval_loss: 0.0000, mem: 1910.16 MB\n",
      "DT: 1.575, iter: 01034, train_loss: 5.7989, eval_loss: 0.0000, mem: 1910.17 MB\n",
      "DT: 1.591, iter: 01035, train_loss: 5.8769, eval_loss: 0.0000, mem: 1910.23 MB\n",
      "DT: 1.571, iter: 01036, train_loss: 5.9809, eval_loss: 0.0000, mem: 1910.16 MB\n",
      "DT: 1.575, iter: 01037, train_loss: 5.9049, eval_loss: 0.0000, mem: 1910.22 MB\n",
      "DT: 1.570, iter: 01038, train_loss: 5.8823, eval_loss: 0.0000, mem: 1852.28 MB\n",
      "DT: 1.601, iter: 01039, train_loss: 6.0250, eval_loss: 0.0000, mem: 1852.54 MB\n",
      "DT: 1.547, iter: 01040, train_loss: 6.0975, eval_loss: 0.0000, mem: 1910.34 MB\n",
      "DT: 1.557, iter: 01041, train_loss: 5.8428, eval_loss: 0.0000, mem: 1862.06 MB\n",
      "DT: 1.600, iter: 01042, train_loss: 5.9670, eval_loss: 0.0000, mem: 1862.46 MB\n",
      "DT: 1.541, iter: 01043, train_loss: 6.0666, eval_loss: 0.0000, mem: 1862.41 MB\n",
      "DT: 1.603, iter: 01044, train_loss: 5.9134, eval_loss: 0.0000, mem: 1862.13 MB\n",
      "DT: 1.668, iter: 01045, train_loss: 6.0663, eval_loss: 0.0000, mem: 1862.20 MB\n",
      "DT: 1.644, iter: 01046, train_loss: 5.9496, eval_loss: 0.0000, mem: 1849.51 MB\n",
      "DT: 1.641, iter: 01047, train_loss: 6.0922, eval_loss: 0.0000, mem: 1897.43 MB\n",
      "DT: 1.600, iter: 01048, train_loss: 5.9095, eval_loss: 0.0000, mem: 1873.52 MB\n",
      "DT: 1.513, iter: 01049, train_loss: 5.9771, eval_loss: 0.0000, mem: 1873.55 MB\n",
      "DT: 1.580, iter: 01050, train_loss: 6.0083, eval_loss: 0.0000, mem: 1945.02 MB\n",
      "DT: 1.581, iter: 01051, train_loss: 5.8932, eval_loss: 0.0000, mem: 1944.94 MB\n",
      "DT: 1.639, iter: 01052, train_loss: 5.7947, eval_loss: 0.0000, mem: 1945.13 MB\n",
      "DT: 1.606, iter: 01053, train_loss: 5.9196, eval_loss: 0.0000, mem: 1944.78 MB\n",
      "DT: 1.694, iter: 01054, train_loss: 6.0308, eval_loss: 0.0000, mem: 1920.27 MB\n",
      "DT: 1.560, iter: 01055, train_loss: 5.9382, eval_loss: 0.0000, mem: 1920.37 MB\n",
      "DT: 1.534, iter: 01056, train_loss: 5.9107, eval_loss: 0.0000, mem: 1920.43 MB\n",
      "DT: 1.596, iter: 01057, train_loss: 6.0485, eval_loss: 0.0000, mem: 1920.37 MB\n",
      "DT: 1.628, iter: 01058, train_loss: 5.7111, eval_loss: 0.0000, mem: 1920.45 MB\n",
      "DT: 1.697, iter: 01059, train_loss: 6.0110, eval_loss: 0.0000, mem: 1921.66 MB\n",
      "DT: 1.636, iter: 01060, train_loss: 5.8481, eval_loss: 0.0000, mem: 1921.72 MB\n",
      "DT: 1.701, iter: 01061, train_loss: 5.8835, eval_loss: 0.0000, mem: 1921.64 MB\n",
      "DT: 1.563, iter: 01062, train_loss: 5.9538, eval_loss: 0.0000, mem: 1921.70 MB\n",
      "DT: 1.542, iter: 01063, train_loss: 5.9668, eval_loss: 0.0000, mem: 1921.66 MB\n",
      "DT: 1.697, iter: 01064, train_loss: 5.8681, eval_loss: 0.0000, mem: 1921.80 MB\n",
      "DT: 1.617, iter: 01065, train_loss: 5.9887, eval_loss: 0.0000, mem: 1945.74 MB\n",
      "DT: 1.613, iter: 01066, train_loss: 5.9847, eval_loss: 0.0000, mem: 1909.93 MB\n",
      "DT: 1.571, iter: 01067, train_loss: 6.1212, eval_loss: 0.0000, mem: 1910.08 MB\n",
      "DT: 1.555, iter: 01068, train_loss: 5.9658, eval_loss: 0.0000, mem: 1910.02 MB\n",
      "DT: 1.605, iter: 01069, train_loss: 5.7973, eval_loss: 0.0000, mem: 1909.97 MB\n",
      "DT: 1.561, iter: 01070, train_loss: 5.8916, eval_loss: 0.0000, mem: 1909.90 MB\n",
      "DT: 1.568, iter: 01071, train_loss: 6.0783, eval_loss: 0.0000, mem: 1864.19 MB\n",
      "DT: 1.505, iter: 01072, train_loss: 5.8949, eval_loss: 0.0000, mem: 1947.81 MB\n",
      "DT: 1.655, iter: 01073, train_loss: 6.0393, eval_loss: 0.0000, mem: 1971.91 MB\n",
      "DT: 1.702, iter: 01074, train_loss: 5.8537, eval_loss: 0.0000, mem: 1933.49 MB\n",
      "DT: 1.628, iter: 01075, train_loss: 5.9635, eval_loss: 0.0000, mem: 1933.32 MB\n",
      "DT: 1.539, iter: 01076, train_loss: 6.0872, eval_loss: 0.0000, mem: 1933.38 MB\n",
      "DT: 1.573, iter: 01077, train_loss: 6.0390, eval_loss: 0.0000, mem: 1933.45 MB\n",
      "DT: 1.548, iter: 01078, train_loss: 5.9772, eval_loss: 0.0000, mem: 1909.51 MB\n",
      "DT: 1.601, iter: 01079, train_loss: 6.0241, eval_loss: 0.0000, mem: 1842.20 MB\n",
      "DT: 1.620, iter: 01080, train_loss: 5.8707, eval_loss: 0.0000, mem: 1925.28 MB\n",
      "DT: 1.535, iter: 01081, train_loss: 6.1577, eval_loss: 0.0000, mem: 1937.32 MB\n",
      "DT: 1.663, iter: 01082, train_loss: 5.8258, eval_loss: 0.0000, mem: 1937.41 MB\n",
      "DT: 1.612, iter: 01083, train_loss: 5.8586, eval_loss: 0.0000, mem: 1911.57 MB\n",
      "DT: 1.643, iter: 01084, train_loss: 5.9565, eval_loss: 0.0000, mem: 1935.51 MB\n",
      "DT: 1.584, iter: 01085, train_loss: 6.1238, eval_loss: 0.0000, mem: 1830.05 MB\n",
      "DT: 1.606, iter: 01086, train_loss: 5.7706, eval_loss: 0.0000, mem: 1866.11 MB\n",
      "DT: 1.772, iter: 01087, train_loss: 5.9547, eval_loss: 0.0000, mem: 1830.39 MB\n",
      "DT: 1.473, iter: 01088, train_loss: 6.0387, eval_loss: 0.0000, mem: 1878.07 MB\n",
      "DT: 1.592, iter: 01089, train_loss: 5.9132, eval_loss: 0.0000, mem: 1878.09 MB\n",
      "DT: 1.567, iter: 01090, train_loss: 6.0092, eval_loss: 0.0000, mem: 1937.96 MB\n",
      "DT: 1.530, iter: 01091, train_loss: 6.0204, eval_loss: 0.0000, mem: 1874.57 MB\n",
      "DT: 1.667, iter: 01092, train_loss: 5.9839, eval_loss: 0.0000, mem: 1874.56 MB\n",
      "DT: 1.587, iter: 01093, train_loss: 5.9626, eval_loss: 0.0000, mem: 1898.53 MB\n",
      "DT: 1.582, iter: 01094, train_loss: 5.9806, eval_loss: 0.0000, mem: 1874.51 MB\n",
      "DT: 1.601, iter: 01095, train_loss: 6.0267, eval_loss: 0.0000, mem: 1922.39 MB\n",
      "DT: 1.567, iter: 01096, train_loss: 5.8597, eval_loss: 0.0000, mem: 1934.33 MB\n",
      "DT: 1.596, iter: 01097, train_loss: 5.8909, eval_loss: 0.0000, mem: 1874.71 MB\n",
      "DT: 1.523, iter: 01098, train_loss: 5.7967, eval_loss: 0.0000, mem: 1874.68 MB\n",
      "DT: 1.614, iter: 01099, train_loss: 5.9891, eval_loss: 0.0000, mem: 1934.38 MB\n",
      "DT: 1.584, iter: 01100, train_loss: 5.7429, eval_loss: 0.0000, mem: 1886.41 MB\n",
      "DT: 1.719, iter: 01101, train_loss: 6.1152, eval_loss: 0.0000, mem: 1886.47 MB\n",
      "DT: 1.748, iter: 01102, train_loss: 6.0360, eval_loss: 0.0000, mem: 1886.37 MB\n",
      "DT: 1.606, iter: 01103, train_loss: 5.9518, eval_loss: 0.0000, mem: 1902.93 MB\n",
      "DT: 1.687, iter: 01104, train_loss: 5.8805, eval_loss: 0.0000, mem: 1939.00 MB\n",
      "DT: 1.648, iter: 01105, train_loss: 5.8219, eval_loss: 0.0000, mem: 1879.11 MB\n",
      "DT: 1.623, iter: 01106, train_loss: 6.0248, eval_loss: 0.0000, mem: 1878.96 MB\n",
      "DT: 1.582, iter: 01107, train_loss: 6.0405, eval_loss: 0.0000, mem: 1878.86 MB\n",
      "DT: 1.543, iter: 01108, train_loss: 6.0702, eval_loss: 0.0000, mem: 1879.04 MB\n",
      "DT: 1.537, iter: 01109, train_loss: 5.9318, eval_loss: 0.0000, mem: 1888.59 MB\n",
      "DT: 1.705, iter: 01110, train_loss: 5.8912, eval_loss: 0.0000, mem: 1852.79 MB\n",
      "DT: 1.608, iter: 01111, train_loss: 5.8060, eval_loss: 0.0000, mem: 1863.90 MB\n",
      "DT: 1.671, iter: 01112, train_loss: 5.9843, eval_loss: 0.0000, mem: 1871.34 MB\n",
      "DT: 1.590, iter: 01113, train_loss: 5.9349, eval_loss: 0.0000, mem: 1955.40 MB\n",
      "DT: 1.587, iter: 01114, train_loss: 5.9734, eval_loss: 0.0000, mem: 1858.71 MB\n",
      "DT: 1.636, iter: 01115, train_loss: 5.8980, eval_loss: 0.0000, mem: 1894.53 MB\n",
      "DT: 1.646, iter: 01116, train_loss: 5.8689, eval_loss: 0.0000, mem: 1906.59 MB\n",
      "DT: 1.643, iter: 01117, train_loss: 5.9308, eval_loss: 0.0000, mem: 1906.61 MB\n",
      "DT: 1.680, iter: 01118, train_loss: 6.0029, eval_loss: 0.0000, mem: 1906.58 MB\n",
      "DT: 1.522, iter: 01119, train_loss: 6.1082, eval_loss: 0.0000, mem: 1854.48 MB\n",
      "DT: 1.559, iter: 01120, train_loss: 5.6522, eval_loss: 0.0000, mem: 1914.33 MB\n",
      "DT: 1.661, iter: 01121, train_loss: 5.7166, eval_loss: 0.0000, mem: 1854.50 MB\n",
      "DT: 1.605, iter: 01122, train_loss: 6.0079, eval_loss: 0.0000, mem: 1830.45 MB\n",
      "DT: 1.563, iter: 01123, train_loss: 5.9567, eval_loss: 0.0000, mem: 1830.48 MB\n",
      "DT: 1.656, iter: 01124, train_loss: 5.8273, eval_loss: 0.0000, mem: 1854.29 MB\n",
      "DT: 1.558, iter: 01125, train_loss: 5.8247, eval_loss: 0.0000, mem: 1890.39 MB\n",
      "DT: 1.544, iter: 01126, train_loss: 6.0682, eval_loss: 0.0000, mem: 1853.67 MB\n",
      "DT: 1.547, iter: 01127, train_loss: 6.0764, eval_loss: 0.0000, mem: 1875.43 MB\n",
      "DT: 1.566, iter: 01128, train_loss: 6.0754, eval_loss: 0.0000, mem: 1911.37 MB\n",
      "DT: 1.573, iter: 01129, train_loss: 6.1340, eval_loss: 0.0000, mem: 1911.30 MB\n",
      "DT: 1.632, iter: 01130, train_loss: 5.9245, eval_loss: 0.0000, mem: 1911.39 MB\n",
      "DT: 1.538, iter: 01131, train_loss: 5.8074, eval_loss: 0.0000, mem: 1911.33 MB\n",
      "DT: 1.552, iter: 01132, train_loss: 5.8776, eval_loss: 0.0000, mem: 1934.57 MB\n",
      "DT: 1.563, iter: 01133, train_loss: 6.0246, eval_loss: 0.0000, mem: 1910.73 MB\n",
      "DT: 1.572, iter: 01134, train_loss: 5.6260, eval_loss: 0.0000, mem: 1910.79 MB\n",
      "DT: 1.513, iter: 01135, train_loss: 5.8826, eval_loss: 0.0000, mem: 1910.98 MB\n",
      "DT: 1.534, iter: 01136, train_loss: 5.8750, eval_loss: 0.0000, mem: 1911.04 MB\n",
      "DT: 1.559, iter: 01137, train_loss: 6.1640, eval_loss: 0.0000, mem: 1934.86 MB\n",
      "DT: 1.597, iter: 01138, train_loss: 5.8514, eval_loss: 0.0000, mem: 1946.79 MB\n",
      "DT: 1.577, iter: 01139, train_loss: 5.9623, eval_loss: 0.0000, mem: 1946.88 MB\n",
      "DT: 1.650, iter: 01140, train_loss: 5.9446, eval_loss: 0.0000, mem: 1946.80 MB\n",
      "DT: 1.512, iter: 01141, train_loss: 6.0225, eval_loss: 0.0000, mem: 1946.86 MB\n",
      "DT: 1.540, iter: 01142, train_loss: 5.8837, eval_loss: 0.0000, mem: 1946.92 MB\n",
      "DT: 1.488, iter: 01143, train_loss: 5.7811, eval_loss: 0.0000, mem: 1863.73 MB\n",
      "DT: 1.503, iter: 01144, train_loss: 5.7589, eval_loss: 0.0000, mem: 1863.77 MB\n",
      "DT: 1.492, iter: 01145, train_loss: 5.9856, eval_loss: 0.0000, mem: 1902.97 MB\n",
      "DT: 1.501, iter: 01146, train_loss: 5.9802, eval_loss: 0.0000, mem: 1914.91 MB\n",
      "DT: 1.512, iter: 01147, train_loss: 5.8950, eval_loss: 0.0000, mem: 1863.94 MB\n",
      "DT: 1.546, iter: 01148, train_loss: 5.9012, eval_loss: 0.0000, mem: 1887.72 MB\n",
      "DT: 1.499, iter: 01149, train_loss: 6.1350, eval_loss: 0.0000, mem: 1864.13 MB\n",
      "DT: 1.617, iter: 01150, train_loss: 5.8055, eval_loss: 0.0000, mem: 1864.27 MB\n",
      "DT: 1.890, iter: 01151, train_loss: 6.0299, eval_loss: 0.0000, mem: 1864.07 MB\n",
      "DT: 1.636, iter: 01152, train_loss: 6.0070, eval_loss: 0.0000, mem: 1898.15 MB\n",
      "DT: 1.520, iter: 01153, train_loss: 5.6570, eval_loss: 0.0000, mem: 1873.56 MB\n",
      "DT: 1.762, iter: 01154, train_loss: 5.7997, eval_loss: 0.0000, mem: 1873.76 MB\n",
      "DT: 1.669, iter: 01155, train_loss: 6.0135, eval_loss: 0.0000, mem: 1896.06 MB\n",
      "DT: 1.639, iter: 01156, train_loss: 5.9068, eval_loss: 0.0000, mem: 1944.05 MB\n",
      "DT: 1.619, iter: 01157, train_loss: 6.0961, eval_loss: 0.0000, mem: 1919.60 MB\n",
      "DT: 1.515, iter: 01158, train_loss: 6.0395, eval_loss: 0.0000, mem: 1955.54 MB\n",
      "DT: 1.660, iter: 01159, train_loss: 5.8725, eval_loss: 0.0000, mem: 1955.35 MB\n",
      "DT: 1.542, iter: 01160, train_loss: 5.8201, eval_loss: 0.0000, mem: 1955.60 MB\n",
      "DT: 1.596, iter: 01161, train_loss: 6.0550, eval_loss: 0.0000, mem: 1955.58 MB\n",
      "DT: 1.619, iter: 01162, train_loss: 5.8863, eval_loss: 0.0000, mem: 1910.80 MB\n",
      "DT: 1.554, iter: 01163, train_loss: 5.8292, eval_loss: 0.0000, mem: 1884.49 MB\n",
      "DT: 1.574, iter: 01164, train_loss: 5.9005, eval_loss: 0.0000, mem: 1896.30 MB\n",
      "DT: 1.603, iter: 01165, train_loss: 5.9492, eval_loss: 0.0000, mem: 1944.32 MB\n",
      "DT: 1.648, iter: 01166, train_loss: 5.8452, eval_loss: 0.0000, mem: 1944.72 MB\n",
      "DT: 1.587, iter: 01167, train_loss: 5.8071, eval_loss: 0.0000, mem: 1944.66 MB\n",
      "DT: 1.607, iter: 01168, train_loss: 6.1079, eval_loss: 0.0000, mem: 1944.64 MB\n",
      "DT: 1.536, iter: 01169, train_loss: 6.0699, eval_loss: 0.0000, mem: 1895.75 MB\n",
      "DT: 1.653, iter: 01170, train_loss: 5.7313, eval_loss: 0.0000, mem: 1895.87 MB\n",
      "DT: 1.542, iter: 01171, train_loss: 5.7581, eval_loss: 0.0000, mem: 1895.70 MB\n",
      "DT: 1.563, iter: 01172, train_loss: 6.0103, eval_loss: 0.0000, mem: 1924.87 MB\n",
      "DT: 1.533, iter: 01173, train_loss: 5.9582, eval_loss: 0.0000, mem: 1924.88 MB\n",
      "DT: 1.530, iter: 01174, train_loss: 5.8603, eval_loss: 0.0000, mem: 1924.99 MB\n",
      "DT: 1.597, iter: 01175, train_loss: 6.0930, eval_loss: 0.0000, mem: 1951.06 MB\n",
      "DT: 1.614, iter: 01176, train_loss: 5.8989, eval_loss: 0.0000, mem: 1924.37 MB\n",
      "DT: 1.579, iter: 01177, train_loss: 5.9189, eval_loss: 0.0000, mem: 1936.18 MB\n",
      "DT: 1.538, iter: 01178, train_loss: 6.0344, eval_loss: 0.0000, mem: 1936.34 MB\n",
      "DT: 1.542, iter: 01179, train_loss: 6.0023, eval_loss: 0.0000, mem: 1936.34 MB\n",
      "DT: 1.594, iter: 01180, train_loss: 5.9141, eval_loss: 0.0000, mem: 1907.35 MB\n",
      "DT: 1.544, iter: 01181, train_loss: 6.0086, eval_loss: 0.0000, mem: 1907.50 MB\n",
      "DT: 1.575, iter: 01182, train_loss: 5.9770, eval_loss: 0.0000, mem: 1907.60 MB\n",
      "DT: 1.512, iter: 01183, train_loss: 5.9610, eval_loss: 0.0000, mem: 1907.43 MB\n",
      "DT: 1.511, iter: 01184, train_loss: 5.9706, eval_loss: 0.0000, mem: 1919.08 MB\n",
      "DT: 1.565, iter: 01185, train_loss: 5.8953, eval_loss: 0.0000, mem: 1919.20 MB\n",
      "DT: 1.676, iter: 01186, train_loss: 5.9099, eval_loss: 0.0000, mem: 1889.56 MB\n",
      "DT: 1.539, iter: 01187, train_loss: 5.9152, eval_loss: 0.0000, mem: 1949.14 MB\n",
      "DT: 1.675, iter: 01188, train_loss: 5.7851, eval_loss: 0.0000, mem: 1782.37 MB\n",
      "DT: 1.677, iter: 01189, train_loss: 5.9537, eval_loss: 0.0000, mem: 1905.32 MB\n",
      "DT: 1.690, iter: 01190, train_loss: 5.9586, eval_loss: 0.0000, mem: 1892.85 MB\n",
      "DT: 1.563, iter: 01191, train_loss: 6.0008, eval_loss: 0.0000, mem: 1894.26 MB\n",
      "DT: 1.559, iter: 01192, train_loss: 5.9889, eval_loss: 0.0000, mem: 1894.40 MB\n",
      "DT: 1.560, iter: 01193, train_loss: 5.9515, eval_loss: 0.0000, mem: 1904.72 MB\n",
      "DT: 1.565, iter: 01194, train_loss: 5.9864, eval_loss: 0.0000, mem: 1904.66 MB\n",
      "DT: 1.583, iter: 01195, train_loss: 5.9246, eval_loss: 0.0000, mem: 1904.98 MB\n",
      "DT: 1.545, iter: 01196, train_loss: 5.9362, eval_loss: 0.0000, mem: 1871.43 MB\n",
      "DT: 1.640, iter: 01197, train_loss: 5.9777, eval_loss: 0.0000, mem: 1904.41 MB\n",
      "DT: 1.585, iter: 01198, train_loss: 6.0536, eval_loss: 0.0000, mem: 1879.00 MB\n",
      "DT: 1.607, iter: 01199, train_loss: 5.9912, eval_loss: 0.0000, mem: 1891.05 MB\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=config['wandb_project'], name='mingpt_' + config['wandb_name'], config=config)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "def on_batch_end(t):\n",
    "    # if t.iter_num % 2 == 0: \n",
    "    eval_loss = 0 # custom_evaluate(t.model, device=t.device)\n",
    "    mem = get_mem()\n",
    "    print(f'DT: {t.iter_dt:.3f}, iter: {t.iter_num:05d}, train_loss: {t.loss:.4f}, eval_loss: {eval_loss:.4f}, mem: {mem / (1024 * 1024):.2f} MB')\n",
    "    wandb.log({\n",
    "        \"time_s\": t.iter_time - start,\n",
    "        \"iter\": t.iter_num,\n",
    "        \"train/loss\": t.loss,\n",
    "        # \"val/loss\": val_loss,\n",
    "        # \"val/perplexity\": val_perplexity,\n",
    "        # \"val/acc\": val_acc,\n",
    "        \"dt_ms\": t.iter_dt * 1000,\n",
    "        \"mem_cuda\": torch.cuda.memory_allocated() / 1e9,\n",
    "        \"mem\": mem,\n",
    "    })\n",
    "    # benchmark.append({'iter': t.iter_num, 'train_loss': t.loss, 'eval_loss': eval_loss, 'time': t.iter_time, 'mem': mem})\n",
    "    gc.collect()\n",
    "\n",
    "trainer.add_callback('on_batch_end', on_batch_end)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter': 0,\n",
       "  'train_loss': tensor(10.8628, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593017.1018848,\n",
       "  'mem': 1785602048},\n",
       " {'iter': 1,\n",
       "  'train_loss': tensor(10.5546, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593018.4563336,\n",
       "  'mem': 2024230912},\n",
       " {'iter': 2,\n",
       "  'train_loss': tensor(10.4861, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593019.8006442,\n",
       "  'mem': 2070908928},\n",
       " {'iter': 3,\n",
       "  'train_loss': tensor(10.2586, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593020.986921,\n",
       "  'mem': 2083426304},\n",
       " {'iter': 4,\n",
       "  'train_loss': tensor(10.2835, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593021.920062,\n",
       "  'mem': 2158997504},\n",
       " {'iter': 5,\n",
       "  'train_loss': tensor(10.2894, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593022.7859998,\n",
       "  'mem': 2247143424},\n",
       " {'iter': 6,\n",
       "  'train_loss': tensor(10.3551, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593023.662099,\n",
       "  'mem': 2322505728},\n",
       " {'iter': 7,\n",
       "  'train_loss': tensor(10.3704, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593024.6077392,\n",
       "  'mem': 2435653632},\n",
       " {'iter': 8,\n",
       "  'train_loss': tensor(10.0149, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593025.5422826,\n",
       "  'mem': 2479988736},\n",
       " {'iter': 9,\n",
       "  'train_loss': tensor(10.1732, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593026.4457662,\n",
       "  'mem': 2505129984},\n",
       " {'iter': 10,\n",
       "  'train_loss': tensor(9.9552, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593027.3574898,\n",
       "  'mem': 2592858112},\n",
       " {'iter': 11,\n",
       "  'train_loss': tensor(9.9202, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593028.2656667,\n",
       "  'mem': 2655809536},\n",
       " {'iter': 12,\n",
       "  'train_loss': tensor(9.8927, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593029.1658294,\n",
       "  'mem': 2731618304},\n",
       " {'iter': 13,\n",
       "  'train_loss': tensor(9.8906, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593030.0644233,\n",
       "  'mem': 2819461120},\n",
       " {'iter': 14,\n",
       "  'train_loss': tensor(9.7461, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593030.96478,\n",
       "  'mem': 2832158720},\n",
       " {'iter': 15,\n",
       "  'train_loss': tensor(9.9422, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593031.8876545,\n",
       "  'mem': 2882555904},\n",
       " {'iter': 16,\n",
       "  'train_loss': tensor(9.7844, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593032.907588,\n",
       "  'mem': 2945273856},\n",
       " {'iter': 17,\n",
       "  'train_loss': tensor(9.6894, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593033.8537142,\n",
       "  'mem': 2970587136},\n",
       " {'iter': 18,\n",
       "  'train_loss': tensor(9.7077, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593034.7837868,\n",
       "  'mem': 3008270336},\n",
       " {'iter': 19,\n",
       "  'train_loss': tensor(9.6597, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593035.7268658,\n",
       "  'mem': 3020877824},\n",
       " {'iter': 20,\n",
       "  'train_loss': tensor(9.7862, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593036.704292,\n",
       "  'mem': 3071098880},\n",
       " {'iter': 21,\n",
       "  'train_loss': tensor(9.6143, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593037.6416926,\n",
       "  'mem': 3134197760},\n",
       " {'iter': 22,\n",
       "  'train_loss': tensor(9.5772, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593038.5812569,\n",
       "  'mem': 3184386048},\n",
       " {'iter': 23,\n",
       "  'train_loss': tensor(9.4101, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593039.525725,\n",
       "  'mem': 3222294528},\n",
       " {'iter': 24,\n",
       "  'train_loss': tensor(9.5205, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593040.4779081,\n",
       "  'mem': 3335475200},\n",
       " {'iter': 25,\n",
       "  'train_loss': tensor(9.2257, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593041.4318771,\n",
       "  'mem': 3360718848},\n",
       " {'iter': 26,\n",
       "  'train_loss': tensor(9.6592, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593042.3866568,\n",
       "  'mem': 3423567872},\n",
       " {'iter': 27,\n",
       "  'train_loss': tensor(9.2134, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593043.3478758,\n",
       "  'mem': 3436040192},\n",
       " {'iter': 28,\n",
       "  'train_loss': tensor(9.0811, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593044.3100498,\n",
       "  'mem': 3423526912},\n",
       " {'iter': 29,\n",
       "  'train_loss': tensor(9.1258, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593045.2789295,\n",
       "  'mem': 3461443584},\n",
       " {'iter': 30,\n",
       "  'train_loss': tensor(9.1076, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593046.2520742,\n",
       "  'mem': 3473989632},\n",
       " {'iter': 31,\n",
       "  'train_loss': tensor(9.0082, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593047.2190564,\n",
       "  'mem': 3574595584},\n",
       " {'iter': 32,\n",
       "  'train_loss': tensor(9.3066, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593048.1947157,\n",
       "  'mem': 3587117056},\n",
       " {'iter': 33,\n",
       "  'train_loss': tensor(8.8473, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593049.1612015,\n",
       "  'mem': 3662680064},\n",
       " {'iter': 34,\n",
       "  'train_loss': tensor(8.8512, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593050.4119778,\n",
       "  'mem': 3712925696},\n",
       " {'iter': 35,\n",
       "  'train_loss': tensor(8.8350, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593051.4516973,\n",
       "  'mem': 3700514816},\n",
       " {'iter': 36,\n",
       "  'train_loss': tensor(9.0866, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593052.4199433,\n",
       "  'mem': 3801137152},\n",
       " {'iter': 37,\n",
       "  'train_loss': tensor(8.9262, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593053.385686,\n",
       "  'mem': 3813789696},\n",
       " {'iter': 38,\n",
       "  'train_loss': tensor(8.9309, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593054.3609684,\n",
       "  'mem': 3901804544},\n",
       " {'iter': 39,\n",
       "  'train_loss': tensor(8.8957, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593055.3253415,\n",
       "  'mem': 3939495936},\n",
       " {'iter': 40,\n",
       "  'train_loss': tensor(8.7853, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593056.324085,\n",
       "  'mem': 3977170944},\n",
       " {'iter': 41,\n",
       "  'train_loss': tensor(8.5133, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593057.304445,\n",
       "  'mem': 4065206272},\n",
       " {'iter': 42,\n",
       "  'train_loss': tensor(8.6711, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593058.2635481,\n",
       "  'mem': 4090347520},\n",
       " {'iter': 43,\n",
       "  'train_loss': tensor(8.8084, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593059.2316935,\n",
       "  'mem': 4077830144},\n",
       " {'iter': 44,\n",
       "  'train_loss': tensor(8.4586, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593060.2021537,\n",
       "  'mem': 4165959680},\n",
       " {'iter': 45,\n",
       "  'train_loss': tensor(8.7596, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593061.1813095,\n",
       "  'mem': 4241350656},\n",
       " {'iter': 46,\n",
       "  'train_loss': tensor(8.4508, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593062.2791834,\n",
       "  'mem': 4266704896},\n",
       " {'iter': 47,\n",
       "  'train_loss': tensor(8.2475, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593063.461335,\n",
       "  'mem': 4304326656}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
