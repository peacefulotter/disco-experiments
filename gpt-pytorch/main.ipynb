{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/minGPT.git\n",
    "!pip install -e ./minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.55M\n"
     ]
    }
   ],
   "source": [
    "model_type = 'gpt-nano'\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = 50257 # openai's model vocabulary\n",
    "model_config.block_size = 1024  # openai's model block_size (i.e. input context length)\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "batch_size = 16\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(\n",
    "#     lambda batch: tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt'), \n",
    "#     remove_columns=['text'], \n",
    "#     batch_size=batch_size,\n",
    "#     batched=True, \n",
    "# )\n",
    "# return dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 582510\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 2461\n",
       " }))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wikitext(split, tokenizer, max_length=512):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=split)\n",
    "    dataset = dataset.filter(lambda x: len(x['text']) > 0)\n",
    "    # dataset = dataset.map(lambda x: {'text': x['text'], 'length': [len(y) for y in x['text']] }, batched=True, batch_size=64)\n",
    "    # dataset = dataset.sort('length')\n",
    "    # def encode(batch):\n",
    "    #     return tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    # dataset.set_transform(encode)\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt'), \n",
    "        remove_columns=['text'], \n",
    "        batch_size=batch_size,\n",
    "        batched=True, \n",
    "    )\n",
    "    dataset = dataset.remove_columns(['attention_mask'])\n",
    "    dataset = dataset.with_format(\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = get_wikitext('train[:50%]', tokenizer, max_length=max_length)\n",
    "eval_dataset = get_wikitext('validation', tokenizer, max_length=max_length)\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikitextDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    # x = tokenizer(self.dataset[idx][\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    # x = x['input_ids']\n",
    "    \n",
    "    # y = torch.empty(x.shape)\n",
    "    # y[:,:-1] = x[:,1:]\n",
    "    # y[:,-1] = torch.ones(x.shape[0]) * 50256\n",
    "\n",
    "    # =========================================================\n",
    "\n",
    "    # x = self.dataset[idx]['input_ids']\n",
    "    # y = None\n",
    "    # if isinstance(idx, torch.Tensor) or isinstance(idx, slice):\n",
    "    #     y = torch.empty(x.shape)\n",
    "    #     y[:,:-1] = x[:,1:]\n",
    "    #     y[:,-1] = torch.ones(x.shape[0]) * 50256\n",
    "    # else:\n",
    "    #     y = torch.empty(x.shape, dtype=torch.long)\n",
    "    #     y[:-1] = x[1:]\n",
    "    #     y[-1] = 50256\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataset[idx]['input_ids']\n",
    "        y = torch.empty(x.shape, dtype=x.dtype)\n",
    "        y[:-1] = x[1:]\n",
    "        y[-1] = 50256\n",
    "        return x, y\n",
    "\n",
    "tr_dataset = WikitextDataset(train_dataset)\n",
    "ev_dataset = WikitextDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256],\n",
       "          [ 2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,  1279,\n",
       "            2954,    29, 17740,   357,  4960,  1058, 10545,   230,    99,   161,\n",
       "             254,   112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,\n",
       "             837,  6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,\n",
       "            1267,   837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,\n",
       "            6711,  2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
       "              31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13,\n",
       "           44206,   329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,\n",
       "             287,  2869,   837,   340,   318,   262,  2368,   983,   287,   262,\n",
       "             569, 18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,\n",
       "             286, 16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,\n",
       "             663, 27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,\n",
       "             983,   290,  5679,   262,   366, 17871,  5321,   366,   837,   257,\n",
       "           23634,  2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,\n",
       "             262,  5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,\n",
       "             290,   389, 46852,  1028,   262, 11773,  4326,   366,  1279,  2954,\n",
       "              29, 12552,   366,   764,   220,   198, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       "  tensor([[ 2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,  1279,\n",
       "            2954,    29, 17740,   357,  4960,  1058, 10545,   230,    99,   161,\n",
       "             254,   112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,\n",
       "             837,  6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,\n",
       "            1267,   837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,\n",
       "            6711,  2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
       "              31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13,\n",
       "           44206,   329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,\n",
       "             287,  2869,   837,   340,   318,   262,  2368,   983,   287,   262,\n",
       "             569, 18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,\n",
       "             286, 16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,\n",
       "             663, 27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,\n",
       "             983,   290,  5679,   262,   366, 17871,  5321,   366,   837,   257,\n",
       "           23634,  2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,\n",
       "             262,  5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,\n",
       "             290,   389, 46852,  1028,   262, 11773,  4326,   366,  1279,  2954,\n",
       "              29, 12552,   366,   764,   220,   198, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256],\n",
       "          [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "           50256, 50256, 50256, 50256, 50256, 50256]])),\n",
       " (tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256]),\n",
       "  tensor([  569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256])))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dataset[:2], tr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WikitextDataset(Dataset):\n",
    "#     def __init__(self, dataset, max_length=512):\n",
    "#         x = dataset['input_ids']\n",
    "#         size, _ = x.shape\n",
    "#         y = torch.empty((size, max_length))\n",
    "#         y[:,:-1] = x[:,1:]\n",
    "#         y[:,-1] = torch.ones(size) * 50256\n",
    "#         self.data = torch.cat((x.unsqueeze(2), y.unsqueeze(2)), dim=2)\n",
    "#         self.data = self.data.reshape((size, 2, max_length))\n",
    "#         self.data = self.data.to(torch.int64)\n",
    "#         print(self.data.shape)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x, y = self.data[idx]\n",
    "#         return x, y\n",
    "\n",
    "# tr_dataset = WikitextDataset(train_dataset, max_length)\n",
    "# ev_dataset = WikitextDataset(eval_dataset, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2382630912"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import psutil\n",
    "process = psutil.Process()\n",
    "\n",
    "def get_mem():\n",
    "    return process.memory_info().rss \n",
    "\n",
    "get_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n",
      "DT: 0.000, iter: 00000, train_loss: 10.9032, eval_loss: 0.0000, mem: 3281.75 MB\n",
      "DT: 1.184, iter: 00001, train_loss: 10.4913, eval_loss: 0.0000, mem: 3287.70 MB\n",
      "DT: 1.253, iter: 00002, train_loss: 10.4378, eval_loss: 0.0000, mem: 3467.96 MB\n",
      "DT: 1.008, iter: 00003, train_loss: 10.3889, eval_loss: 0.0000, mem: 3575.82 MB\n",
      "DT: 0.972, iter: 00004, train_loss: 10.1820, eval_loss: 0.0000, mem: 3683.86 MB\n",
      "DT: 0.979, iter: 00005, train_loss: 10.1112, eval_loss: 0.0000, mem: 3683.82 MB\n",
      "DT: 0.979, iter: 00006, train_loss: 10.1139, eval_loss: 0.0000, mem: 3731.95 MB\n",
      "DT: 1.024, iter: 00007, train_loss: 10.0356, eval_loss: 0.0000, mem: 3756.01 MB\n",
      "DT: 8.943, iter: 00008, train_loss: 10.1388, eval_loss: 0.0000, mem: 3792.07 MB\n",
      "DT: 2.230, iter: 00009, train_loss: 9.9840, eval_loss: 0.0000, mem: 3839.73 MB\n",
      "DT: 1.914, iter: 00010, train_loss: 9.8615, eval_loss: 0.0000, mem: 3923.80 MB\n",
      "DT: 1.787, iter: 00011, train_loss: 9.8537, eval_loss: 0.0000, mem: 4043.74 MB\n",
      "DT: 1.878, iter: 00012, train_loss: 9.8639, eval_loss: 0.0000, mem: 4163.80 MB\n",
      "DT: 1.896, iter: 00013, train_loss: 9.7969, eval_loss: 0.0000, mem: 4307.99 MB\n",
      "DT: 1.439, iter: 00014, train_loss: 9.6187, eval_loss: 0.0000, mem: 4536.18 MB\n",
      "DT: 1.290, iter: 00015, train_loss: 9.7219, eval_loss: 0.0000, mem: 4716.00 MB\n",
      "DT: 1.370, iter: 00016, train_loss: 9.5483, eval_loss: 0.0000, mem: 4919.93 MB\n",
      "DT: 1.359, iter: 00017, train_loss: 9.6290, eval_loss: 0.0000, mem: 5099.76 MB\n",
      "DT: 1.373, iter: 00018, train_loss: 9.5197, eval_loss: 0.0000, mem: 5291.71 MB\n",
      "DT: 1.356, iter: 00019, train_loss: 9.5946, eval_loss: 0.0000, mem: 5339.82 MB\n",
      "DT: 1.355, iter: 00020, train_loss: 9.4606, eval_loss: 0.0000, mem: 5567.76 MB\n",
      "DT: 1.062, iter: 00021, train_loss: 9.5366, eval_loss: 0.0000, mem: 5711.82 MB\n",
      "DT: 1.078, iter: 00022, train_loss: 9.5101, eval_loss: 0.0000, mem: 5783.72 MB\n",
      "DT: 1.009, iter: 00023, train_loss: 9.4134, eval_loss: 0.0000, mem: 5927.91 MB\n",
      "DT: 1.012, iter: 00024, train_loss: 9.3377, eval_loss: 0.0000, mem: 6095.98 MB\n",
      "DT: 1.018, iter: 00025, train_loss: 9.2790, eval_loss: 0.0000, mem: 6239.92 MB\n",
      "DT: 1.105, iter: 00026, train_loss: 9.2295, eval_loss: 0.0000, mem: 6395.73 MB\n",
      "DT: 1.097, iter: 00027, train_loss: 9.3375, eval_loss: 0.0000, mem: 6647.80 MB\n",
      "DT: 1.047, iter: 00028, train_loss: 9.1226, eval_loss: 0.0000, mem: 6719.99 MB\n",
      "DT: 1.106, iter: 00029, train_loss: 9.0648, eval_loss: 0.0000, mem: 6827.80 MB\n",
      "DT: 1.177, iter: 00030, train_loss: 8.9879, eval_loss: 0.0000, mem: 7043.85 MB\n",
      "DT: 1.188, iter: 00031, train_loss: 8.9003, eval_loss: 0.0000, mem: 7259.91 MB\n",
      "DT: 1.417, iter: 00032, train_loss: 9.0021, eval_loss: 0.0000, mem: 7296.09 MB\n",
      "DT: 1.135, iter: 00033, train_loss: 8.9431, eval_loss: 0.0000, mem: 7559.91 MB\n",
      "DT: 1.176, iter: 00034, train_loss: 8.7933, eval_loss: 0.0000, mem: 7787.73 MB\n",
      "DT: 1.144, iter: 00035, train_loss: 8.6403, eval_loss: 0.0000, mem: 7980.05 MB\n",
      "DT: 1.133, iter: 00036, train_loss: 8.9705, eval_loss: 0.0000, mem: 8232.12 MB\n",
      "DT: 1.144, iter: 00037, train_loss: 8.7739, eval_loss: 0.0000, mem: 8471.93 MB\n",
      "DT: 1.165, iter: 00038, train_loss: 8.6506, eval_loss: 0.0000, mem: 8615.75 MB\n",
      "DT: 1.155, iter: 00039, train_loss: 8.8069, eval_loss: 0.0000, mem: 8807.68 MB\n",
      "DT: 1.234, iter: 00040, train_loss: 8.5708, eval_loss: 0.0000, mem: 8939.83 MB\n",
      "DT: 1.129, iter: 00041, train_loss: 8.5797, eval_loss: 0.0000, mem: 9072.00 MB\n",
      "DT: 1.147, iter: 00042, train_loss: 8.4063, eval_loss: 0.0000, mem: 9191.99 MB\n",
      "DT: 1.254, iter: 00043, train_loss: 8.5065, eval_loss: 0.0000, mem: 9299.69 MB\n",
      "DT: 1.132, iter: 00044, train_loss: 8.1788, eval_loss: 0.0000, mem: 9431.93 MB\n",
      "DT: 1.240, iter: 00045, train_loss: 8.6584, eval_loss: 0.0000, mem: 9599.86 MB\n",
      "DT: 1.119, iter: 00046, train_loss: 8.2209, eval_loss: 0.0000, mem: 9791.80 MB\n",
      "DT: 1.632, iter: 00047, train_loss: 8.3562, eval_loss: 0.0000, mem: 9826.25 MB\n",
      "DT: 1.191, iter: 00048, train_loss: 8.2142, eval_loss: 0.0000, mem: 9947.70 MB\n",
      "DT: 1.217, iter: 00049, train_loss: 8.0990, eval_loss: 0.0000, mem: 10068.26 MB\n",
      "DT: 1.245, iter: 00050, train_loss: 8.1183, eval_loss: 0.0000, mem: 10163.86 MB\n",
      "DT: 1.284, iter: 00051, train_loss: 7.7585, eval_loss: 0.0000, mem: 10211.13 MB\n",
      "DT: 1.131, iter: 00052, train_loss: 7.8720, eval_loss: 0.0000, mem: 10406.70 MB\n",
      "DT: 1.197, iter: 00053, train_loss: 8.0834, eval_loss: 0.0000, mem: 10387.76 MB\n",
      "DT: 1.269, iter: 00054, train_loss: 7.8027, eval_loss: 0.0000, mem: 10399.27 MB\n",
      "DT: 1.337, iter: 00055, train_loss: 7.9160, eval_loss: 0.0000, mem: 10222.44 MB\n",
      "DT: 1.289, iter: 00056, train_loss: 7.6964, eval_loss: 0.0000, mem: 10299.75 MB\n",
      "DT: 1.156, iter: 00057, train_loss: 7.7094, eval_loss: 0.0000, mem: 10423.20 MB\n",
      "DT: 1.351, iter: 00058, train_loss: 7.6197, eval_loss: 0.0000, mem: 10267.73 MB\n",
      "DT: 1.330, iter: 00059, train_loss: 7.6162, eval_loss: 0.0000, mem: 10131.92 MB\n",
      "DT: 1.265, iter: 00060, train_loss: 7.6346, eval_loss: 0.0000, mem: 10308.01 MB\n",
      "DT: 1.155, iter: 00061, train_loss: 7.5217, eval_loss: 0.0000, mem: 10072.79 MB\n",
      "DT: 1.499, iter: 00062, train_loss: 7.7603, eval_loss: 0.0000, mem: 10314.02 MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 6e-4\n",
    "train_config.max_iters = 3600\n",
    "train_config.batch_size = batch_size\n",
    "train_config.num_workers = 4\n",
    "trainer = Trainer(train_config, model, tr_dataset)\n",
    "\n",
    "ev_loader = DataLoader(\n",
    "    ev_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def custom_evaluate(model, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in ev_loader:\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        x, y = batch\n",
    "        logits, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "benchmark = []\n",
    "\n",
    "def on_batch_end(t):\n",
    "    # if t.iter_num % 2 == 0: \n",
    "    eval_loss = 0 # custom_evaluate(t.model, device=t.device)\n",
    "    mem = get_mem()\n",
    "    print(f'DT: {t.iter_dt:.3f}, iter: {t.iter_num:05d}, train_loss: {t.loss:.4f}, eval_loss: {eval_loss:.4f}, mem: {mem / (1024 * 1024):.2f} MB')\n",
    "    benchmark.append({'iter': t.iter_num, 'train_loss': t.loss, 'eval_loss': eval_loss, 'time': t.iter_time, 'mem': mem})\n",
    "    gc.collect()\n",
    "\n",
    "trainer.add_callback('on_batch_end', on_batch_end)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter': 0,\n",
       "  'train_loss': tensor(10.8628, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593017.1018848,\n",
       "  'mem': 1785602048},\n",
       " {'iter': 1,\n",
       "  'train_loss': tensor(10.5546, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593018.4563336,\n",
       "  'mem': 2024230912},\n",
       " {'iter': 2,\n",
       "  'train_loss': tensor(10.4861, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593019.8006442,\n",
       "  'mem': 2070908928},\n",
       " {'iter': 3,\n",
       "  'train_loss': tensor(10.2586, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593020.986921,\n",
       "  'mem': 2083426304},\n",
       " {'iter': 4,\n",
       "  'train_loss': tensor(10.2835, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593021.920062,\n",
       "  'mem': 2158997504},\n",
       " {'iter': 5,\n",
       "  'train_loss': tensor(10.2894, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593022.7859998,\n",
       "  'mem': 2247143424},\n",
       " {'iter': 6,\n",
       "  'train_loss': tensor(10.3551, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593023.662099,\n",
       "  'mem': 2322505728},\n",
       " {'iter': 7,\n",
       "  'train_loss': tensor(10.3704, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593024.6077392,\n",
       "  'mem': 2435653632},\n",
       " {'iter': 8,\n",
       "  'train_loss': tensor(10.0149, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593025.5422826,\n",
       "  'mem': 2479988736},\n",
       " {'iter': 9,\n",
       "  'train_loss': tensor(10.1732, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593026.4457662,\n",
       "  'mem': 2505129984},\n",
       " {'iter': 10,\n",
       "  'train_loss': tensor(9.9552, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593027.3574898,\n",
       "  'mem': 2592858112},\n",
       " {'iter': 11,\n",
       "  'train_loss': tensor(9.9202, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593028.2656667,\n",
       "  'mem': 2655809536},\n",
       " {'iter': 12,\n",
       "  'train_loss': tensor(9.8927, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593029.1658294,\n",
       "  'mem': 2731618304},\n",
       " {'iter': 13,\n",
       "  'train_loss': tensor(9.8906, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593030.0644233,\n",
       "  'mem': 2819461120},\n",
       " {'iter': 14,\n",
       "  'train_loss': tensor(9.7461, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593030.96478,\n",
       "  'mem': 2832158720},\n",
       " {'iter': 15,\n",
       "  'train_loss': tensor(9.9422, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593031.8876545,\n",
       "  'mem': 2882555904},\n",
       " {'iter': 16,\n",
       "  'train_loss': tensor(9.7844, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593032.907588,\n",
       "  'mem': 2945273856},\n",
       " {'iter': 17,\n",
       "  'train_loss': tensor(9.6894, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593033.8537142,\n",
       "  'mem': 2970587136},\n",
       " {'iter': 18,\n",
       "  'train_loss': tensor(9.7077, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593034.7837868,\n",
       "  'mem': 3008270336},\n",
       " {'iter': 19,\n",
       "  'train_loss': tensor(9.6597, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593035.7268658,\n",
       "  'mem': 3020877824},\n",
       " {'iter': 20,\n",
       "  'train_loss': tensor(9.7862, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593036.704292,\n",
       "  'mem': 3071098880},\n",
       " {'iter': 21,\n",
       "  'train_loss': tensor(9.6143, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593037.6416926,\n",
       "  'mem': 3134197760},\n",
       " {'iter': 22,\n",
       "  'train_loss': tensor(9.5772, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593038.5812569,\n",
       "  'mem': 3184386048},\n",
       " {'iter': 23,\n",
       "  'train_loss': tensor(9.4101, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593039.525725,\n",
       "  'mem': 3222294528},\n",
       " {'iter': 24,\n",
       "  'train_loss': tensor(9.5205, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593040.4779081,\n",
       "  'mem': 3335475200},\n",
       " {'iter': 25,\n",
       "  'train_loss': tensor(9.2257, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593041.4318771,\n",
       "  'mem': 3360718848},\n",
       " {'iter': 26,\n",
       "  'train_loss': tensor(9.6592, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593042.3866568,\n",
       "  'mem': 3423567872},\n",
       " {'iter': 27,\n",
       "  'train_loss': tensor(9.2134, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593043.3478758,\n",
       "  'mem': 3436040192},\n",
       " {'iter': 28,\n",
       "  'train_loss': tensor(9.0811, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593044.3100498,\n",
       "  'mem': 3423526912},\n",
       " {'iter': 29,\n",
       "  'train_loss': tensor(9.1258, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593045.2789295,\n",
       "  'mem': 3461443584},\n",
       " {'iter': 30,\n",
       "  'train_loss': tensor(9.1076, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593046.2520742,\n",
       "  'mem': 3473989632},\n",
       " {'iter': 31,\n",
       "  'train_loss': tensor(9.0082, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593047.2190564,\n",
       "  'mem': 3574595584},\n",
       " {'iter': 32,\n",
       "  'train_loss': tensor(9.3066, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593048.1947157,\n",
       "  'mem': 3587117056},\n",
       " {'iter': 33,\n",
       "  'train_loss': tensor(8.8473, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593049.1612015,\n",
       "  'mem': 3662680064},\n",
       " {'iter': 34,\n",
       "  'train_loss': tensor(8.8512, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593050.4119778,\n",
       "  'mem': 3712925696},\n",
       " {'iter': 35,\n",
       "  'train_loss': tensor(8.8350, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593051.4516973,\n",
       "  'mem': 3700514816},\n",
       " {'iter': 36,\n",
       "  'train_loss': tensor(9.0866, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593052.4199433,\n",
       "  'mem': 3801137152},\n",
       " {'iter': 37,\n",
       "  'train_loss': tensor(8.9262, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593053.385686,\n",
       "  'mem': 3813789696},\n",
       " {'iter': 38,\n",
       "  'train_loss': tensor(8.9309, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593054.3609684,\n",
       "  'mem': 3901804544},\n",
       " {'iter': 39,\n",
       "  'train_loss': tensor(8.8957, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593055.3253415,\n",
       "  'mem': 3939495936},\n",
       " {'iter': 40,\n",
       "  'train_loss': tensor(8.7853, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593056.324085,\n",
       "  'mem': 3977170944},\n",
       " {'iter': 41,\n",
       "  'train_loss': tensor(8.5133, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593057.304445,\n",
       "  'mem': 4065206272},\n",
       " {'iter': 42,\n",
       "  'train_loss': tensor(8.6711, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593058.2635481,\n",
       "  'mem': 4090347520},\n",
       " {'iter': 43,\n",
       "  'train_loss': tensor(8.8084, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593059.2316935,\n",
       "  'mem': 4077830144},\n",
       " {'iter': 44,\n",
       "  'train_loss': tensor(8.4586, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593060.2021537,\n",
       "  'mem': 4165959680},\n",
       " {'iter': 45,\n",
       "  'train_loss': tensor(8.7596, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593061.1813095,\n",
       "  'mem': 4241350656},\n",
       " {'iter': 46,\n",
       "  'train_loss': tensor(8.4508, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593062.2791834,\n",
       "  'mem': 4266704896},\n",
       " {'iter': 47,\n",
       "  'train_loss': tensor(8.2475, grad_fn=<NllLossBackward0>),\n",
       "  'eval_loss': 0,\n",
       "  'time': 1698593063.461335,\n",
       "  'mem': 4304326656}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
