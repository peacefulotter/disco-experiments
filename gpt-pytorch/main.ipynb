{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/minGPT.git\n",
    "!pip install -e ./minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.55M\n"
     ]
    }
   ],
   "source": [
    "model_type = 'gpt-nano'\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = 50257 # openai's model vocabulary\n",
    "model_config.block_size = 1024  # openai's model block_size (i.e. input context length)\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 90068\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "batch_size = 64\n",
    "max_length = 512\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split='train[:5%]')\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda batch: tokenizer(batch[\"text\"], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt'), \n",
    "    remove_columns=['text'], \n",
    "    batch_size=batch_size,\n",
    "    batched=True, \n",
    ")\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90068, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "class WikitextDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        x = dataset['input_ids']\n",
    "        size, _ = x.shape\n",
    "        y = torch.empty((size, max_length))\n",
    "        y[:,:-1] = x[:,1:]\n",
    "        y[:,-1] = torch.ones(size) * 50256\n",
    "        self.data = torch.cat((x.unsqueeze(2), y.unsqueeze(2)), dim=2)\n",
    "        self.data = self.data.reshape((size, 2, max_length))\n",
    "        self.data = self.data.to(torch.int64)\n",
    "        print(self.data.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return x, y\n",
    "\n",
    "dataset = WikitextDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import psutil\n",
    "process = psutil.Process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n",
      "1698332923.6657112 | DT: 0.0, iter: 0, loss: 0.04953203722834587, mem: 5228290048\n",
      "1698332927.0223153 | DT: 3.3566040992736816, iter: 1, loss: 0.057481199502944946, mem: 5183942656\n",
      "1698332929.613135 | DT: 2.590819835662842, iter: 2, loss: 0.49202868342399597, mem: 5190336512\n",
      "1698332932.1125753 | DT: 2.4994401931762695, iter: 3, loss: 0.2675166428089142, mem: 5190610944\n",
      "1698332934.9692745 | DT: 2.856699228286743, iter: 4, loss: 0.07993975281715393, mem: 5221822464\n",
      "1698332937.5030942 | DT: 2.5338196754455566, iter: 5, loss: 0.4100649058818817, mem: 5190447104\n",
      "1698332940.013746 | DT: 2.5106518268585205, iter: 6, loss: 0.012333880178630352, mem: 5196398592\n",
      "1698332942.7196143 | DT: 2.7058682441711426, iter: 7, loss: 0.01463429443538189, mem: 5234278400\n",
      "1698332945.373458 | DT: 2.653843641281128, iter: 8, loss: 0.10016204416751862, mem: 5233557504\n",
      "1698332947.997038 | DT: 2.623579978942871, iter: 9, loss: 0.01660393364727497, mem: 5193203712\n",
      "1698332951.0798109 | DT: 3.082772970199585, iter: 10, loss: 0.18006542325019836, mem: 5197512704\n",
      "1698332953.7536323 | DT: 2.673821449279785, iter: 11, loss: 0.010812096297740936, mem: 5197770752\n",
      "1698332956.4642649 | DT: 2.710632562637329, iter: 12, loss: 0.013046545907855034, mem: 5234933760\n",
      "1698332959.1309938 | DT: 2.666728973388672, iter: 13, loss: 0.20640169084072113, mem: 5226921984\n",
      "1698332962.0504563 | DT: 2.9194624423980713, iter: 14, loss: 0.021394526585936546, mem: 5167697920\n",
      "1698332965.4962687 | DT: 3.445812463760376, iter: 15, loss: 0.00719390157610178, mem: 5176139776\n",
      "1698332968.4696062 | DT: 2.973337411880493, iter: 16, loss: 0.008817593567073345, mem: 5176000512\n",
      "1698332971.1881115 | DT: 2.718505382537842, iter: 17, loss: 0.0024188843090087175, mem: 5176205312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/edge/epfl/ma3/disco/gpt-pytorch/main.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/edge/epfl/ma3/disco/gpt-pytorch/main.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39miter_time\u001b[39m}\u001b[39;00m\u001b[39m | DT: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39miter_dt\u001b[39m}\u001b[39;00m\u001b[39m, iter: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39miter_num\u001b[39m}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39mloss\u001b[39m}\u001b[39;00m\u001b[39m, mem: \u001b[39m\u001b[39m{\u001b[39;00mprocess\u001b[39m.\u001b[39mmemory_info()\u001b[39m.\u001b[39mrss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/edge/epfl/ma3/disco/gpt-pytorch/main.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m trainer\u001b[39m.\u001b[39madd_callback(\u001b[39m'\u001b[39m\u001b[39mon_batch_end\u001b[39m\u001b[39m'\u001b[39m, on_batch_end)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/edge/epfl/ma3/disco/gpt-pytorch/main.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/epfl/ma3/disco/gpt-pytorch/minGPT/mingpt/trainer.py:97\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m model\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 97\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     98\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), config\u001b[39m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 6e-4\n",
    "train_config.max_iters = 60_000\n",
    "train_config.batch_size = 16\n",
    "trainer = Trainer(train_config, model, dataset)\n",
    "def on_batch_end(t):\n",
    "    print(f'{t.iter_time} | DT: {t.iter_dt}, iter: {t.iter_num}, loss: {t.loss}, mem: {process.memory_info().rss}')\n",
    "trainer.add_callback('on_batch_end', on_batch_end)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
